\chapter{Introduction}
\label{sec:introduction}

\section{Context}

% 1.1 Background and Context
% =============================================================================
% Begin by providing an overview of the field of neural networks and their
% importance in various domains such as machine learning, computer vision,
% natural language processing, and more. Briefly mention the rapid advancements
% in neural network architectures and techniques that have led to significant
% breakthroughs.

\Textcite{frenayParameterinsensitiveKernelExtreme2011} presented a
``parameter-insensitive'' kernel inspired from extreme learning machines
(ELMs)~\cite{huangExtremeLearningMachine2006}. According to the authors, the
meta-parameter associated with the kernel, once it reaches a certain threshold,
further increasing its value
does not affect the performance of the
results obtained. In the paper they demonstrate the effectiveness of the kernel
in a Support Vector Regression (SVR) task. Showing that results obtained
with this kernel are not significantly different from the results obtained
in the \emph{state-of-the-art} of each dataset using Gaussian kernels.

If the kernel is indeed parameter-insensitive, then it is possible that other
infinite neural network kernels also have this property. Infinite neural network
kernels are a family of kernels which are derived from infinite neural networks,
which are networks with infinitely many hidden units.

\subsection{Goals}

The aim of this project is to study the effects of these infinite neural network
kernels in support vector machines; understanding their behaviour in practical
learning problems, with a special focus on the dependence (or lack of) of
performance on the kernel
hyperparameters.

Having a parameter-insensitive kernel has the potential to cut down on the time
spent on hyperparameter tuning, which is one of the most time-consuming tasks in
machine learning.

\subsection{Scope and Limitations}%
\label{sub:scope_and_limitations}

Due to the time and computational constraints, the scope of this project is
limited to the study on a few datasets. The datasets chosen try to cover a wide
range of characteristics, but are in no way exhaustive. But should be enough to
provide a good understanding of the behaviour of the kernels and gain some
insights on their limitations.

\section{Methodology}

\subsection{Literature Review}%
\label{sub:literature_review}

First, we review the literature on infinite neural networks and any works
related to kernels which can be derived from them. Aim to understand the
current state of the art in regard to infinite neural network kernels and
their applications.

Looking at papers which cite
\textcite{frenayParameterinsensitiveKernelExtreme2011}, we find some surveys
on \cite{huangTrendsExtremeLearning2015,huangExtremeLearningMachines2011}, most
citations mention the kernel as a curiosity when talking about extreme learning
machines \cite{yuLearningDeepRepresentations2015}

An important consideration is not to confuse the ``ELM kernel'' with the
``Kernel ELM'' (KELM) \cite{huangExtremeLearningMachine2012}. The latter is an
extension of the ELM architecture to support kernel learning, which has gained
popularity in recent years. We are interested in the former, which is a kernel
derived from the interpretation of the ELM as an infinite neural network and is
not related to the KELM.

The only other infinite neural network kernels we have found in the literature
are the ones presented in \textcite{choLargemarginClassificationInfinite2010},
which are derived from infinite neural networks with a different activation
function than the one used by
\citeauthor{frenayParameterinsensitiveKernelExtreme2011}. These kernels
use the angle of the input vectors to compute the similarity between them,
and therefore use $\arccos$ to obtain the angle. Throughout this thesis
we will refer to these kernels as the ``arccosine kernels'' and the kernel
from \citeauthor{frenayParameterinsensitiveKernelExtreme2011} as the
``arcsine kernel''.

We have found no other kernels derived from infinite neural networks in the
literature, it seems that this is an unexplored area or that analytical
expressions for such kernels are impossible to derive for other configurations.

\begin{table}[H]
    \caption{Summary of infinite neural network kernels with analytical expressions found in the literature.}
    \label{tab:kernels_summary}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Kernel}  & \textbf{Distribution} & \textbf{Activation function} & \textbf{Reference}                                                                       & \textbf{Equation}      \\
        \midrule
        Arc-sine         & \textit{Gaussian}     & \textit{erf}                 & \cite{williamsComputationInfiniteNeural1998,frenayParameterinsensitiveKernelExtreme2011} & \ref{eq:kernel_asin}   \\
        \addlinespace
        Arc-cosine $n=0$ & \textit{Gaussian}     & \textit{heavyside}           & \cite{choLargemarginClassificationInfinite2010,pandeyGoDeepWide2014}                     & \ref{eq:kernel_cho_n0} \\
        Arc-cosine $n=1$ & \textit{Gaussian}     & \textit{ReLu}                & \cite{choLargemarginClassificationInfinite2010,pandeyGoDeepWide2014}                     & \ref{eq:kernel_cho_n1} \\
        Arc-cosine $n=2$ & \textit{Gaussian}     & \textit{RePu}                & \cite{choLargemarginClassificationInfinite2010,pandeyGoDeepWide2014}                     & \ref{eq:kernel_cho_n2} \\
        \bottomrule
    \end{tabular}
\end{table}


\Cref{tab:kernels_summary} gives an overview of the infinite neural network
kernels with known analytical expressions in the literature which will be
used in this thesis.
We have 4 kernels in total, all of them derived from infinite neural networks
with Gaussian priors but with different activation functions. All of them have a
single hyperparameter, $\sigma_w$, which is the standard deviation of the
Gaussian prior. Additionally, we can compare the \emph{normalized} and
\emph{non-normalized} versions of the kernels. This gives a potential total of
8 kernels to compare.

\subsection{Objectives}
Once we have a clear understanding of the state of the art, we can define the
objectives of the thesis.

\begin{description}
    \item[O1] Reproduce the results obtained by
        \textcite{frenayParameterinsensitiveKernelExtreme2011}.
    \item[O2] Compare the performance of the kernels with different datasets and
        hyperparameters.
    \item[O3] See how the other infinite neural network kernels behave in comparison
        with the arcsine kernel.
    \item[O4] Keep the computational cost of the experiments low in order to be
        able to run them in a reasonable amount of time.
        \begin{itemize}
            \item The experiments should be able to run on a laptop.
            \item The experiments should be able to run in a reasonable amount
                  of time (less than a day).
        \end{itemize}
    \item[O5] Make the experiments reproducible and the code available to the
        public. % TODO: add link to code, and Zenodo
        \begin{itemize}
            \item The code should be well documented.
            \item The code should be easy to use.
            \item All dependencies should be documented.
        \end{itemize}
    \item[O6] Apply meta-learning to find the best kernel and hyperparameters
        for a given dataset.
\end{description}

The first are related to the results of our experiments and the last two are
constraints on how the experiments should be performed in order to ensure that
they are both reproducible and computationally feasible.

\subsubsection{Hypotheses}
\label{sec:objectives_and_hypotheses}

% \section{Kernel summary}
% \label{sec:kernels}

% TODO: formalize all the hypothesis
We formulate the following hypotheses regrading the behaviour of the kernels and
their hyperparameters. These hypotheses are based on the results obtained by
\textcite{frenayParameterinsensitiveKernelExtreme2011}:

\begin{hypothesis}\label{hyp:threshold}
    Kernels originating from infinite neural networks will exhibit the same
    behaviour described by
    \textcite{frenayParameterinsensitiveKernelExtreme2011}; where there is a
    threshold value of their hyperparameter past which the kernel prediction
    capability does not improve.
\end{hypothesis}

\begin{hypothesis}
    The value of the hyper-parameter at the threshold described in
    \ref{hyp:threshold}, will be the optimal (or close to optimal) value for the
    kernel.
\end{hypothesis}

\begin{hypothesis}
    Infinite neural network kernels are able to generalize to other datasets and
    obtain results comparable to the \emph{state-of-the-art}.
\end{hypothesis}

The aim is to design experiments that will allow us to test these hypotheses.

\subsection{Data Collection and Generation}%

Due to the nature of the analysis, we need a wide variety of datasets in order
to observe the behaviour of the kernels in different scenarios. The chosen
datasets are summarized in \cref{tab:datasets_regression,tab:datasets_classification}.

\begin{table}[H]
    \begin{threeparttable}
        \caption{Regression datasets used in this thesis}
        \label{tab:datasets_regression}
        \input{tables/datasets.tex}
        \begin{tablenotes}
            \item[a] The Bank and Pumadyn consist of 8 synthetic datasets each. With different
            number of features (8/32), more linearity or less linearity and more or less noise.
        \end{tablenotes}
        % INFO: keep this up to date with scripts/tables.jl
    \end{threeparttable}
\end{table}

\begin{table}[H]
    \caption{Classification datasets used in this thesis}
    \label{tab:datasets_classification}
    \input{tables/datasets_cat.tex}
    % INFO: keep this up to date with scripts/tables.jl
\end{table}

\subsection{Experiments}%

We can classify the experiments in two groups according to their purposes. The
first group of experiments is used to understand the performance of the
different kernels and their hyperparameters relative to each other. This is
similar to the experiments performed by
\textcite{frenayParameterinsensitiveKernelExtreme2011}, but with a wider range
of datasets and using additional kernels.

The second batch of experiments takes the results from the first group and
applies meta-learning techniques with the goal of determining if it is possible
to reach data driven conclusions about which kernel and hyperparameters to use
for a given dataset.

In \cref{sec:experiments} we describe the experimental design in more detail.

% TODO: Gender competency
% https://www.fib.upc.edu/en/studies/masters/master-data-science/masters-thesis/gender-competency
%
% Wong colorscheme (used in Makie) \cite{wongPointsViewColor2011}
% + patterns when possible

\subsection{Diversity and Inclusion}%

The datasets used in this thesis are publicly available and have been widely
used in other studies. Some datasets concern potentially sensitive topics such
as cancer, alcohol consumption or community crime rates. However, the aim of the
thesis is not to study the data itself, but the behaviour of the kernels in
different scenarios. As such, the datasets were chosen based on their
characteristics and not on the topic they represent and aiming to have a diverse
set of datasets.

Nevertheless, we acknowledge that there may be a bias in the selection of the
datasets. Since we had limited computational resources and time to perform the
experiments, the number of datasets is limited and not as diverse as we would
like.

A special effort has been taken to ensure that the data visualizations
presented in this thesis are accessible. The colors are selected to be
distinguishable by people with color blindness and the figures are designed to
be readable when printed in black and white. In particular,
we used the colors described by \textcite{wongPointsViewColor2011} and
the perceptually uniform colors from \textcite{crameriScientificColourMaps2023}.
When applicable, we used patterns and changes in line styles or markers to
further distinguish the different series in the plots.

% \subsection{Experimental Design}%

\section{Structure of the thesis}
% TODO: make sure this is up to date.

\Cref{sec:theoretical_background} gives an overview of the theoretical
background of the kernels used in this thesis, showing their derivation and
properties. In \cref{sec:implementation} we describe the implementation of the
kernels in \emph{C} and the wrappers used to interface with them from
\emph{Julia}. \Cref{sec:experiments} discussed the experimental design.
\Cref{sec:analysis} provides an analysis of the results from the experiments
and discusses the implications of the findings.
\Cref{sec:conclusions} concludes the thesis and provides some ideas for future
work.

% 4. Methodology
% ==============================================================================

% 4.1 Data Collection and Preparation
% ==============================================================================
% Explain how you plan to gather or generate the data necessary for your
% analysis. If applicable, clarify the characteristics of the datasets you will
% use and how they relate to neural network kernels.

% 4.2 Experimental Design
% ==============================================================================
% Outline the experimental procedures you will follow to investigate the effects
% of infinite neural network kernels. Detail the neural network architectures,
% training strategies, and evaluation metrics you intend to use.

% 5. Structure of the Thesis
% ==============================================================================

% Provide a brief overview of how your thesis is organized, highlighting the
% main sections and the flow of your argument.


\section{Contributions}

% TODO: more things
The main contributions of this thesis are:
\begin{enumerate}
    \item Updated \libsvm in \texttt{nixpkgs} repository and enabled OpenMP support\footnote{\url{https://github.com/NixOS/nixpkgs/pull/221325}}.
    \item Extended \libsvm to support additional kernels\footnote{\url{https://github.com/leixb/libsvm}}.
    \item Extended the Julia wrapper for \libsvm to support these additional kernels and added additional
          features to report and limit the iterations of the solver\footnote{\url{https://github.com/leixb/LIBSVM.jl}}
    \item Analyzed the behaviour of the known infinite neural network kernels with analytical expressions.
          % TODO: make sure this threshold is correct
    \item Shown that these infinite neural network kernels are parameter insensitive for high (\threshold) values of $\sigma$.
    \item For some datasets, we have found that the region where arcsine kernels are parameter insensitive obtains
          significantly worse results than the optimal value of sigma.
    \item Shown that the arcsine kernel does not need to be normalized for high (\threshold) values of $\sigma$.
    \item Shown that the arccosine kernels are parameter insensitive when normalized.
\end{enumerate}
