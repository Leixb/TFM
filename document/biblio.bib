@book{aggarwalNeuralNetworksDeep2018,
  title = {Neural Networks and Deep Learning: A Textbook},
  shorttitle = {Neural Networks and Deep Learning},
  author = {Aggarwal, Charu C.},
  date = {2018},
  publisher = {{Springer}},
  location = {{Cham, Switzerland}},
  isbn = {978-3-319-94462-3},
  langid = {english},
  pagetotal = {xxiii+497},
  keywords = {Aprenentatge automàtic,Xarxes neuronals (Informàtica)},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T15:37:43.543Z},
  file = {/home/leix/Documents/Zotero/storage/DE9SVFM9/Aggarwal - 2018 - Neural networks and deep learning a textbook.pdf}
}

@article{ahujaDeterministicMultikernelExtreme2021,
  title = {Deterministic Multikernel Extreme Learning Machine with Fuzzy Feature Extraction for Pattern Classification},
  author = {Ahuja, B. and Vishwakarma, V.P.},
  date = {2021},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimedia Tools Appl},
  volume = {80},
  number = {21-23},
  pages = {32423--32447},
  publisher = {{Springer}},
  issn = {13807501 (ISSN)},
  doi = {10.1007/s11042-021-11097-3},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111529292&doi=10.1007%2fs11042-021-11097-3&partnerID=40&md5=d73394e0df157b2573ef5ec22614d06a},
  abstract = {In this paper a novel multikernel deterministic extreme learning machine (ELM) and its variants are developed for classification of non-linear problems. Over a decade ELM is proved to be efficacious learning algorithms, but due to the non-deterministic and single kernel dependent feature mapping proprietary, it cannot be efficiently applied to real time classification problems that require invariant output solution. We address this problem by analytically calculation of input and hidden layer parameters for achieving the deterministic solution and exploiting the data fusion proficiency of multiple kernel learning. This investigation originates a novel deterministic ELM with single layer architecture in which kernel function is aggregation of linear combination of disparate base kernels. The weight of kernels depends upon perspicacity of problem and is empirically calculated. To further enhance the performance we utilize the capabilities of fuzzy set to find the pixel-wise coalition of face images with different classes. This handles the uncertainty involved in face recognition under varying environment condition. The pixel-wise membership value extracts the unseen information from images up to significant extent. The validity of the proposed approach is tested extensively on diverse set of face databases: databases with and without illumination variations and discrete types of kernels. The proposed algorithms achieve 100\% recognition rate for Yale database, when seven and eight images per identity are considered for training. Also, the superior recognition rate is achieved for AT \& T, Georgia Tech and AR databases, when compared with contemporary methods that prove the efficacy of proposed approaches in uncontrolled conditions significantly. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
  langid = {english},
  keywords = {/unread,Data fusion,Database systems,Deterministic learning,ELM,Environment conditions,Extreme learning machine (ELM),Face recognition,Fuzzy feature extraction,Fuzzy inference,Illumination variation,Image enhancement,KELM,Knowledge acquisition,Learning algorithms,Linear combinations,Machine learning,Membership values,MKL,Multiple Kernel Learning,Nonlinear problems,Pattern classification,Pixels},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.016Z}
}

@article{alstonBeginnerGuideConducting2021,
  title = {A {{Beginner}}'s {{Guide}} to {{Conducting Reproducible Research}}},
  author = {Alston, Jesse M. and Rick, Jessica A.},
  date = {2021},
  journaltitle = {The Bulletin of the Ecological Society of America},
  volume = {102},
  number = {2},
  pages = {e01801},
  issn = {2327-6096},
  doi = {10.1002/bes2.1801},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bes2.1801},
  urldate = {2023-08-27},
  langid = {english},
  file = {/home/leix/Documents/Zotero_Papers/TFM/meta-learning/Alston_Rick_2021_A Beginner's Guide to Conducting Reproducible Research.pdf;/home/leix/Documents/Zotero/storage/X86C5YVG/bes2.html}
}

@software{altmeyer2022CounterfactualExplanations,
  title = {{{CounterfactualExplanations}}.Jl - a Julia Package for Counterfactual Explanations and Algorithmic Recourse},
  author = {Altmeyer, Patrick},
  date = {2022},
  url = {https://github.com/juliatrustworthyai/CounterfactualExplanations.jl}
}

@article{altmeyerExplainingBlackBoxModels2023,
  title = {Explaining {{Black-Box Models}} through {{Counterfactuals}}},
  author = {Altmeyer, Patrick and family=Deursen, given=Arie, prefix=van, useprefix=false and family=Liem, given=Cynthia C., prefix=s, useprefix=false},
  date = {2023-08-12},
  journaltitle = {Proceedings of the JuliaCon Conferences},
  volume = {1},
  number = {1},
  pages = {130},
  issn = {2642-4029},
  doi = {10.21105/jcon.00130},
  url = {https://proceedings.juliacon.org/papers/10.21105/jcon.00130},
  urldate = {2023-09-07},
  abstract = {Altmeyer et al., (2023). Explaining Black-Box Models through Counterfactuals. JuliaCon Proceedings, 1(1), 130, https://doi.org/10.21105/jcon.00130},
  langid = {english},
  file = {/home/leix/Documents/Zotero/storage/9XNFQZ5N/Altmeyer et al. - 2023 - Explaining Black-Box Models through Counterfactuals.pdf}
}

@article{anDetectionMethodWalnut2022,
  title = {Detection {{Method}} for {{Walnut Shell-Kernel Separation Accuracy Based}} on {{Near-Infrared Spectroscopy}}},
  author = {An, M. and Cao, C. and Wu, Z. and Luo, K.},
  date = {2022},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {22},
  number = {21},
  publisher = {{MDPI}},
  issn = {14248220 (ISSN)},
  doi = {10.3390/s22218301},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141588969&doi=10.3390%2fs22218301&partnerID=40&md5=0924d4c2acca605a253a49aa89b12c31},
  abstract = {In this study, Near-infrared (NIR) spectroscopy was adopted for the collection of 1200 spectra of three types of walnut materials after breaking the shells. A detection model of the walnut shell-kernel separation accuracy was established. The preprocessing method of de-trending (DT) was adopted. A classification model based on a support vector machine (SVM) and an extreme learning machine (ELM) was established with the principal component factor as the input variable. The effect of the penalty value (C) and kernel width (g) on the SVM model was discussed. The selection criteria of the number of hidden layer nodes (L) in the ELM model were studied, and a genetic algorithm (GA) was used to optimize the input layer weight (W) and the hidden layer threshold value (B) of the ELM. The results revealed that the classification accuracy of SVM and ELM models for the shell, kernel, and chimera was 97.78\% and 97.11\%. The proposed method can serve as a reference for the detection of walnut shell-kernel separation accuracy. © 2022 by the authors.},
  langid = {english},
  keywords = {/unread,algorithm,Algorithms,Detection methods,ELM,Extreme learning machine (ELM),Genetic algorithms,Infrared devices,Juglans,Learning machines,Learning systems,Machine modelling,near infrared spectroscopy,Near infrared spectroscopy,NIRS,Radial basis function networks,Separation,shell-kernel separation,Shell-kernel separation,Shells (structures),Spectra's,{Spectroscopy, Near-Infrared},support vector machine,Support Vector Machine,Support vector machines,Support vectors machine,SVM,walnut,Walnut,Walnut shells},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.023Z}
}

@article{anitaThreeDimensionalAnalysis2019,
  title = {Three Dimensional Analysis of Spect Images for Diagnosing Early Parkinson’s Disease Using Radial Basis Function Kernel Extreme Learning Machine},
  author = {Anita, S. and Aruna Priya, P.},
  date = {2019},
  journaltitle = {Current Medical Imaging},
  shortjournal = {Curr. Med. Imaging.},
  volume = {15},
  number = {5},
  pages = {461--470},
  issn = {15734056 (ISSN)},
  doi = {10.2174/1573405614666171219154154},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070896261&doi=10.2174%2f1573405614666171219154154&partnerID=40&md5=b80727c8b9f02a647399262267bc5e05},
  abstract = {Background: Parkinson’s Disease (PD) is caused by the deficiency of dopamine, the neurotransmitter that has an effect on specific uptake region of the substantia nigra. Identification of PD is quite tough at an early stage. Objective: The present work proposes an expert system for three dimensional Single-Photon Emission Computed Tomography (SPECT) image to diagnose the early PD. Methods: The trans axial image slices are selected on the basis of their high specific uptake region. The processing techniques like preprocessing, segmentation and feature extraction are implemented to extract the quantification parameters like Intensity, correlation, entropy, skewness and kurtosis of the images. The Support Vector Machine (SVM) and Extreme Learning Machine (ELM) classifiers using Radial Basis Function kernel (RBF) are implemented and their results are compared in order to achieve better performance of the system. The performance of the system is evaluated in terms of sensitivity, specificity analysis, accuracy, Receiver Operating Curve (ROC) and Area Under the Curve (AUC). Results: It is found that RBF-ELM provides high accuracy of 98.2\% in diagnosing early PD. In addition, the similarity among the features is found out using K-means clustering algorithm to compute the threshold level for early PD. The computed threshold level is validated using Analysis of Variance (ANOVA). Conclusion: The proposed system has a great potential to assist the clinicians in the early diagnosis process of PD. © 2019 Bentham Science Publishers.},
  langid = {english},
  keywords = {/unread,Article,controlled study,correlation analysis,diagnostic accuracy,diagnostic test accuracy study,Dopamine,dopamine transporter,Early Parkinson’s disease,ELM,entropy,evaluation study,expert system,Extreme learning machine (ELM),Hoehn and Yahr scale,human,image extraction,image segmentation,imaging and display,intensity,K means clustering,kurtosis,learning algorithm,machine learning,major clinical study,mathematical parameters,Parkinson disease,preprocessing,priority journal,radial basis function kernel,radiopharmaceutical agent,receiver operating characteristic,sensitivity and specificity,signal noise ratio,single photon emission computed tomography,skewness,support vector machine,SVM,three dimensional imaging,Three dimensional SPECT images},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.997Z}
}

@online{AnyoneAnyBackground,
  title = {Anyone, from Any Background, Should Feel Encouraged to Participate and Contribute to {{ACM}}},
  url = {https://www.acm.org/diversity-inclusion},
  urldate = {2023-10-08},
  abstract = {Differences – in age, race, gender and sexual orientation, nationality, physical ability, thinking style and experience – bring richness to our efforts in providing quality programs and services for the global computing community.  ACM is committed to creating an environment that welcomes new ideas and perspectives, and where hostility or other antisocial behaviors are not tolerated.},
  langid = {english},
  file = {/home/leix/Documents/Zotero/storage/62LH67YP/diversity-inclusion.html}
}

@thesis{arquemartinezDissenyImplementacioEstudi2021,
  type = {bathesis},
  title = {Disseny, implementació i estudi de funcions de kernel per a vectors de variables no reals},
  author = {Arqué Martínez, Arnau},
  date = {2021-06-22},
  institution = {{Universitat Politècnica de Catalunya}},
  url = {https://upcommons.upc.edu/handle/2117/355889},
  urldate = {2023-03-17},
  langid = {catalan},
  keywords = {Aprenentatge automàtic,Àrees temàtiques de la UPC::Informàtica::Intel·ligència artificial::Aprenentatge automàtic,Funcions de,Funcions de kernel,Kernel,Kernel functions,Kernel methods,Machine learning,Mètodes kernel,Support vector machine,Support Vector Machine},
  annotation = {Accepted: 2021-11-09T16:22:33Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/Implementation/Arqué Martínez_2021_Disseny, implementació i estudi de funcions de kernel per a vectors de.pdf}
}

@inproceedings{bahiuddinStudyExtremeLearning2017,
  title = {Study of Extreme Learning Machine Activation Functions for Magnetorheological Fluid Modelling in Medical Devices Application},
  booktitle = {2017 {{International Conference}} on {{Robotics}}, {{Automation}} and {{Sciences}} ({{ICORAS}})},
  author = {Bahiuddin, Irfan and Mazlan, Saiful Amri and Shapiai, Mohd Ibrahim and Imaduddin, Fitrian and {Ubaidillah}},
  date = {2017-11},
  pages = {1--5},
  publisher = {{IEEE}},
  location = {{Melaka}},
  doi = {10.1109/ICORAS.2017.8308053},
  url = {http://ieeexplore.ieee.org/document/8308053/},
  urldate = {2023-02-27},
  eventtitle = {2017 {{International Conference}} on {{Robotics}}, {{Automation}} and {{Sciences}} ({{ICORAS}})},
  isbn = {978-1-5386-1908-7},
  annotation = {9 citations (Crossref) [2023-02-27] Read\_Status: Read Read\_Status\_Date: 2023-02-27T17:46:39.152Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/citen scopus/Bahiuddin et al_2017_Study of extreme learning machine activation functions for magnetorheological.pdf}
}

@online{BasicsJuliaData,
  title = {Basics · {{Julia Data Format}}},
  url = {https://docs.juliahub.com/JLD2/O1EyT/0.4.0/},
  urldate = {2023-08-27},
  file = {/home/leix/Documents/Zotero/storage/5IY36JYA/0.4.0.html}
}

@article{belanche-munozAnalysisKernelMatrices2023,
  title = {Analysis of {{Kernel Matrices}} via the von {{Neumann Entropy}} and {{Its Relation}} to {{RVM Performances}}},
  author = {Belanche-Muñoz, L.A. and Wiejacha, M.},
  date = {2023},
  journaltitle = {Entropy},
  shortjournal = {Entropy},
  volume = {25},
  number = {1},
  publisher = {{MDPI}},
  issn = {10994300 (ISSN)},
  doi = {10.3390/e25010154},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146739782&doi=10.3390%2fe25010154&partnerID=40&md5=b0847e978d76c452bd3f0a734411c76e},
  abstract = {Kernel methods have played a major role in the last two decades in the modeling and visualization of complex problems in data science. The choice of kernel function remains an open research area and the reasons why some kernels perform better than others are not yet understood. Moreover, the high computational costs of kernel-based methods make it extremely inefficient to use standard model selection methods, such as cross-validation, creating a need for careful kernel design and parameter choice. These reasons justify the prior analyses of kernel matrices, i.e., mathematical objects generated by the kernel functions. This paper explores these topics from an entropic standpoint for the case of kernelized relevance vector machines (RVMs), pinpointing desirable properties of kernel matrices that increase the likelihood of obtaining good model performances in terms of generalization power, as well as relate these properties to the model’s fitting ability. We also derive a heuristic for achieving close-to-optimal modeling results while keeping the computational costs low, thus providing a recipe for efficient analysis when processing resources are limited. © 2023 by the authors.},
  langid = {english},
  keywords = {generalization error,relevance vector machines,von Neumann entropy},
  annotation = {0 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T18:12:49.267Z},
  file = {/home/leix/Documents/Zotero/storage/2X44E7GG/Belanche-Muñoz and Wiejacha - 2023 - Analysis of Kernel Matrices via the von Neumann En.pdf}
}

@unpublished{bellGaussianMeasuresBochner2015,
  title = {Gaussian Measures and {{Bochner}}’s Theorem},
  author = {Bell, Jordan},
  date = {2015-04},
  location = {{Department of Mathematics, University of Toronto}},
  url = {http://individual.utoronto.ca/jordanbell/notes/bochnertheorem.pdf},
  howpublished = {Lecture Notes},
  langid = {english},
  file = {/home/leix/Documents/Zotero/storage/8SJ2B7W4/Bell - Gaussian measures and Bochner’s theorem.pdf}
}

@article{bensusanCasaBatloPasseig2000,
  title = {Casa {{Batló}} Is in {{Passeig}} de {{Gràcia}} or How Landmark Performances Can Describe Tasks},
  author = {Bensusan, Hilan and Giraud-Carrier, Christophe},
  date = {2000-10-05},
  abstract = {Task description is crucial not only every meta-learning enterprise but also to related endeavours like transfer of learning. The paper evaluates the performance of a newly introduced method of task description, landmarking, in a supervised meta-learning scenario. The method relies on correlations between simple and more sophisticated learning algorithms to select the best tool for a task. The results compare favorably with an information-based method and suggest that landmarking is promising. 1 Introduction Task description is the key to use knowledge acquired in some learning tasks elsewhere. We need to describe tasks to induce meta-learning knowledge, to cluster similar tasks together and to perform any kind of supervised meta-learning. Also, task description is a guide to transfer of learning [1, 15]. An inductively good task description is one on which we can rely on to discovers which tasks are relevantly similar. Several approaches to task description can currently be f...},
  keywords = {/unread},
  file = {/home/leix/Documents/Zotero_Papers/TFM/meta-learning/Bensusan_Giraud-Carrier_2000_Casa Batló is in Passeig de Gràcia or how landmark performances can describe.pdf}
}

@article{bezanson2017julia,
  title = {Julia: {{A}} Fresh Approach to Numerical Computing},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B},
  date = {2017},
  journaltitle = {SIAM review},
  volume = {59},
  number = {1},
  pages = {65--98},
  publisher = {{SIAM}},
  url = {https://doi.org/10.1137/141000671},
  file = {/home/leix/Documents/Zotero_Papers/TFM/Implementation/Bezanson et al_2017_Julia.pdf}
}

@article{bilalliPredictivePowerMetafeatures2017,
  title = {On the Predictive Power of Meta-Features in {{OpenML}}},
  author = {Bilalli, Besim and Abelló, Alberto and Aluja-Banet, Tomàs},
  date = {2017-12-20},
  journaltitle = {International Journal of Applied Mathematics and Computer Science},
  volume = {27},
  number = {4},
  pages = {697--712},
  doi = {10.1515/amcs-2017-0048},
  url = {https://sciendo.com/article/10.1515/amcs-2017-0048},
  urldate = {2023-06-03},
  abstract = {Abstract The demand for performing data analysis is steadily rising. As a consequence, people of different profiles (i.e., nonexperienced users) have started to analyze their data. However, this is challenging for them. A key step that poses difficulties and determines the success of the analysis is data mining (model/algorithm selection problem). Meta-learning is a technique used for assisting non-expert users in this step. The effectiveness of meta-learning is, however, largely dependent on the description/characterization of datasets (i.e., meta-features used for meta-learning). There is a need for improving the effectiveness of meta-learning by identifying and designing more predictive meta-features. In this work, we use a method from exploratory factor analysis to study the predictive power of different meta-features collected in OpenML, which is a collaborative machine learning platform that is designed to store and organize meta-data about datasets, data mining algorithms, models and their evaluations. We first use the method to extract latent features, which are abstract concepts that group together meta-features with common characteristics. Then, we study and visualize the relationship of the latent features with three different performance measures of four classification algorithms on hundreds of datasets available in OpenML, and we select the latent features with the highest},
  langid = {english},
  annotation = {23 citations (Crossref) [2023-06-03]},
  file = {/home/leix/Documents/Zotero_Papers/TFM/meta-learning/Bilalli et al_2017_On the predictive power of meta-features in OpenML.pdf}
}

@online{BinaryBuilder,
  title = {{{BinaryBuilder}}},
  url = {https://binarybuilder.org/},
  urldate = {2023-04-16},
  keywords = {/unread},
  file = {/home/leix/Documents/Zotero/storage/RZLZFJIS/binarybuilder.org.html}
}

@article{bishopBayesianNeuralNetworks1997,
  title = {Bayesian {{Neural Networks}}},
  author = {Bishop, Christopher M.},
  date = {1997-07},
  journaltitle = {Journal of the Brazilian Computer Society},
  shortjournal = {J. Braz. Comp. Soc.},
  volume = {4},
  pages = {61--68},
  publisher = {{Sociedade Brasileira de Computação}},
  issn = {0104-6500, 1678-4804},
  doi = {10.1590/S0104-65001997000200006},
  url = {https://www.scielo.br/j/jbcos/a/NYV8LvmFH7pWpC77KZJQwSP/?lang=en},
  urldate = {2023-08-18},
  abstract = {Bayesian techniques have been developed over many years in a range of different fields, but have only recently been applied to the problem of learning in neural networks. As well as providing a consistent framework for statistical pattern recognition, the Bayesian approach offers a number of practical advantages including a solution to the problem of over-fitting. This article provides an introductory overview of the application of Bayesian methods to neural networks. It assumes the reader is familiar with standard feed-forward network models and how to train them using conventional techniques},
  langid = {english},
  keywords = {Bayesian techniques,feedforward networks,statistical pattern recognition},
  file = {/home/leix/Documents/Zotero/storage/I2KMZF87/NYV8LvmFH7pWpC77KZJQwSP.html}
}

@incollection{bishopChapterSinglelayerNetworks1995,
  title = {Chapter 3 - {{Single-layer Networks}}},
  booktitle = {Neural Networks for Pattern Recognition},
  author = {Bishop, C.},
  date = {1995},
  url = {https://www.semanticscholar.org/paper/Neural-networks-for-pattern-recognition-Bishop/b9b1b1654ce0eea729c4160bfedcbb3246460b1d},
  urldate = {2023-08-18},
  abstract = {From the Publisher:  This is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications. Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition.},
  file = {/home/leix/Documents/Zotero/storage/SU4PCPM8/Bishop - 1995 - Neural networks for pattern recognition.pdf}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  date = {2006},
  series = {Information Science and Statistics},
  publisher = {{Springer}},
  location = {{New York}},
  isbn = {978-0-387-31073-2},
  langid = {english},
  pagetotal = {xx+738},
  keywords = {Aprenentatge automàtic,Intel·ligència artificial,Reconeixement de formes (Informàtica)},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-25T00:39:20.216Z},
  file = {/home/leix/Documents/Zotero/storage/CDUBHAHP/Bishop - 2006 - Pattern recognition and machine learning.pdf}
}

@article{blaomMLJJuliaPackage2020,
  title = {{{MLJ}}: {{A Julia}} Package for Composable Machine Learning},
  shorttitle = {{{MLJ}}},
  author = {Blaom, Anthony and Kiraly, Franz and Lienart, Thibaut and Simillides, Yiannis and Arenas, Diego and Vollmer, Sebastian},
  date = {2020-11-07},
  journaltitle = {Journal of Open Source Software},
  shortjournal = {JOSS},
  volume = {5},
  number = {55},
  pages = {2704},
  issn = {2475-9066},
  doi = {10.21105/joss.02704},
  url = {https://joss.theoj.org/papers/10.21105/joss.02704},
  urldate = {2023-03-17},
  keywords = {/unread},
  annotation = {15 citations (Crossref) [2023-03-17]},
  file = {/home/leix/Documents/Zotero_Papers/TFM/Implementation/Blaom et al_2020_MLJ.pdf;/home/leix/Documents/Zotero/storage/H2AM2X97/10.21105.joss.02704.pdf}
}

@incollection{boccatoUnorganizedMachinesTuring2013,
  title = {Unorganized {{Machines}}: {{From Turing}}'s {{Ideas}} to {{Modern Connectionist Approaches}}},
  booktitle = {Nat. {{Comput}}. for {{Simul}}. and {{Knowl}}. {{Discov}}.},
  author = {Boccato, L. and Soares, E.S. and Fernandes, M.M.L.P. and Soriano, D.C. and Attux, R.},
  date = {2013},
  pages = {221--236},
  publisher = {{IGI Global}},
  doi = {10.4018/978-1-4666-4253-9.ch015},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944726616&doi=10.4018%2f978-1-4666-4253-9.ch015&partnerID=40&md5=944790f8abea8fb940326332613a4866},
  abstract = {This work presents a discussion about the relationship between the contributions of Alan Turing-the centenary of whose birth is celebrated in 2012-to the field of artificial neural networks and modern unorganized machines: reservoir computing (RC) approaches and extreme learning machines (ELMs). Firstly, the authors review Turing's connectionist proposals and also expose the fundamentals of the main RC paradigms-echo state networks and liquid state machines,-as well as of the design and training of ELMs. Throughout this exposition, the main points of contact between Turing's ideas and these modern perspectives are outlined, being, then, duly summarized in the second and final part of the work. This paper is useful in offering a distinct appreciation of Turing's pioneering contributions to the field of neural networks and also in indicating some perspectives for the future development of the field that may arise from the synergy between these views. © 2014, IGI Global.},
  isbn = {9781466642553 (ISBN); 9781466642539 (ISBN)},
  langid = {english},
  keywords = {/unread,Connectionist approach,Echo state networks,Extreme learning machine (ELM),Learning systems,Liquid state machines,Neural networks,Points of contact,Reservoir Computing,Turing machines},
  annotation = {Read\_Status: Not Reading Read\_Status\_Date: 2023-02-25T13:04:15.377Z}
}

@unpublished{boneLectureNotesAdvanced2022,
  title = {Lecture Notes on {{Advanced Machine Learning}}},
  author = {Boné, Aleix},
  editora = {Belanche-Muñoz, L.A.},
  editoratype = {collaborator},
  date = {2022},
  howpublished = {Lecture Notes},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-25T18:52:41.021Z},
  file = {/home/leix/Documents/Zotero/storage/VCR4JG98/Boné - 2022 - Lecture notes on Advanced Machine Learning.pdf}
}

@inproceedings{brandersSurvivalAnalysisCox2015,
  title = {Survival Analysis with {{Cox}} Regression and Random Non-Linear Projections},
  booktitle = {Eur. {{Symp}}. {{Artif}}. {{Neural Networks}}, {{ESANN}}},
  author = {Branders, S. and Frénay, B. and Dupont, P.},
  date = {2015},
  pages = {119--124},
  publisher = {{i6doc.com publication}},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961827973&partnerID=40&md5=ba5d4939f1307ac804ec2d9674b3c60a},
  abstract = {Proportional Cox hazard models are commonly used in survival analysis, since they define risk scores which can be directly interpreted in terms of hazards. Yet they cannot account for non-linearities in their covariates. This paper shows how to use random non-linear projections to efficiently address this limitation.},
  isbn = {9782875870148 (ISBN)},
  langid = {english},
  keywords = {/notrelevant,Artificial intelligence,Bioinformatics,Covariates,Cox regression,Hazard models,Hazards,Learning systems,Neural networks,Nonlinear projections,Risk assessment,Risk score,Survival analysis},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-26T20:28:42.314Z},
  file = {/home/leix/Documents/Zotero/storage/RRD88EHQ/Branders et al. - 2015 - Survival analysis with Cox regression and random n.pdf}
}

@article{buntineBayesianBackPropagation1991,
  title = {Bayesian {{Back-Propagation}}},
  author = {Buntine, Wray L. and Weigend, A.},
  date = {1991},
  journaltitle = {Complex Syst.},
  url = {https://www.semanticscholar.org/paper/Bayesian-Back-Propagation-Buntine-Weigend/c83684f6207697c12850db423fd9747572cf1784},
  urldate = {2023-08-18},
  abstract = {Connectionist feed-forward networks, t rained with backpropagat ion, can be used both for nonlinear regression and for (discrete one-of-C ) classification. This paper presents approximate Bayesian meth ods to statistical components of back-propagat ion: choosing a cost funct ion and penalty term (interpreted as a form of prior probability), pruning insignifican t weights, est imat ing the uncertainty of weights, predict ing for new pat terns ("out -of-sample") , est imating the uncertainty in the choice of this predict ion ("erro r bars" ), estimating the generalizat ion erro r, comparing different network st ructures, and handling missing values in the t raining patterns. These methods extend some heurist ic techniques suggested in the literature, and in most cases require a small addit ional facto r in comput at ion during back-propagat ion, or computation once back-pro pagat ion has finished.},
  keywords = {/unread}
}

@inproceedings{caiAutodetectionAnisakidLarvae2016,
  title = {Auto-Detection of {{Anisakid}} Larvae in {{Cod Fillets}} by {{UV}} Fluorescent Imaging with {{OS-ELM}}},
  booktitle = {{{IEEE Reg}} 10 {{Annu Int Conf Proc TENCON}}},
  author = {Cai, W. and Cao, L. and Lin, H. and Sui, J. and Nian, R. and Hu, J. and Lendasse, A.},
  date = {2016},
  volume = {2016-January},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/TENCON.2015.7373102},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962144803&doi=10.1109%2fTENCON.2015.7373102&partnerID=40&md5=baac69655bf7e09f15dfad8caec84cab},
  abstract = {In this paper, one auto-detection scheme of Anisakid larvae in cod fillets is developed on the basis of online sequential extreme learning machine (OS-ELM) in a single hidden layer feedforward neural networks (SLFN). One UV fluorescent imaging system is first set up to collect and extract the typical image patches with and without Anisakid larvae inside the fish muscles, the UV fluorescent image patches are then fed into SLFN sequentially to learn how to nondestructively identify the parasites in real-time, particularly for a growing size of the training set with new observations arrived again and again. It has been shown in the simulation experiments that the developed nondestructive approach could get online auto-detection performance in both good accuracy and efficiency during the test, even for those Anisakid larvae deeply embedded in the cod fillets. © 2015 IEEE.},
  isbn = {21593442 (ISSN); 9781479986415 (ISBN)},
  langid = {english},
  keywords = {Anisakid Larvae,Auto-detection,E-learning,Feedforward neural networks,Fluorescence,Fluorescent imaging,Knowledge acquisition,Learning systems,Network layers,Online sequential extreme learning machine,Online Sequential Extreme Learning Machine,Optical variables measurement,Single Hidden Layer Feedforward Neural Networks,Single-hidden layer feedforward neural networks,UV Fluorescent Imaging},
  annotation = {0 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T12:50:11.622Z},
  file = {/home/leix/Documents/Zotero/storage/ZQRLRY7R/wenqiangcai2015.pdf.pdf}
}

@article{caoLandmarkRecognitionCompact2016,
  title = {Landmark Recognition with Compact {{BoW}} Histogram and Ensemble {{ELM}}},
  author = {Cao, J. and Chen, T. and Fan, J.},
  date = {2016},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimedia Tools Appl},
  volume = {75},
  number = {5},
  pages = {2839--2857},
  publisher = {{Springer New York LLC}},
  issn = {13807501 (ISSN)},
  doi = {10.1007/s11042-014-2424-1},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959916630&doi=10.1007%2fs11042-014-2424-1&partnerID=40&md5=53ca242d7bb053e72f34564f4885e511},
  abstract = {Along with the rapid development of mobile terminal devices, landmark recognition applications based on mobile devices have been widely researched in recent years. Due to the fast response time requirement of mobile users, an accurate and efficient landmark recognition system is thus urgent for mobile applications. In this paper, we propose a landmark recognition framework by employing a novel discriminative feature selection method and the improved extreme learning machine (ELM) algorithm. The scalable vocabulary tree (SVT) is first used to generate a set of preliminary codewords for landmark images. An efficient codebook learning algorithm derived from the word mutual information and Visual Rank technique is proposed to filter out those unimportant codewords. Then, the selected visual words, as the codebook for image encoding, are used to produce a compact Bag-of-Words (BoW) histogram. The fast ELM algorithm and the ensemble approach using the ELM classifier are utilized for landmark recognition. Experiments on the Nanyang Technological University campus’s landmark database and the Fifteen Scene database are conducted to illustrate the advantages of the proposed framework. © 2015, Springer Science+Business Media New York.},
  langid = {english},
  keywords = {Compact BoW histogram,ELM kernel,Elm kernels,Ensemble method,Ensemble methods,Extreme learning machine,Graphic methods,Image coding,Knowledge acquisition,Landmark recognition,Learning algorithms,Learning systems,Mobile devices,Mobile telecommunication systems,Support vector machine,Support vector machines},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-03-09T21:42:11.343Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/elm kernel scopus/Cao et al_2016_Landmark recognition with compact BoW histogram and ensemble ELM.pdf}
}

@inproceedings{caoOptimizationbasedExtremeLearning2014,
  title = {Optimization-Based Extreme Learning Machine with Multi-Kernel Learning Approach for Classification},
  booktitle = {Proc. {{Int}}. {{Conf}}. {{Pattern Recognit}}.},
  author = {Cao, L.-L. and Huang, W.-B. and Sun, F.-C.},
  date = {2014},
  pages = {3564--3569},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/ICPR.2014.613},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919934813&doi=10.1109%2fICPR.2014.613&partnerID=40&md5=6f9f4be0707688866aee22cb5c2d6518},
  abstract = {The optimization method based extreme learning machine (optimization-based ELM) is generalized from single-hidden-layer feed-forward neural networks (SLFNs) by making use of kernels instead of neuron-alike hidden nodes. This approach is known for its high scalability, low computational complexity, and mild optimization constrains. The multi-kernel learning (MKL) framework Simple MKL iteratively determines the combination of kernels by gradient descent wrapping a standard support vector machine (SVM) solver. Simple MKL can be applied to many kinds of supervised learning problems to receive a more stable performance with rapid convergence speed. This paper proposes a new approach: MK-ELM (multi-kernel extreme learning machine) that applies Simple MKL framework to the optimization-based ELM algorithm. The performance analysis on binary classification problems with various scales shows that MK-ELM tends to achieve the best generalization performance as well as being the most insensitive to parameters comparing to optimization-based ELM and Simple MKL. As a result, MK-ELM can be implemented in real applications easily. © 2014 IEEE.},
  isbn = {10514651 (ISSN); 9781479952083 (ISBN)},
  langid = {english},
  keywords = {Binary classification problems,Extreme learning machine (ELM),Feedforward neural networks,Generalization performance,Gradient methods,Knowledge acquisition,Learning systems,Low computational complexity,Multi-kernel extreme learning machine (MK-ELM),Multi-kernel learning,Multi-kernel learning (MKL),Multilayer neural networks,Optimization,Optimization-based ELM,Pattern recognition,SimpleMKL,Single-hidden layer feed-forward neural network,Supervised learning problems,Support vector machines},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-27T18:57:16.045Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/elm kernel scopus/Cao et al_2014_Optimization-based extreme learning machine with multi-kernel learning approach.pdf}
}

@inproceedings{caoReceivedSignalStrength2018,
  title = {Received Signal Strength Based Indoor Localization Using {{ISODATA}} and {{MK-ELM}} Technique},
  booktitle = {Proc. {{IEEE Conf}}. {{Ubiquitous Position}}., {{Indoor Navig}}. {{Location-Based Serv}}., {{UPINLBS}}},
  author = {Cao, Y. and Yan, J.},
  date = {2018},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/UPINLBS.2018.8559789},
  abstract = {With the development of the smart city, indoor localization has received much attentions. In this paper, a novel received signal strength (RSS) based fingerprint localization algorithm was proposed by utilizing iterative self-organizing data analysis techniques algorithm (ISODATA) and multiple kernel extreme learning machine (MK-ELM) technique. In the offline phase, the measurement label of each RSS measurement training data is given after using ISODATA clustering. And then the measurement-label training set and the measurement-position training subsets can be formed. Next, using the MK-ELM algorithm, the measurement classification function and the position regression sub-function can be learned by the measurement-label training set, measurement-position training subset respectively. In the online phase, the classification result of the obtained RSS measurements is obtained firstly. Then the corresponding regression function is chosen for the final position estimation. The experimental results illustrated its performance with respect to position estimation and computational complexity. © 2018 IEEE.},
  isbn = {9781538637555 (ISBN)},
  langid = {english},
  keywords = {/unread,Classification (of information),Classification functions,Classification results,Extreme learning machine (ELM),Indoor localization,Indoor positioning systems,ISODATA,ISODATA Clustering,Iterative methods,Iterative self organizing data analysis techniques algorithms,Knowledge acquisition,Learning systems,Localization algorithm,Location based services,Multiple Kernel Extreme Learning Machine (MK-ELM),Neural networks,Received signal strength,Received Signal Strength (RSS),Telecommunication services},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.983Z}
}

@article{CC01a,
  title = {{{LIBSVM}}: {{A}} Library for Support Vector Machines},
  author = {Chang, Chih-Chung and Lin, Chih-Jen},
  date = {2011},
  journaltitle = {ACM Transactions on Intelligent Systems and Technology},
  volume = {2},
  number = {3},
  pages = {27:1--27:27},
  url = {http://www.csie.ntu.edu.tw/~cjlin/libsvm},
  keywords = {/unread},
  file = {/home/leix/Documents/Zotero/storage/HDH8GLR4/Chang and Lin - 2011 - LIBSVM A library for support vector machines.pdf}
}

@article{changTrainingVSupportVector2002,
  title = {Training V-{{Support Vector Regression}}: {{Theory}} and {{Algorithms}}},
  shorttitle = {Training V-{{Support Vector Regression}}},
  author = {Chang, Chih-Chung and Lin, Chih-Jen},
  date = {2002-08-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {14},
  number = {8},
  pages = {1959--1977},
  issn = {0899-7667},
  doi = {10.1162/089976602760128081},
  url = {https://doi.org/10.1162/089976602760128081},
  urldate = {2023-09-07},
  abstract = {We discuss the relation betweenɛ-support vector regression (ɛ-SVR) and v-support vector regression (v-SVR). In particular, we focus on properties that are different from those of C-support vector classification (C-SVC) andv-support vector classification (v-SVC). We then discuss some issues that do not occur in the case of classification: the possible range of ɛ and the scaling of target values. A practical decomposition method forv-SVR is implemented, and computational experiments are conducted. We show some interesting numerical observations specific to regression.},
  file = {/home/leix/Documents/Zotero/storage/BXCRT5KN/Training-v-Support-Vector-Regression-Theory-and.html}
}

@article{chenDeformedKernelBased2013,
  title = {Deformed Kernel Based Extreme Learning Machine},
  author = {Chen, Z. and Xiong, X.S. and Bing, L.},
  date = {2013},
  journaltitle = {Journal of Computers (Finland)},
  shortjournal = {J. Comput.},
  volume = {8},
  number = {6},
  pages = {1602--1609},
  issn = {1796203X (ISSN)},
  doi = {10.4304/jcp.8.6.1602-1609},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877955047&doi=10.4304%2fjcp.8.6.1602-1609&partnerID=40&md5=b078a1a7a827e60da960bf9e9eb0cef0},
  abstract = {The extreme learning machine (ELM) is a newly emerging supervised learning method. In order to use the information provided by unlabeled samples and improve the performance of the ELM, we deformed the kernel in the ELM by modeling the marginal distribution with the graph Laplacian, which is built with both labeled and unlabeled samples. We further approximated the deformed kernel by means of random feature mapping. The experimental results showed that the proposed semi-supervised extreme learning machine tends to achieve outstanding generalization performance at a relatively faster learning speed than traditional semi-supervised learning algorithms. © 2013 ACADEMY PUBLISHER.},
  langid = {english},
  keywords = {/unread,Extreme learning machine (ELM),Generalization performance,Knowledge acquisition,Learning algorithms,Marginal distribution,Random feature mapping,Random features,Reproducing Kernel Hilbert spaces,Reproducing kernel hilbert spaces (RKHS),Semi-supervised learning,Supervised learning,Supervised learning methods,Unlabeled samples},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.949Z}
}

@article{chenKernelizedSimilarityLearning2023,
  title = {Kernelized {{Similarity Learning}} and {{Embedding}} for {{Dynamic Texture Synthesis}}},
  author = {Chen, S. and Zhang, P. and Xie, G.-S. and Peng, Q. and Cao, Z. and Yuan, W. and You, X.},
  date = {2023},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  shortjournal = {IEEE Trans. Syst. Man Cybern. Syst.},
  volume = {53},
  number = {2},
  pages = {824--837},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {21682216 (ISSN)},
  doi = {10.1109/TSMC.2022.3188500},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135231124&doi=10.1109%2fTSMC.2022.3188500&partnerID=40&md5=51a7ea91fc3fb310f4ec265f84cb62ee},
  abstract = {Dynamic texture (DT) exhibits statistical stationarity in the spatial domain and stochastic repetitiveness in the temporal dimension, indicating that different frames of DT possess a high similarity correlation that is critical prior knowledge. However, existing methods cannot effectively learn a synthesis model for high-dimensional DT from a small number of training samples. In this article, we propose a novel DT synthesis method, which makes full use of similarity as prior knowledge to address this issue. Our method is based on the proposed kernel similarity embedding, which can not only mitigate the high dimensionality and small sample issues, but also has the advantage of modeling nonlinear feature relationships. Specifically, we first put forward two hypotheses that are essential for the DT model to generate new frames using similarity correlations. Then, we integrate kernel learning and the extreme learning machine into a unified synthesis model to learn kernel similarity embeddings for representing DTs. Extensive experiments on DT videos collected from the Internet and two benchmark datasets, i.e., Gatech Graphcut Textures and Dyntex, demonstrate that the learned kernel similarity embeddings can provide discriminative representations for DTs. Further, our method can preserve the long-term temporal continuity of the synthesized DT sequences with excellent sustainability and generalization. Meanwhile, it effectively generates realistic DT videos with higher speed and lower computation than the current state-of-the-art methods. © 2022 IEEE.},
  langid = {english},
  keywords = {/unread,Computational modelling,Correlation,Dynamic texture,Dynamic texture (DT),Dynamic textures,Embeddings,extreme learning machine (ELM),Extreme learning machine (ELM),HTTP,Kernel,kernel similarity embedding,Kernel similarity embedding,Knowledge acquisition,Learning machines,Machine learning,Media streaming,Prior-knowledge,Random processes,Similarity embedding,similarity prior knowledge,Similarity prior knowledge,Stochastic systems,Streaming medium,Textures,Vehicle's dynamics},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.027Z}
}

@article{chenNovelHumanActivity2019,
  title = {A Novel Human Activity Recognition Scheme for Smart Health Using Multilayer Extreme Learning Machine},
  author = {Chen, M. and Li, Y. and Luo, X. and Wang, W. and Wang, L. and Zhao, W.},
  date = {2019},
  journaltitle = {IEEE Internet of Things Journal},
  shortjournal = {IEEE Internet Things J.},
  volume = {6},
  number = {2},
  pages = {1410--1418},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {23274662 (ISSN)},
  doi = {10.1109/JIOT.2018.2856241},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049956888&doi=10.1109%2fJIOT.2018.2856241&partnerID=40&md5=99d179ea45dc9efe46845ed7aad0c475},
  abstract = {In recent years, more and more wearable sensors have been employed in smart health applications. Wearable sensors not only can be used to collect valuable health-related data of their users, they can be also used in conjunction with other infrastructure-bound sensors, such as Microsoft Kinect sensor, to facilitate privacy-aware fine-grained activity tracking. This fusion of multimodal data promises a new type of smart health applications that coach a user to live a healthier life style by monitoring the user in realtime and reminding him or her when he or she engages in an unhealthy activity. In this paper, we investigate how to achieve fine-grained activity recognition in the context of such an application. In our scheme, the identification accuracy is improved by incorporating a nonlinear and local similarity measure, namely kernel risk-sensitive loss, into a novel multilayer neural network learning algorithm, called as stacked extreme learning machine. Furthermore, to achieve a good generalization performance with minimal human intervention, Jaya as a popular optimization algorithm, is also used to adjust key parameters in our proposed approach. The experiments are conducted to verify the effectiveness of the proposed scheme. © 2014 IEEE.},
  langid = {english},
  keywords = {/unread,Action recognition,Extreme learning machine (ELM),Health,Health risks,Image recognition,Intelligent sensors,Internet of things,Jaya,kernel risk-sensitive loss (KRSL),Knowledge acquisition,Learning algorithms,Learning systems,Modal analysis,Multilayer neural networks,Multilayers,Neural networks,Nonhomogeneous media,Personnel training,Risk assessment,smart health,stacked extreme learning machine (S-ELM),Wearable sensors},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.998Z}
}

@article{chenRobustLearningKernel2018,
  title = {Robust Learning with {{Kernel}} Mean P-Power Error Loss},
  author = {Chen, B. and Xing, L. and Wang, X. and Qin, J. and Zheng, N.},
  date = {2018},
  journaltitle = {IEEE Transactions on Cybernetics},
  shortjournal = {IEEE Trans. Cybern.},
  volume = {48},
  number = {7},
  pages = {2101--2113},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {21682267 (ISSN)},
  doi = {10.1109/TCYB.2017.2727278},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028918804&doi=10.1109%2fTCYB.2017.2727278&partnerID=40&md5=beb5f63b8aca3e03583ffbbcf49f4c6d},
  abstract = {Correntropy is a second order statistical measure in kernel space, which has been successfully applied in robust learning and signal processing. In this paper, we define a nonsecond order statistical measure in kernel space, called the kernel mean-p power error (KMPE), including the correntropic loss (C-Loss) as a special case. Some basic properties of KMPE are presented. In particular, we apply the KMPE to extreme learning machine (ELM) and principal component analysis (PCA), and develop two robust learning algorithms, namely ELM-KMPE and PCA-KMPE. Experimental results on synthetic and benchmark data show that the developed algorithms can achieve better performance when compared with some existing methods. © 2013 IEEE.},
  langid = {english},
  keywords = {/unread,algorithm,article,Benchmarking,Correlation methods,Errors,Extraterrestrial measurements,Extreme learning machine (ELM),Kernel,kernel mean p-power error (KMPE),Knowledge acquisition,learning algorithm,Learning algorithms,Learning systems,Loss measurement,Measurement uncertainty,Power errors,principal component analysis,Principal component analysis,principal component analysis (PCA),robust learning,Robust learning,Robustness (control systems),Signal processing,Uncertainty analysis},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.983Z}
}

@article{choDetectionStressLevels2017,
  title = {Detection of Stress Levels from Biosignals Measured in Virtual Reality Environments Using a Kernel-Based Extreme Learning Machine},
  author = {Cho, D. and Ham, J. and Oh, J. and Park, J. and Kim, S. and Lee, N.-K. and Lee, B.},
  date = {2017},
  journaltitle = {Sensors (Switzerland)},
  shortjournal = {Sensors},
  volume = {17},
  number = {10},
  publisher = {{MDPI AG}},
  issn = {14248220 (ISSN)},
  doi = {10.3390/s17102435},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032574814&doi=10.3390%2fs17102435&partnerID=40&md5=5094fcc565f911db790333ef95370086},
  abstract = {Virtual reality (VR) is a computer technique that creates an artificial environment composed of realistic images, sounds, and other sensations. Many researchers have used VR devices to generate various stimuli, and have utilized them to perform experiments or to provide treatment. In this study, the participants performed mental tasks using a VR device while physiological signals were measured: a photoplethysmogram (PPG), electrodermal activity (EDA), and skin temperature (SKT). In general, stress is an important factor that can influence the autonomic nervous system (ANS). Heart-rate variability (HRV) is known to be related to ANS activity, so we used an HRV derived from the PPG peak interval. In addition, the peak characteristics of the skin conductance (SC) from EDA and SKT variation can also reflect ANS activity; we utilized them as well. Then, we applied a kernel-based extreme-learning machine (K-ELM) to correctly classify the stress levels induced by the VR task to reflect five different levels of stress situations: baseline, mild stress, moderate stress, severe stress, and recovery. Twelve healthy subjects voluntarily participated in the study. Three physiological signals were measured in stress environment generated by VR device. As a result, the average classification accuracy was over 95\% using K-ELM and the integrated feature (IT = HRV + SC + SKT). In addition, the proposed algorithm can embed a microcontroller chip since K-ELM algorithm have very short computation time. Therefore, a compact wearable device classifying stress levels using physiological signals can be developed. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.},
  langid = {english},
  keywords = {/unread,adult,algorithm,article,Artificial environments,autonomic nervous system,Autonomic nervous system,Autonomic nervous system (ANS),Biomedical signal processing,Classification accuracy,clinical article,controlled study,E-learning,Electrodermal activity,Electronic design automation,Extreme learning machine (ELM),female,Heart,heart rate variability,Heart rate variability,Heart rate variability (HRV),human,human experiment,Kernel-based extreme learning machine (K-ELM),Knowledge acquisition,Learning systems,machine learning,male,mental task,Physiological signals,Physiology,skin conductance,skin temperature,stress,virtual reality,Virtual reality,Virtual reality (VR),Virtual-reality environment},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.974Z}
}

@article{choLargemarginClassificationInfinite2010,
  title = {Large-Margin Classification in Infinite Neural Networks},
  author = {Cho, Youngmin and Saul, Lawrence K.},
  date = {2010-10},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Comput},
  volume = {22},
  number = {10},
  eprint = {20608866},
  eprinttype = {pmid},
  pages = {2678--2697},
  issn = {1530-888X},
  doi = {10.1162/NECO_a_00018},
  abstract = {We introduce a new family of positive-definite kernels for large margin classification in support vector machines (SVMs). These kernels mimic the computation in large neural networks with one layer of hidden units. We also show how to derive new kernels, by recursive composition, that may be viewed as mapping their inputs through a series of nonlinear feature spaces. These recursively derived kernels mimic the computation in deep networks with multiple hidden layers. We evaluate SVMs with these kernels on problems designed to illustrate the advantages of deep architectures. Compared to previous benchmarks, we find that on some problems, these SVMs yield state-of-the-art results, beating not only other SVMs but also deep belief nets.},
  langid = {english},
  keywords = {/important,Algorithms,Artificial Intelligence,{Neural Networks, Computer},Nonlinear Dynamics,{Pattern Recognition, Automated},{Signal Processing, Computer-Assisted}},
  annotation = {Read\_Status: To Read Read\_Status\_Date: 2023-03-10T18:26:14.097Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/kernels/Cho_Saul_2010_Large-margin classification in infinite neural networks.pdf}
}

@book{cholletDeepLearningPython2018,
  title = {Deep Learning with {{Python}}},
  author = {Chollet, François},
  date = {2018},
  publisher = {{Manning Publications Co.}},
  location = {{Shelter Island, New York}},
  isbn = {978-1-61729-443-3},
  langid = {english},
  pagetotal = {xxi+361},
  keywords = {Aprenentatge automàtic,Python (Llenguatge de programació),Xarxes neuronals (Informàtica)},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T15:37:43.545Z},
  file = {/home/leix/Documents/Zotero/storage/KTSSW42I/Chollet - 2018 - Deep learning with Python.pdf}
}

@article{chorowskiReviewPerformanceComparison2014,
  title = {Review and Performance Comparison of {{SVM-}} and {{ELM-based}} Classifiers},
  author = {Chorowski, J. and Wang, J. and Zurada, J.M.},
  date = {2014},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {128},
  pages = {507--516},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2013.08.009},
  abstract = {This paper presents how commonly used machine learning classifiers can be analyzed using a common framework of convex optimization. Four classifier models, the Support Vector Machine (SVM), the Least-Squares SVM (LSSVM), the Extreme Learning Machine (ELM), and the Margin Loss ELM (MLELM) are discussed to demonstrate how specific parametrizations of a general problem statement affect the classifier design and performance, and how ideas from the four different classifiers can be mixed and used together. Furthermore, 21 public domain benchmark datasets are used to experimentally evaluate five performance metrics of each model and corroborate the theoretical analysis. Comparison of classification accuracies under a nested cross-validation evaluation shows that with an exception all four models perform similarly on the evaluated datasets. However, the four classifiers command different amounts of computational resources for both testing and training. These requirements are directly linked to their formulations as different convex optimization problems. © 2013 Elsevier B.V.},
  langid = {english},
  keywords = {accuracy,algorithm,article,classification,Classification,Classification (of information),Classification accuracy,classifier,Convex optimization,Convex optimization problems,Convex quadratic programming,Data processing,ELM,Extreme learning machine (ELM),Learning systems,Least Square Support Vector Machine,linear system,machine learning,Machine learning,margin loss extreme learning machine,mathematical computing,mathematical model,Nested cross validations,nonlinear system,priority journal,Randomization,regression analysis,support vector machine,Support vector machines,SVM},
  annotation = {122 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-25T13:20:38.811Z},
  file = {/home/leix/Documents/Zotero/storage/6HB5WWIF/Chorowski et al. - 2014 - Review and performance comparison of SVM- and ELM-.pdf}
}

@article{CourseAbstractHarmonic1995,
  title = {A Course in Abstract Harmonic Analysis},
  date = {1995-01-01},
  url = {https://typeset.io/papers/a-course-in-abstract-harmonic-analysis-403v50r41j},
  urldate = {2023-02-25},
  abstract = {Banach Algebras and Spectral Theory Banach Algebras: Basic Concepts Gelfand Theory Nonunital Banach Algebras The Spectral Theorem Spectral Theory of *-Representations Von Neumann Algebras Notes and References Locally Compact Groups Topological Groups Haar Measure Interlude: Some Technicalities The Modular Function Convolutions Homogeneous Spaces Notes and References Basic Representation Theory Unitary Representations Representations of a Group and Its Group Algebra Functions of Positive Type Notes and References Analysis on Locally Compact Abelian Groups The Dual Group The Fourier Transform The Pontrjagin Duality Theorem Representations of Locally Compact Abelian Groups Closed Ideals in L1(G) Spectral Synthesis The Bohr Compactification Notes and References Analysis on Compact Groups Representations of Compact Groups The Peter-Weyl Theorem Fourier Analysis on Compact Groups Examples Notes and References Induced Representations The Inducing Construction The Frobenius Reciprocity Theorem Pseudomeasures and Induction in Stages Systems of Imprimitivity The Imprimitivity Theorem Introduction to the Mackey Machine Examples: The Classics More Examples, Good and Bad Notes and References Further Topics in Representation Theory The Group C* Algebra The Structure of the Dual Space Tensor Products of Representations Direct Integral Decompositions The Plancherel Theorem Examples Appendices A Hilbert Space Miscellany Trace-Class and Hilbert-Schmidt Operators Tensor Products of Hilbert Spaces Vector-Valued Integrals},
  langid = {english},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-03-08T17:20:18.445Z},
  file = {/home/leix/Documents/Zotero/storage/A2E5RS55/1995 - A course in abstract harmonic analysis.pdf;/home/leix/Documents/Zotero/storage/NPWCYLTI/a-course-in-abstract-harmonic-analysis-403v50r41j.html}
}

@software{crameriScientificColourMaps2023,
  title = {Scientific Colour Maps},
  author = {Crameri, Fabio},
  date = {2023-06-14},
  doi = {10.5281/zenodo.8035877},
  url = {https://zenodo.org/record/8035877},
  urldate = {2023-10-04},
  abstract = {Suite of scientific, colour-vision deficiency friendly~and~perceptually-uniform colour maps (www.fabiocrameri.ch/colourmaps) that include all readers and~significantly reduce visual errors. Book graphic design masterclasses on how to best use colour in a scientific context via www.fabiocrameri.ch/masterclasses. Commission professional graphic design support via Undertone.design. Support the underlying cause and the~development of the Scientific colour maps via~www.fabiocrameri.ch/products.},
  organization = {{Zenodo}},
  keywords = {batlow,color gradient,color map,color palette,color perception,color scheme,colormap,colour palettes,colour schemes,colour vision deficiency,CVD,data representation,perceptually uniform,scientific colormap,scientific colour maps,scientific colourmap,scientific visualisation,visualisation,visualization},
  file = {/home/leix/Documents/Zotero/storage/F9E35IXI/8035877.html}
}

@article{datserisDrWatsonPerfectSidekick2020a,
  title = {{{DrWatson}}: The Perfect Sidekick for Your Scientific Inquiries},
  shorttitle = {{{DrWatson}}},
  author = {Datseris, George and Isensee, Jonas and Pech, Sebastian and Gál, Tamás},
  date = {2020-10-29},
  journaltitle = {Journal of Open Source Software},
  volume = {5},
  number = {54},
  pages = {2673},
  issn = {2475-9066},
  doi = {10.21105/joss.02673},
  url = {https://joss.theoj.org/papers/10.21105/joss.02673},
  urldate = {2023-08-27},
  abstract = {Datseris et al., (2020). DrWatson: the perfect sidekick for your scientific inquiries. Journal of Open Source Software, 5(54), 2673, https://doi.org/10.21105/joss.02673},
  langid = {english},
  file = {/home/leix/Documents/Zotero_Papers/TFM/Reproducibility/Datseris et al_2020_DrWatson.pdf}
}

@inproceedings{deepikaHistogramOrientedGradients2018,
  title = {Histogram of {{Oriented Gradients Based Reduced Feature}} for {{Traffic Sign Recognition}}},
  booktitle = {Int. {{Conf}}. {{Adv}}. {{Comput}}., {{Commun}}. {{Inf}}., {{ICACCI}}},
  author = {{Deepika} and Vashisth, S. and Saurav, S.},
  date = {2018},
  pages = {2206--2212},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/ICACCI.2018.8554624},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060018022&doi=10.1109%2fICACCI.2018.8554624&partnerID=40&md5=09f0ab513e96b1e8ecd8863b92cd2f0d},
  abstract = {Vehicles equipped with automatic traffic sign recognition (TSR) capability has recently attained a lot of attention among computer vision community. There has been a need for a robust TSR which can provide the current state of traffic signs on the road which will in turn assists drivers, autonomous vehicles, and mobile robots. An efficient and effective TSR could facilitate drivers in taking necessary decisions in advance which will ultimately reduce many fatal road accidents and deaths which takes place due to lack of road sign awareness among the drivers. Although there has been a lot of research in this direction, the design of a robust and an efficient TSR system is still an unsolved problem. Therefore, in this work, we have proposed an efficient method of TSR using Histogram of Oriented Gradients (HOG). We have tried to exploit the usefulness of HOG features for TSR by selecting a proper set of parameters which results in generating more discriminative features. Further, dimensionality reduction using Principal Component Analysis (PCA) has also been carried out to reduce the number of redundant features. Finally, the reduced features have been classified using a Kernel Extreme Learning Machine (K-ELM) classifier. Numerous experiments have been performed using GTSRB traffic sign recognition benchmark dataset and results show the effectiveness of the proposed approach. © 2018 IEEE.},
  isbn = {9781538653142 (ISBN)},
  langid = {english},
  keywords = {/unread,Adaptive histogram equalization,CLAHE (Contrast Limited Adaptive Histogram Equalization),Equalizers,Extreme learning machine (ELM),Graphic methods,Histogram Equalization (HE),Histogram equalizations,Histogram of oriented gradients (HOG),Histogram of Oriented Gradients (HOG),Kernel Extreme Learning Machine (K-ELM),Knowledge acquisition,Learning systems,Neural networks,Pattern recognition,Principal component analysis,Principal Component Analysis (PCA),Roads and streets,Traffic sign recognition,Traffic Sign Recognition,Traffic signs},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.984Z}
}

@article{dietterichApproximateStatisticalTests1998,
  title = {Approximate {{Statistical Tests}} for {{Comparing Supervised Classification Learning Algorithms}}},
  author = {Dietterich, T. G.},
  date = {1998-09-15},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Comput},
  volume = {10},
  number = {7},
  eprint = {9744903},
  eprinttype = {pmid},
  pages = {1895--1923},
  issn = {1530-888X},
  doi = {10.1162/089976698300017197},
  abstract = {This article reviews five approximate statistical tests for determining whether one learning algorithm outperforms another on a particular learning task. These tests are compared experimentally to determine their probability of incorrectly detecting a difference when no difference exists (type I error). Two widely used statistical tests are shown to have high probability of type I error in certain situations and should never be used: a test for difference of two proportions and a paired-differences t test based on taking several random train-test splits. A third test, a paired-differences t test based on 10-fold cross-validation, exhibits somewhat elevated probability of type I error. A fourth test, McNemar's test, is shown to have low type I error. The fifth test is a new test, 5 x 2 cv, based on five iterations of twofold cross-validation. Experiments show that this test also has acceptable type I error. The article also measures the power (ability to detect algorithm differences when they do exist) of these tests. The cross-validated t test is the most powerful. The 5 x 2 cv test is shown to be slightly more powerful than McNemar's test. The choice of the best test is determined by the computational cost of running the learning algorithm. For algorithms that can be executed only once, McNemar's test is the only test with acceptable type I error. For algorithms that can be executed 10 times, the 5 x 2 cv test is recommended, because it is slightly more powerful and because it directly measures variation due to the choice of training set.},
  langid = {english},
  annotation = {2027 citations (Crossref) [2023-04-10]},
  file = {/home/leix/Documents/Zotero/storage/VMDEM9F3/dietterich1998.pdf.pdf}
}

@article{dingKernelBasedOnline2018,
  title = {Kernel Based Online Learning for Imbalance Multiclass Classification},
  author = {Ding, S. and Mirza, B. and Lin, Z. and Cao, J. and Lai, X. and Nguyen, T.V. and Sepulveda, J.},
  date = {2018},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {277},
  pages = {139--148},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2017.02.102},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028702271&doi=10.1016%2fj.neucom.2017.02.102&partnerID=40&md5=5056f6851c14ba432467430eb6cbf1d9},
  abstract = {In this paper, we propose a weighted online sequential extreme learning machine with kernels (WOS-ELMK) for class imbalance learning (CIL). The existing online sequential extreme learning machine (OS-ELM) methods for CIL use random feature mapping. WOS-ELMK is the first OS-ELM method which uses kernel mapping for online class imbalance learning. The kernel mapping avoids the non-optimal hidden node problem associated with weighted OS-ELM (WOS-ELM) and other existing OS-ELM methods for CIL. WOS-ELMK tackles both the binary class and multiclass imbalance problems in one-by-one as well as chunk-by-chunk learning modes. For imbalanced big data streams, a fixed size window scheme is also implemented for WOS-ELMK. We empirically show that WOS-ELMK obtains superior performance in general than some recently proposed CIL approaches on 17 binary class and 8 multiclass imbalanced datasets. © 2017},
  langid = {english},
  keywords = {/unread,Article,Class imbalance,class imbalance learning,classification algorithm,Data streams,E-learning,Extreme learning machine (ELM),Kernel learning,Knowledge acquisition,machine learning,Machine learning,Mapping,mathematical model,Multiclass,Online learning,priority journal,weighted online sequential extreme learning machine with kernel},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.985Z}
}

@article{dingRandomCompactGaussian2021,
  title = {Random Compact {{Gaussian}} Kernel: {{Application}} to {{ELM}} Classification and Regression[{{Formula}} Presented]},
  author = {Ding, X. and Liu, J. and Yang, F. and Cao, J.},
  date = {2021},
  journaltitle = {Knowledge-Based Systems},
  shortjournal = {Knowl Based Syst},
  volume = {217},
  publisher = {{Elsevier B.V.}},
  issn = {09507051 (ISSN)},
  doi = {10.1016/j.knosys.2021.106848},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100804918&doi=10.1016%2fj.knosys.2021.106848&partnerID=40&md5=ca300cceb570d21d05cf3d52aad37414},
  abstract = {Extreme learning machine (ELM) kernels using random feature mapping have recently gained a lot of popularity because they require low human supervision. However, the superiority in the mapping mechanism of ELM kernels is accompanied by a higher computation cost, rendering the kernel learning algorithms hard to tackle large scale learning tasks. On the other hand, the implicit mapping used in the conventional Gaussian kernel is computationally cheaper than the explicit computation of the ELM kernel, but requires trivial human intervention. This paper proposes to merge both properties by defining a new kernel, the random compact Gaussian (RCG) kernel. The random feature mapping property enables RCG kernel to save parameters selection time, while the implicitly mapping property enables RCG kernel to save kernel calculation time. The proposed kernel works by scaling one kernel parameter in the conventional Gaussian kernel to multiple kernel parameters, and generating all parameters randomly based on a continuous probability distribution. We prove that the RCG kernel is a Mercer kernel. The kernel is calculated implicitly before seeing the training samples and used to train ELMs. The experiments on 25 binary classification and regression benchmark problems show that the RCG kernel typically outperforms other competitive kernels. Compared to ELM kernel, RCG kernel not only achieves the better generalization performance on most datasets, but also needs much less kernel calculation cost. In addition, the sensitivity analysis of the kernel parameters of k-fold cross validation is conducted and the results show that the RCG kernel is robust and stable for repeated trials. © 2021 Elsevier B.V.},
  langid = {english},
  keywords = {Bench-mark problems,Binary classification,Continuous probability distribution,ELM kernel,Extreme learning machine (ELM),Gaussian distribution,Generalization performance,K fold cross validations,Kernel learning,Large-scale learning,Learning algorithms,Learning systems,Mapping,Mercer kernel,Parameters selection,Random compact Gaussian,Sensitivity analysis},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-27T19:03:44.472Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/elm kernel scopus/Ding et al_2021_Random compact Gaussian kernel.pdf}
}

@online{DirenvUnclutterYoura,
  title = {Direnv – Unclutter Your .Profile},
  url = {https://direnv.net/},
  urldate = {2023-08-27},
  abstract = {unclutter your .profile},
  langid = {american},
  organization = {{direnv}},
  file = {/home/leix/Documents/Zotero/storage/B3YIYP74/direnv.net.html}
}

@book{dobsonIntroductionGeneralizedLinear2018,
  title = {An {{Introduction}} to {{Generalized Linear Models}}},
  author = {Dobson, Annette J. and Barnett, Adrian G.},
  date = {2018-04-11},
  edition = {4th edition},
  publisher = {{Chapman and Hall/CRC}},
  location = {{Boca Raton}},
  isbn = {978-1-138-74151-5},
  langid = {english},
  pagetotal = {376},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T15:37:43.548Z},
  file = {/home/leix/Documents/Zotero/storage/VPC7H4PJ/Dobson and Barnett - 2018 - An Introduction to Generalized Linear Models.pdf}
}

@inproceedings{elizabethraniLoadAllocationQuality2021,
  title = {Load {{Allocation}} as {{Quality}} and Secured in {{Mobile Cloud Networking Location}}},
  booktitle = {J. {{Phys}}. {{Conf}}. {{Ser}}.},
  author = {Elizabeth Rani, G. and Ajay Sukumar, G.V. and Umesh Chandra, T. and Anki Reddy, K. and Sakthimohan, M.},
  editor = {{Rajesh M.} and {Gnanasekar J.M.} and {Sitharthan R.}},
  date = {2021},
  volume = {1979},
  number = {1},
  publisher = {{IOP Publishing Ltd}},
  doi = {10.1088/1742-6596/1979/1/012045},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112475290&doi=10.1088%2f1742-6596%2f1979%2f1%2f012045&partnerID=40&md5=bb77a9ed35a83af65f44932f500ce8b9},
  abstract = {In portable cloud organizations (MCNs), a versatile client is associated with a cloud worker through an organization entryway, which is answerable for giving the necessary nature of-administration (QoS) to the clients. On the off chance that a client expands its administration interest, the associating passage may neglect to give the mentioned QoS because of the over-burden interest, while different entryways remain underloaded. Because of the expansion in burden in one Gateway, the sharing of burden among all the passages is one of the imminent answers for giving QoS-ensured administrations to the portable clients. Moreover, if a client makes trouble, the circumstance turns out to be all the more testing. In this paper, we address the issue of QoS-ensured made sure about help provisioning in MCNs. We plan a utility boost issue for quality-guaranteed made sure about burden sharing (QuaLShare) in MCN, and decide its ideal arrangement utilizing sell off hypothesis. In QuaLShare, the over-burden passage identifies the getting out of hand Gateways, and, at that point, keeps them from partaking in the closeout cycle. Hypothetically, we portray both the issue and the arrangement approaches in a MCN climate. At last, we explore the presence of Nash Equilibrium of the proposed plot. We expand the answer for the instance of different clients, trailed by hypothetical examination. Mathematical investigation sets up the accuracy of the proposed calculations. © Published under licence by IOP Publishing Ltd.},
  isbn = {17426588 (ISSN)},
  langid = {english},
  keywords = {/notrelevant,Auction theory,Bandwidth distribution,Load allocation,Load sharing,Mobile cloud computing,Mobile clouds,Nash equilibria,Nash equilibrium,Physics,Quality guaranteed},
  annotation = {0 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T17:55:13.219Z},
  file = {/home/leix/Documents/Zotero/storage/75G8DIGK/Elizabeth Rani et al. - 2021 - Load Allocation as Quality and secured in Mobile C.pdf}
}

@online{FastDeclarativeReproducible,
  title = {Fast, {{Declarative}}, {{Reproducible}}, and {{Composable Developer Environments}}},
  url = {https://devenv.sh/},
  urldate = {2023-08-27},
  abstract = {Fast, Declarative, Reproducible, and Composable Developer Environments using Nix},
  langid = {english},
  file = {/home/leix/Documents/Zotero/storage/3MH55PHY/devenv.sh.html}
}

@article{frenayParameterinsensitiveKernelExtreme2011,
  title = {Parameter-Insensitive Kernel in Extreme Learning for Non-Linear Support Vector Regression},
  author = {Frénay, Benoît and Verleysen, Michel},
  date = {2011-09-01},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  series = {Advances in {{Extreme Learning Machine}}: {{Theory}} and {{Applications}}},
  volume = {74},
  number = {16},
  pages = {2526--2531},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2010.11.037},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231211002591},
  abstract = {Support vector regression (SVR) is a state-of-the-art method for regression which uses the ε‐sensitive loss and produces sparse models. However, non-linear SVRs are difficult to tune because of the additional kernel parameter. In this paper, a new parameter-insensitive kernel inspired from extreme learning is used for non-linear SVR. Hence, the practitioner has only two meta-parameters to optimise. The proposed approach reduces significantly the computational complexity yet experiments show that it yields performances that are very close from the state-of-the-art. Unlike previous works which rely on Monte-Carlo approximation to estimate the kernel, this work also shows that the proposed kernel has an analytic form which is computationally easier to evaluate.},
  langid = {english},
  keywords = {/important,article,Computational complexity,controlled study,ELM kernel,Extreme learning machine (ELM),Infinite number of neurons,Infinite numbers,intermethod comparison,kernel method,Kernel parameter,mathematical computing,MONTE CARLO,Non-linear,nonlinear system,parametric test,priority journal,Regression analysis,State-of-the-art methods,support vector machine,Support vector regression,Support vector regressions},
  annotation = {90 citations (Semantic Scholar/DOI) [2023-02-21] 72 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-25T12:23:16.333Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/kernels/Frénay_Verleysen_2011_Parameter-insensitive kernel in extreme learning for non-linear support vector.pdf;/home/leix/Documents/Zotero/storage/QNW47R8T/S0925231211002591.html}
}

@article{frenayReinforcedExtremeLearning2016,
  title = {Reinforced {{Extreme Learning Machines}} for {{Fast Robust Regression}} in the {{Presence}} of {{Outliers}}},
  author = {Frénay, B. and Verleysen, M.},
  date = {2016},
  journaltitle = {IEEE Transactions on Cybernetics},
  shortjournal = {IEEE Trans. Cybern.},
  volume = {46},
  number = {12},
  pages = {3351--3363},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {21682267 (ISSN)},
  doi = {10.1109/TCYB.2015.2504404},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027723377&doi=10.1109%2fTCYB.2015.2504404&partnerID=40&md5=e8c8255f408aea023a81e3abbe1cb23e},
  abstract = {Extreme learning machines (ELMs) are fast methods that obtain state-of-the-art results in regression. However, they are not robust to outliers and their meta-parameter (i.e., the number of neurons for standard ELMs and the regularization constant of output weights for L2-regularized ELMs) selection is biased by such instances. This paper proposes a new robust inference algorithm for ELMs which is based on the pointwise probability reinforcement methodology. Experiments show that the proposed approach produces results which are comparable to the state of the art, while being often faster. © 2015 IEEE.},
  langid = {english},
  keywords = {Extreme learning machine (ELM),Inference engines,Knowledge acquisition,Learning systems,outliers,Point wise,pointwise probability reinforcements (PPRs),regularization,Reinforcement,robust inference,Robust inference,Statistics},
  annotation = {13 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T13:01:06.346Z},
  file = {/home/leix/Documents/Zotero/storage/2UHJ55PJ/frenay2015.pdf.pdf;/home/leix/Documents/Zotero/storage/MGIDX2CI/frenay2015.pdf.pdf}
}

@inproceedings{frenayUsingSVMsRandomised2010,
  title = {Using {{SVMs}} with Randomised Feature Spaces: An Extreme Learning Approach},
  shorttitle = {Using {{SVMs}} with Randomised Feature Spaces},
  author = {Frénay, Benoît and Verleysen, Michel},
  date = {2010-01-01},
  abstract = {Extreme learning machines are fast models which almost compare to standard SVMs in terms of accuracy, but are much faster. However, they optimise a sum of squared errors whereas SVMs are maximum-margin classifiers. This paper proposes to merge both approaches by defining a new kernel. This kernel is computed by the first layer of an extreme learning machine and used to train a SVM. Experiments show that this new kernel compares to the standard RBF kernel in terms of accuracy and is faster. Indeed, experiments show that the number of neurons of the ELM behind the randomised kernel does not need to be tuned and can be set to a sufficient value without altering the accuracy significantly. 1},
  keywords = {/unread},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:06.187Z},
  file = {/home/leix/Documents/Zotero/storage/LYWUVJ2K/Frénay and Verleysen - 2010 - Using SVMs with randomised feature spaces an extr.pdf}
}

@article{fuASELMAdaptiveSemisupervised2016,
  title = {{{ASELM}}: {{Adaptive}} Semi-Supervised {{ELM}} with Application in Question Subjectivity Identification},
  author = {Fu, H. and Niu, Z. and Zhang, C. and Yu, H. and Ma, J. and Chen, J. and Chen, Y. and Liu, J.},
  date = {2016},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {207},
  pages = {599--609},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2016.05.041},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977590895&doi=10.1016%2fj.neucom.2016.05.041&partnerID=40&md5=7590ef671c5eaa818e6d686262d317cb},
  abstract = {Question subjectivity identification in Community Question Answering (CQA) has attracted a lot of attentions in recent years. With the rapid development of CQA, subjective questions posted by users are growing exponentially, which presents two challenges for question subjectivity identification. The first one is the data imbalance between subjective and objective questions. The second one is that the amount of manually labelled training data is hard to catch up with the fast developing speed of CQA. In this paper, we propose an adaptive semi-supervised Extreme Learning Machine (ASELM) to solve those two challenges. To resolve the data imbalance problem, ASELM employs the different impacts on identification performance caused by the imbalanced data. Second, the proposed method introduces the unlabelled data, and builds a model about the ratio between the number of labelled and unlabelled data based on Gaussian Model, which is applied to automatically generate the constraint on the unlabelled data. Experimental results showed ASELM improved identification performance for the imbalanced data, and outperformed the performance of basic ELM, SELM, Weighted ELM and SS-ELM on both F1 measure and accuracy. © 2016 Elsevier B.V.},
  langid = {english},
  keywords = {/unread,adaptive semi supervised extreme learning machine,Adaptive semi-supervised ELM,Article,classification algorithm,community question answering,Community question answering,Computer applications,data analysis,Data imbalance,data synthesis,ELM,Extreme learning machine (ELM),Gaussian model,Imbalanced data,information processing,intermethod comparison,kernel method,learning algorithm,Learning systems,machine learning,measurement accuracy,Neural networks,performance,priority journal,question subjectivity identification,Question subjectivity identification,Semi-supervised,Training data},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.967Z}
}

@article{fuFastDetectionImpact2016,
  title = {Fast Detection of Impact Location Using Kernel Extreme Learning Machine},
  author = {Fu, H. and Vong, C.-M. and Wong, P.-K. and Yang, Z.},
  date = {2016},
  journaltitle = {Neural Computing and Applications},
  shortjournal = {Neural Comput. Appl.},
  volume = {27},
  number = {1},
  pages = {121--130},
  publisher = {{Springer-Verlag London Ltd}},
  issn = {09410643 (ISSN)},
  doi = {10.1007/s00521-014-1568-2},
  abstract = {Damage location detection has direct relationship with the field of aerospace structure as the detection system can inspect any exterior damage that may affect the operations of the equipment. In the literature, several kinds of learning algorithms have been applied in this field to construct the detection system and some of them gave good results. However, most learning algorithms are time-consuming due to their computational complexity so that the real-time requirement in many practical applications cannot be fulfilled. Kernel extreme learning machine (kernel ELM) is a learning algorithm, which has good prediction performance while maintaining extremely fast learning speed. Kernel ELM is originally applied to this research to predict the location of impact event on a clamped aluminum plate that simulates the shell of aerospace structures. The results were compared with several previous work, including support vector machine (SVM), and conventional back-propagation neural networks (BPNN). The comparison result reveals the effectiveness of kernel ELM for impact detection, showing that kernel ELM has comparable accuracy to SVM but much faster speed on current application than SVM and BPNN. © 2014, Springer-Verlag London.},
  langid = {english},
  keywords = {/unread,Aerospace structure,Back-propagation neural networks,Backpropagation,Complex networks,Damage detection,Damage location,Damage location detection,Extreme learning machine (ELM),Kernel ELM,Knowledge acquisition,Learning algorithms,Learning systems,Location,Neural networks,Plate structure,Plates (structural components),Prediction performance,Real time requirement,Support vector machines},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.968Z}
}

@article{gautamAdaptiveOnlineLearning2021,
  title = {Adaptive {{Online Learning}} with {{Regularized Kernel}} for {{One-Class Classification}}},
  author = {Gautam, C. and Tiwari, A. and Suresh, S. and Ahuja, K.},
  date = {2021},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  shortjournal = {IEEE Trans. Syst. Man Cybern. Syst.},
  volume = {51},
  number = {3},
  pages = {1917--1932},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {21682216 (ISSN)},
  doi = {10.1109/TSMC.2019.2907672},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101196948&doi=10.1109%2fTSMC.2019.2907672&partnerID=40&md5=6ea1ff6454d4e234b51e85b75356d40e},
  abstract = {In the past few years, kernel-based one-class extreme learning machine (ELM) receives quite a lot of attention by researchers for offline/batch learning due to its noniterative and fast learning capability. This paper extends this concept for adaptive online learning with regularized kernel-based one-class ELM classifiers for detection of outliers, and are collectively referred to as ORK-OCELM. Two frameworks, viz., boundary and reconstruction, are presented to detect the target class in ORK-OCELM. The kernel hyperplane-based baseline one-class ELM model considers whole data in a single chunk, however, the proposed one-class classifiers are adapted in an online fashion from the stream of training samples. The performance of ORK-OCELM is evaluated on a standard benchmark as well as synthetic datasets for both types of environments, i.e., stationary and nonstationary. While evaluating on stationary datasets, these classifiers are compared against batch learning-based one-class classifiers. Similarly, while evaluating on nonstationary datasets, the comparison is done with incremental learning-based online one-class classifiers. The results indicate that the proposed classifiers yield better or similar outcomes for both. In the nonstationary dataset evaluation, adaptability of the proposed classifiers in a changing environment is also demonstrated. It is further shown that the proposed classifiers have large stream data handling capability even under limited system memory. Moreover, the proposed classifiers gain significant time improvement compared to traditional online one-class classifiers (in all aspects of training and testing). A faster learning ability of the proposed classifiers makes them more suitable for real-time anomaly detection. © 2013 IEEE.},
  langid = {english},
  keywords = {/unread,Adaptive online learning,Anomaly detection,Benchmarking,Changing environment,Classification (of information),Data streams,E-learning,Extreme learning machine (ELM),Incremental learning,kernel,Learning systems,One-class Classification,one-class classification (OCC),One-class classifier,online sequential,outliers detection,Real-time anomaly detections,regularization,Training and testing},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.017Z}
}

@article{gautamConstructionExtremeLearning2017,
  title = {On the Construction of Extreme Learning Machine for Online and Offline One-Class Classification—{{An}} Expanded Toolbox},
  author = {Gautam, C. and Tiwari, A. and Leng, Q.},
  date = {2017},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {261},
  pages = {126--143},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2016.04.070},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013371957&doi=10.1016%2fj.neucom.2016.04.070&partnerID=40&md5=585c6df00cc6e45b93b1107bcd6f0979},
  abstract = {One-class classification (OCC) has been prime concern for researchers and effectively employed in various disciplines. But, traditional methods based one-class classifiers are very time consuming due to its iterative process and various parameters tuning. In this paper, we present six OCC methods and their thirteen variants based on extreme learning machine (ELM) and online sequential ELM (OSELM). Our proposed classifiers mainly lie in two categories: reconstruction based and boundary based, where three proposed classifiers belong to reconstruction based and three belong to boundary based. We are presenting both types of learning viz., online and offline learning for OCC. Out of six methods, four are offline and remaining two are online methods. Out of four offline methods, two methods perform random feature mapping and two methods perform kernel feature mapping. We present a comprehensive discussion on these methods and their comparison to each other. Kernel feature mapping based approaches have been tested with RBF kernel and online version of one-class classifiers is tested with both types of nodes viz., additive and RBF. It is well known fact that threshold decision is a crucial factor in case of OCC, so, three different threshold deciding criteria have been employed so far and analyze the effectiveness of one threshold deciding criteria over another. Further, these methods are tested on two artificial datasets to check their boundary construction capability and on eight benchmark datasets from different discipline to evaluate the performance of the classifiers. Our proposed classifiers exhibit better performance compared to ten traditional one-class classifiers and ELM based two one-class classifiers. Through proposed one-class classifiers, we intend to expand the functionality of the most used toolbox for OCC i.e. DD toolbox. All of our methods are totally compatible with all the present features of the toolbox. © 2017 Elsevier B.V.},
  langid = {english},
  keywords = {/unread,accuracy,Article,Autoassociative ELM (AAELM),Benchmarking,Classification (of information),classification algorithm,classifier,data analysis,E-learning,Extreme learning machine (ELM),Iterative methods,kernel method,Knowledge acquisition,linear system,machine learning,Machine learning,Mapping,mathematical analysis,mathematical model,One-class Classification,One-class classification (OCC),One-class ELM (OCELM),Online sequential ELM (OSELM),online system,prediction,priority journal,support vector machine},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.975Z}
}

@article{gewenigerClusteringFuzzyNeural2013,
  title = {Clustering by Fuzzy Neural Gas and Evaluation of Fuzzy Clusters},
  author = {Geweniger, T. and Fischer, L. and Kaden, M. and Lange, M. and Villmann, T.},
  date = {2013},
  journaltitle = {Computational Intelligence and Neuroscience},
  shortjournal = {Comput. Intell. Neurosci.},
  volume = {2013},
  issn = {16875265 (ISSN)},
  doi = {10.1155/2013/165248},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893733810&doi=10.1155%2f2013%2f165248&partnerID=40&md5=594fccf7b6984981b0d4c256720dcf51},
  abstract = {We consider some modifications of the neural gas algorithm. First, fuzzy assignments as known from fuzzy c-means and neighborhood cooperativeness as known from self-organizing maps and neural gas are combined to obtain a basic Fuzzy Neural Gas. Further, a kernel variant and a simulated annealing approach are derived. Finally, we introduce a fuzzy extension of the ConnIndex to obtain an evaluation measure for clusterings based on fuzzy vector quantization. © 2013 Tina Geweniger et al.},
  langid = {english},
  keywords = {algorithm,Algorithms,article,artificial neural network,cluster analysis,Cluster Analysis,Clusterings,Conformal mapping,Evaluation measures,Fuzzy C mean,Fuzzy extension,fuzzy logic,Fuzzy Logic,Fuzzy systems,Fuzzy vector quantization,Fuzzy-neural,Neural gas,Neural gas algorithms,Neural Networks (Computer),Simulated annealing,Vector quantization},
  annotation = {5 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-25T13:14:16.260Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/citen scopus/Geweniger et al_2013_Clustering by fuzzy neural gas and evaluation of fuzzy clusters.pdf;/home/leix/Documents/Zotero_Papers/TFM/citen scopus/Geweniger et al_2013_Clustering by fuzzy neural gas and evaluation of fuzzy clusters2.pdf}
}

@article{ghadhbanANALYSISFEATUREEXTRACTION2021,
  title = {{{ANALYSIS}} of {{FEATURE EXTRACTION}} and {{CLASSIFICATION}} for {{OFFLINE ARABIC HANDWRITING WORD RECOGNITION}}},
  author = {Ghadhban, H.Q. and Othman, M. and Samsudin, N.A.},
  date = {2021},
  journaltitle = {Journal of Engineering Science and Technology},
  shortjournal = {J. Eng. Sci. Technol.},
  volume = {16},
  number = {3},
  pages = {2719--2735},
  publisher = {{Taylor's University}},
  issn = {18234690 (ISSN)},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110271709&partnerID=40&md5=81b75464d16392e476c9282d42c16e0a},
  abstract = {Arabic handwriting recognition is currently experiencing a massive rise in terms of analysis research. It is clear in the lecture review that researchers concentrate on Arabic handwriting recognition. A major increase has been reported in feature extraction and classification techniques compared to before. Offline Arabic handwriting recognition is an exceptionally significant research topic due to the cursive nature of written Arabic words and writing styles which makes the problem of recognizing Arabic words hard and challenging. In this paper, we investigate three feature extractors techniques Chebyshev Moments (CM), Statistical and Contour based Feature (SCF) and Zernike moment (ZER) for offline Arabic handwritten word recognition. We have considered a number of classes with different shapes of an image. These features have been trained separately with three classifiers namely, Extreme Learning Machine (ELM), Support Vector Machine (SVM) and Reduced Kernel Extreme Learning Machine (RKELM). The experiments were evaluated on IFN/ENIT database, experimental results show that shape of feature has a major impact on training classifiers. © School of Engineering, Taylor’s University},
  langid = {english},
  keywords = {/unread,Extreme learning machine (ELM),Feature extraction,Reduced kernel extreme learning machine (RKELM),Support vector machine (SVM),Word recognition},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.018Z}
}

@article{gholamiReliabilitySensitivityAnalysis2020,
  title = {Reliability and Sensitivity Analysis of Robust Learning Machine in Prediction of Bank Profile Morphology of Threshold Sand Rivers},
  author = {Gholami, A. and Bonakdari, H. and Ebtehaj, I. and Khodashenas, S.R.},
  date = {2020},
  journaltitle = {Measurement: Journal of the International Measurement Confederation},
  shortjournal = {Meas J Int Meas Confed},
  volume = {153},
  publisher = {{Elsevier B.V.}},
  issn = {02632241 (ISSN)},
  doi = {10.1016/j.measurement.2019.107411},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077121637&doi=10.1016%2fj.measurement.2019.107411&partnerID=40&md5=2eb780ccdb50e453113857f65ed48177},
  abstract = {This paper investigates how to achieve an equilibrium or stable state in a channel by taking into account widening and variations in its cross-section geometry dimensions. The current proposal is carried out by using an artificial intelligence technique, a Feed-Forward Neural Network (FFNN), which is trained by Extreme Learning Machine (ELM) algorithm to predict the banks profile morphology or shape profile characteristics of banks after stability. Moreover, the performance of proposed FFNN-ELM model is compared with eight famous previous traditional models and also a designed Non-Linear Regression (NLR). The analyses have been validated using a large number of experimental studies at different flow discharge rates. The results of ELM in comparison with NLR and traditional methods shows that the FFNN-ELM model has the best performance with lower error index values of Root Mean Squared Error (RMSE) equal to 5.6E−5 and Mean Absolute Relative Error (MARE) equal to 0.016, compared to NLR (RMSE = 2.2E−4 and MARE = 0.1225) and the most accurate traditional model which is related to Cao and Knight's (1997) model (RMSE = 0.119 and MARE = 0.1095). Therefore, the FFNN-ELM model proposed in this paper maintains a suitable computational efficiency, as second-degree polynomial, for its performance compared to NLR and previous traditional models in estimating shape profile of stable channels banks. Furthermore, the uncertainty of proposed FFNN-ELM is calculated by Monte-Carlo based simulation method to assess the reliability of model to predict bank profile shape. The uncertainty result shows the less uncertainty of the FFNN-ELM model in shape simulation with the percentage of observed data bracketed by 95 percentage prediction uncertainties (95PPU) equal to 81.35\% and d-factor value equal to 1.26 in test stage. Furthermore, the sensitivity of FFNN-ELM model relative to input parameters (flow discharge, Q, and lateral distance from the centreline of the channel, x) represents that in nearly all discharges, the sensitivity value is negative, indicates that by increasing the value of x parameter, the predicted value for the vertical level of the bank's by model decreases. Moreover, by increasing the flow discharge, the sensitivity of the FFNN-ELM model to the two effective parameters of Q and x on the estimation of vertical level of the stable channels bank is increased. © 2019 Elsevier Ltd},
  langid = {english},
  keywords = {Artificial intelligence techniques,Bank profile morphology,Bank profiles,Computational efficiency,ELM model,Errors,Extreme learning machine (ELM),Feedforward neural networks,Flowmeters,Forecasting,Machine learning,Mean square error,Monte Carlo methods,Morphology,Reliability analysis,Reliability and sensitivity,Root mean squared errors,Second degree polynomials,Sensitivity analysis,Threshold channel,Threshold channels,Uncertainty analysis,Uncertainty and sensitivity,Widening process},
  annotation = {3 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T14:45:07.429Z},
  file = {/home/leix/Documents/Zotero/storage/BC6YUPXB/gholami2020.pdf.pdf}
}

@inproceedings{gisbrechtLinearBasisfunctionTSNE2012,
  title = {Linear Basis-Function t-{{SNE}} for Fast Nonlinear Dimensionality Reduction},
  booktitle = {Proc {{Int Jt Conf Neural Networks}}},
  author = {Gisbrecht, A. and Mokbel, B. and Hammer, B.},
  date = {2012},
  doi = {10.1109/IJCNN.2012.6252809},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865103430&doi=10.1109%2fIJCNN.2012.6252809&partnerID=40&md5=2508c594db04d91763b19be88120e842},
  abstract = {t-distributed stochastic neighbor embedding (t-SNE) constitutes a nonlinear dimensionality reduction technique which is particularly suited to visualize high dimensional data sets with intrinsic nonlinear structures. A major drawback, however, consists in its squared complexity which makes the technique infeasible for large data sets or online application in an interactive framework. In addition, since the technique is non parametric, it possesses no direct method to extend the technique to novel data points. In this contribution, we propose an extension of t-SNE to an explicit mapping. In the limit, it reduces to standard non-parametric t-SNE, while offering a feasible nonlinear embedding function for other parameter choices. We evaluate the performance of the technique when trained on a small subpart of the given data only. It turns out that its generalization ability is good when evaluated with the standard quality curve. Further, in many cases, it obtains a quality which approximates the quality of t-SNE when trained on the full data set, albeit only 10\% of the data are used for training. This opens the way towards efficient nonlinear dimensionality reduction techniques as required in interactive settings. © 2012 IEEE.},
  isbn = {9781467314909 (ISBN)},
  langid = {english},
  keywords = {/notrelevant,Data points,Data sets,Direct method,Generalization ability,High dimensional data,Large datasets,Neural networks,Non-parametric,Nonlinear analysis,Nonlinear dimensionality reduction,Nonlinear embedding,Nonlinear structure,On-line applications,Parameter choice,Quality control,Stochastic neighbor embedding},
  annotation = {17 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-25T13:07:02.179Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/citen scopus/Gisbrecht et al_2012_Linear basis-function t-SNE for fast nonlinear dimensionality reduction.pdf}
}

@article{gisbrechtMetricNonmetricProximity2015,
  title = {Metric and Non-Metric Proximity Transformations at Linear Costs},
  author = {Gisbrecht, A. and Schleif, F.-M.},
  date = {2015},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {167},
  pages = {643--657},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2015.04.017},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945932882&doi=10.1016%2fj.neucom.2015.04.017&partnerID=40&md5=f03be1031b6cbc8f9aa2dd64e332861a},
  abstract = {Domain specific (dis-)similarity or proximity measures used e.g. in alignment algorithms of sequence data are popular to analyze complicated data objects and to cover domain specific data properties. Without an underlying vector space these data are given as pairwise (dis-)similarities only. The few available methods for such data focus widely on similarities and do not scale to large datasets. Kernel methods are very effective for metric similarity matrices, also at large scale, but costly transformations are necessary starting with non-metric (dis-) similarities. We propose an integrative combination of Nyström approximation, potential double centering and eigenvalue correction to obtain valid kernel matrices at linear costs in the number of samples. By the proposed approach effective kernel approaches become accessible. Experiments with several larger (dis-)similarity datasets show that the proposed method achieves much better runtime performance than the standard strategy while keeping competitive model accuracy. The main contribution is an efficient and accurate technique, to convert (potentially non-metric) large scale dissimilarity matrices into approximated positive semi-definite kernel matrices at linear costs. © 2015 Elsevier B.V.},
  langid = {english},
  keywords = {Article,controlled study,Dissimilarity learning,Double centering,Eigen-value,Eigenvalues and eigenfunctions,Euclidean,factual database,Indefinite kernel,kernel method,Large dataset,learning algorithm,Linear eigenvalue correction,linear system,Linear transformations,mathematical computing,mathematical model,Mathematical transformations,Matrix algebra,metric proximity transformation,non metric proximity transformation,Nyström approximation,priority journal,Pseudo-Euclidean,statistical parameters,Vector spaces},
  annotation = {24 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-26T20:30:34.752Z},
  file = {/home/leix/Documents/Zotero/storage/LEXL3WK3/gisbrecht2015.pdf.pdf}
}

@inproceedings{gisbrechtOutofsampleKernelExtensions2012,
  title = {Out-of-Sample Kernel Extensions for Nonparametric Dimensionality Reduction},
  booktitle = {{{ESANN Proc}}., {{European Symp}}. {{Artif}}. {{Neural Netw}}., {{Comput}}. {{Intell}}. {{Mach}}. {{Learn}}.},
  author = {Gisbrecht, A. and Lueks, W. and Mokbel, B. and Hammer, B.},
  date = {2012},
  pages = {531--536},
  publisher = {{i6doc.com publication}},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905445275&partnerID=40&md5=108d295f6621bec4fb9186d074aab2ef},
  abstract = {Nonparametric dimensionality reduction (DR) techniques such as locally linear embedding or t-distributed stochastic neighbor (t- SNE) embedding constitute standard tools to visualize high dimensional and complex data in the Euclidean plane. With increasing data volumes and streaming applications, it is often no longer possible to project all data points at once. Rather, out-of-sample extensions (OOS) derived from a small subset of all data points are used. In this contribution, we propose a kernel mapping for OOS in contrast to direct techniques based on the DR method. This can be trained based on a given example set, or it can be trained indirectly based on the cost function of the DR technique. Considering t-SNE as an example and several benchmarks, we show that a kernel mapping outperforms direct OOS as provided by t-SNE. © 2012, i6doc.com publication. All rights reserved.},
  isbn = {9782874190490 (ISBN)},
  langid = {english},
  keywords = {/notrelevant,Artificial intelligence,Complex networks,Cost functions,Dimensionality reduction,Euclidean planes,Geometry,High-dimensional,Kernel mapping,Learning systems,Locally linear embedding,Mapping,Neural networks,Out-of-sample extension,Standard tools,Stochastic systems,Streaming applications},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-25T13:06:05.822Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/citen scopus/Gisbrecht et al_2012_Out-of-sample kernel extensions for nonparametric dimensionality reduction.pdf}
}

@article{gneccoLearningBoundaryConditions2013,
  title = {Learning with Boundary Conditions},
  author = {Gnecco, G. and Gori, M. and Sanguineti, M.},
  date = {2013},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Comp.},
  volume = {25},
  number = {4},
  pages = {1029--1106},
  issn = {08997667 (ISSN)},
  doi = {10.1162/NECO_a_00417},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877806319&doi=10.1162%2fNECO_a_00417&partnerID=40&md5=23f787cff98adee67e4539ea0303dbde},
  abstract = {Kernel machines traditionally arise from an elegant formulation based on measuring the smoothness of the admissible solutions by the norm in the reproducing kernel Hilbert space (RKHS) generated by the chosen kernel. It was pointed out that they can be formulated in a related functional framework, in which the Green's function of suitable differential operators is thought of as a kernel. In this letter, our ownpicture of this intriguing connection is given by emphasizing some relevant distinctions between these different ways of measuring the smoothnessof admissible solutions. In particular, we show that for some kernels, there is no associated differential operator. The crucial relevance of boundary conditions is especially emphasized, which is in fact the truly distinguishing feature of the approach based on differential operators. We provide a general solution to the problem of learning from data and boundary conditions and illustrate the significant role played by boundary conditions with examples. It turns out that the degree of freedom that arises in the traditional formulation of kernel machines is indeed a limitation, which is partly overcome when incorporating the boundary conditions. This likely holds true in many real-world applications in which there is prior knowledge about the expected behavior of classifiers and regressors on the boundary. ©2013 Massachusetts Institute of Technology.},
  langid = {english},
  annotation = {25 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-25T13:16:02.983Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/citen scopus/Gnecco et al_2013_Learning with boundary conditions.pdf}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian},
  editora = {Bengio, Yoshua and Courville, Aaron},
  editoratype = {collaborator},
  date = {2016},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{The MIT Press}},
  location = {{Cambridge, Massachusetts}},
  isbn = {978-0-262-03561-3},
  langid = {english},
  pagetotal = {xxii+775},
  keywords = {Algorismes,Aprenentatge automàtic,Xarxes neuronals (Informàtica)},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T15:37:43.540Z},
  file = {/home/leix/Documents/Zotero/storage/8M7GLWKQ/Goodfellow - 2016 - Deep learning.pdf}
}

@article{guanExtremeLearningMachine2018,
  title = {Extreme Learning Machine with Superpixel-Guided Composite Kernels for {{SAR}} Image Classification},
  author = {Guan, D. and Tang, X. and Wang, L. and Zhang, J.},
  date = {2018},
  journaltitle = {IEICE Transactions on Information and Systems},
  shortjournal = {IEICE Trans Inf Syst},
  volume = {E101D},
  number = {6},
  pages = {1703--1706},
  publisher = {{Institute of Electronics, Information and Communication, Engineers, IEICE}},
  issn = {09168532 (ISSN)},
  doi = {10.1587/transinf.2017EDL8281},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048004251&doi=10.1587%2ftransinf.2017EDL8281&partnerID=40&md5=aa3294594b4331319b4ee032464ae409},
  abstract = {Synthetic aperture radar (SAR) image classification is a popular yet challenging research topic in the field of SAR image interpretation. This paper presents a new classification method based on extreme learning machine (ELM) and the superpixel-guided composite kernels (SGCK). By introducing the generalized likelihood ratio (GLR) similarity, a modified simple linear iterative clustering (SLIC) algorithm is firstly developed to generate superpixel for SAR image. Instead of using a fixed-size region, the shape-adaptive superpixel is used to exploit the spatial information, which is effective to classify the pixels in the detailed and near-edge regions. Following the framework of composite kernels, the SGCK is constructed base on the spatial information and backscatter intensity information. Finally, the SGCK is incorporated an ELM classifier. Experimental results on both simulated SAR image and real SAR image demonstrate that the proposed framework is superior to some traditional classification methods. Copyright © 2018 The Institute of Electronics, Information and Communication Engineers.},
  langid = {english},
  keywords = {/unread,Backscatter intensity,Backscattering,Classification (of information),Classification methods,Clustering algorithms,Composite kernels,Composite kernels (CK),Extreme learning machine (ELM),Generalized likelihood ratio,Image classification,Iterative methods,Knowledge acquisition,Learning systems,Pixels,Radar imaging,SAR image classification,SAR image classifications,Simple Linear Iterative Clustering,Speech recognition,Superpixel,Superpixels,Synthetic aperture radar,Synthetic aperture radar (SAR) images},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.986Z}
}

@article{guoFaultDetectionMicro2023,
  title = {Fault detection of micro motors based on the BA-KELM utilizing multi-domain features},
  shorttitle = {基于多域特征的BA-KELM微型电机故障检测},
  author = {Guo, M. and Li, W. and Zhao, X. and Zhang, X.},
  date = {2023},
  journaltitle = {Zhendong yu Chongji/Journal of Vibration and Shock},
  shortjournal = {J Vib Shock},
  volume = {42},
  number = {2},
  pages = {251--257},
  publisher = {{Chinese Vibration Engineering Society}},
  issn = {10003835 (ISSN)},
  doi = {10.13465/j.cnki.jvs.2023.02.030},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145923429&doi=10.13465%2fj.cnki.jvs.2023.02.030&partnerID=40&md5=ce4cc2630ef45b5843a1b64ded2b76de},
  abstract = {At present, there are few researches on micro-motor fault detection, and the traditional motor diagnosis methods based on single domain features have usually low accuracy. So, a fault detection method for micro motors based on the ensemble empirical mode decomposition (EEMD) and bat algorithm (BA) was proposed. The proposed method includes three steps: constructing sample sets, model training as well as parameters optimizing, and model testing. Firstly, EEMD processing was carried out on the collected micro motor signals, and the main intrinsic mode fuction (IMF) components were selected by virtue of the principle of correlation coefficient. Fusing the calculated time and frequency domain features of the motor signals, a multi-domain feature set was constructed and normalized. Then, these features were divided into a training set and a test set according to a certain proportion. Taking the training set as input and employing, the error rate as fitness, the parameters of the kernel based extreme learning machine ( KELM) model were optimized by means of the BA. Finally, the optimized BA - KELM model was tested by using the test set. The experimental results show that the accuracy of the proposed method is 98.75\%, which is higher than other methods. © 2023 Chinese Vibration Engineering Society. All rights reserved.},
  langid = {chinese},
  keywords = {/unread,Bat algorithm,bat algorithm (BA),Bat algorithms,Empirical mode decomposition,extreme learning machine (ELM),Extreme learning machine (ELM),Fault detection,fault diagnosis,Faults detection,Faults diagnosis,Frequency domain analysis,Kernel based extreme learning machine,kernel based extreme learning machine (KELM),Knowledge acquisition,Learning machines,Machine learning,micro motor,Micromotor,Multi-domain features,Well testing},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.028Z}
}

@article{guoIncrementalExtremeLearning2014,
  title = {An Incremental Extreme Learning Machine for Online Sequential Learning Problems},
  author = {Guo, L. and Hao, J.-H. and Liu, M.},
  date = {2014},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {128},
  pages = {50--58},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2013.03.055},
  abstract = {A fast and outstanding incremental learning algorithm is required to meet the demand of online applications where data comes one by one or chunk by chunk to avoid retraining and save precious time. Although many interesting research results have been achieved, there are still a lot of difficulties in real applications because of their unsatisfying generalization performance or intensive computation cost. This paper presents an Incremental Extreme Learning Machine (IELM) which is developed based on Extreme Learning Machine (ELM), a unified framework of LS-SVM and PSVM presented by Hang et al. (2011) in [15]. Under different application demand and different computational cost and efficiency, three different alternative solutions of IELM are achieved. Detailed comparisons of the IELM algorithm with other incremental algorithms are achieved by simulation on benchmark problems and real critical dimension (CD) prediction problem in lithography of actual semiconductor production line. The results show that kernel based IELM solution performs best while least square IELM solution is the fastest of the three alterative solutions when the number of training data is huge. All the results show that the presented IELM algorithms have better performance than other incremental algorithms such as online sequential ELM (OS-ELM) presented by Liang et al. (2006) [8] and fixed size LSSVM presented by Espinoza et al. (2006) [11]. © 2013 Elsevier B.V.},
  langid = {english},
  keywords = {article,classification algorithm,Computational efficiency,Extreme learning machine (ELM),Fixed size,Fixed size LSSVM (FS-LSSVM),Incremental ELM (IELM),incremental extreme learning machine,Incremental learning,Incremental learning algorithm,intermethod comparison,kernel method,Knowledge acquisition,learning algorithm,Learning algorithms,Learning systems,machine learning,mathematical computing,Online sequential ELM (OS-ELM),online sequential machine learning,prediction,priority journal,process development,quality control,semiconductor production line,simulation,statistical parameters,support vector machine},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-27T18:58:18.216Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/elm kernel scopus/Guo et al_2014_An incremental extreme learning machine for online sequential learning problems.pdf}
}

@article{hammerChallengesNeuralComputation2012,
  title = {Challenges in {{Neural Computation}}},
  author = {Hammer, B.},
  date = {2012},
  journaltitle = {KI - Kunstliche Intelligenz},
  shortjournal = {KI - Kunstl. Intell.},
  volume = {26},
  number = {4},
  pages = {333--340},
  publisher = {{Springer Science and Business Media Deutschland GmbH}},
  issn = {09331875 (ISSN)},
  doi = {10.1007/s13218-012-0209-0},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045382113&doi=10.1007%2fs13218-012-0209-0&partnerID=40&md5=07583b8b77b491666764c74cebf7e283},
  abstract = {This contribution contains a short history of neural computation and an overview about the major learning paradigms and neural architectures used today. © 2012, Springer-Verlag.},
  langid = {english},
  keywords = {/notrelevant,Computers,Learning paradigms,Neural architectures,Neural computations,Neural networks,Neural-networks,Short history},
  annotation = {1 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-25T13:08:27.674Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/citen scopus/Hammer_2012_Challenges in Neural Computation.pdf}
}

@article{hanLARSENELMSelectiveEnsemble2015,
  title = {{{LARSEN-ELM}}: {{Selective}} Ensemble of Extreme Learning Machines Using {{LARS}} for Blended Data},
  author = {Han, B. and He, B. and Nian, R. and Ma, M. and Zhang, S. and Li, M. and Lendasse, A.},
  date = {2015},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {149},
  pages = {285--294},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2014.01.069},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922018056&doi=10.1016%2fj.neucom.2014.01.069&partnerID=40&md5=25bd625d74b28adc668d237f29b0efa2},
  abstract = {Extreme learning machine (ELM) as a neural network algorithm has shown its good performance, such as fast speed, simple structure etc, but also, weak robustness is an unavoidable defect in original ELM for blended data. We present a new machine learning framework called "LARSEN-ELM" to overcome this problem. In our paper, we would like to show two key steps in LARSEN-ELM. In the first step, preprocessing, we select the input variables highly related to the output using least angle regression (LARS). In the second step, training, we employ Genetic Algorithm (GA) based selective ensemble and original ELM. In the experiments, we apply a sum of two sines and four datasets from UCI repository to verify the robustness of our approach. The experimental results show that compared with original ELM and other methods such as OP-ELM, GASEN-ELM and LSBoost, LARSEN-ELM significantly improves robustness performance while keeping a relatively high speed. © 2014 Elsevier B.V.},
  issue = {Part A},
  langid = {english},
  keywords = {Article,data base,Extreme learning machine (ELM),genetic algorithm,Genetic algorithms,Input variables,intermethod comparison,Knowledge acquisition,LARS algorithm,Lars-en,LARSEN extreme learning machine,LARSEN-ELM,Learning algorithms,Learning systems,least angle regression algorithm,Least angle regressions,machine learning,mathematical computing,Neural network algorithm,Robustness,Robustness (control systems),Selective ensemble,Selective ensembles,Simple structures,UCI repository},
  annotation = {17 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-26T20:31:27.147Z},
  file = {/home/leix/Documents/Zotero/storage/89J9MKYZ/han2015.pdf.pdf}
}

@article{hanRMSEELMRecursiveModel2014,
  title = {{{RMSE-ELM}}: {{Recursive}} Model Based Selective Ensemble of Extreme Learning Machines for Robustness Improvement},
  author = {Han, B. and He, B. and Ma, M. and Sun, T. and Yan, T. and Lendasse, A.},
  date = {2014},
  journaltitle = {Mathematical Problems in Engineering},
  shortjournal = {Math. Probl. Eng.},
  volume = {2014},
  publisher = {{Hindawi Publishing Corporation}},
  issn = {1024123X (ISSN)},
  doi = {10.1155/2014/395686},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911164274&doi=10.1155%2f2014%2f395686&partnerID=40&md5=95ca98bb012121cba31dc43808798cab},
  abstract = {For blended data, the robustness of extreme learning machine (ELM) is so weak because the coefficients (weights and biases) of hidden nodes are set randomly and the noisy data exert a negative effect. To solve this problem, a new framework called "RMSE-ELM" is proposed in this paper. It is a two-layer recursive model. In the first layer, the framework trains lots of ELMs in different ensemble groups concurrently and then employs selective ensemble approach to pick out an optimal set of ELMs in each group, which can be merged into a large group of ELMs called candidate pool. In the second layer, selective ensemble approach is recursively used on candidate pool to acquire the final ensemble. In the experiments, we apply UCI blended datasets to confirm the robustness of our new approach in two key aspects (mean square error and standard deviation). The space complexity of our method is increased to some degree, but the result has shown that RMSE-ELM significantly improves robustness with a rapid learning speed compared to representative methods (ELM, OP-ELM, GASEN-ELM, GASEN-BP, and E-GASEN). It becomes a potential framework to solve robustness issue of ELM for high-dimensional blended data in the future. © 2014 Bo Han et al.},
  langid = {english},
  keywords = {/notrelevant,Extreme learning machine (ELM),High-dimensional,Knowledge acquisition,Lakes,Learning systems,New approaches,Recursive modeling,Robustness issues,Selective ensembles,Space complexity,Standard deviation},
  annotation = {0 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-25T22:22:53.291Z},
  file = {/home/leix/Documents/Zotero/storage/CPBK68MP/han2014.pdf.pdf}
}

@article{heClusteringExtremeLearning2014,
  title = {Clustering in Extreme Learning Machine Feature Space},
  author = {He, Q. and Jin, X. and Du, C. and Zhuang, F. and Shi, Z.},
  date = {2014},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {128},
  pages = {88--95},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2012.12.063},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893131953&doi=10.1016%2fj.neucom.2012.12.063&partnerID=40&md5=a421f0c14cf5a186e88bb952d2671683},
  abstract = {Extreme learning machine (ELM), used for the "generalized" single-hidden-layer feedforward networks (SLFNs), is a unified learning platform that can use a widespread type of feature mappings. In theory, ELM can approximate any target continuous function and classify any disjoint regions; in application, many experiment results have already demonstrated the good performance of ELM. In view of the good properties of the ELM feature mapping, the clustering problem using ELM feature mapping techniques is studied in this paper. Experiments show that the proposed ELM kMeans algorithm and ELM NMF (nonnegative matrix factorization) clustering can get better clustering results than the corresponding Mercer kernel based methods and the traditional algorithms using the original data. Moreover, the proposed methods have the advantage of being more convenient to implementation and computation, as the ELM feature mapping is much simpler than the Mercer kernel function based feature mapping methods. © 2013 Elsevier B.V.},
  langid = {english},
  keywords = {algorithm,article,artificial neural network,classification algorithm,cluster analysis,Clustering algorithms,controlled study,Data clustering,ELM feature space,ELM kMeans,ELM NMF clustering,Experiments,Extreme learning machine (ELM),Feature space,K-means,kernel method,kMeans algorithm,Knowledge acquisition,Learning systems,machine learning,Mapping,Mercer kernel method,Network layers,Nonnegative matrix factorization,Nonnegative matrix factorization (NMF),nonnegative matrix factorization algorithm,priority journal,quality control},
  annotation = {77 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-25T22:27:09.194Z},
  file = {/home/leix/Documents/Zotero/storage/L7UYDLNB/he2014.pdf.pdf}
}

@article{heFastFaceRecognition2014,
  title = {Fast {{Face Recognition Via Sparse Coding}} and {{Extreme Learning Machine}}},
  author = {He, B. and Xu, D. and Nian, R. and family=Heeswijk, given=M., prefix=van, useprefix=true and Yu, Q. and Miche, Y. and Lendasse, A.},
  date = {2014},
  journaltitle = {Cognitive Computation},
  shortjournal = {Cognitive Comput.},
  volume = {6},
  number = {2},
  pages = {264--277},
  publisher = {{Springer New York LLC}},
  issn = {18669956 (ISSN)},
  doi = {10.1007/s12559-013-9224-1},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901191688&doi=10.1007%2fs12559-013-9224-1&partnerID=40&md5=05c613e5761110d3dbafca69897c9b63},
  abstract = {Most face recognition approaches developed so far regard the sparse coding as one of the essential means, while the sparse coding models have been hampered by the extremely expensive computational cost in the implementation. In this paper, a novel scheme for the fast face recognition is presented via extreme learning machine (ELM) and sparse coding. The common feature hypothesis is first introduced to extract the basis function from the local universal images, and then the single hidden layer feedforward network (SLFN) is established to simulate the sparse coding process for the face images by ELM algorithm. Some developments have been done to maintain the efficient inherent information embedding in the ELM learning. The resulting local sparse coding coefficient will then be grouped into the global representation and further fed into the ELM ensemble which is composed of a number of SLFNs for face recognition. The simulation results have shown the good performance in the proposed approach that could be comparable to the state-of-the-art techniques at a much higher speed. © 2013 Springer Science+Business Media New York.},
  langid = {english},
  keywords = {Codes (symbols),Common feature hypothesis,Common features,Computational costs,Extreme learning machine (ELM),Face recognition,Feed-forward network,Global representation,Image coding,Information embedding,Knowledge acquisition,Learning systems,Network layers,Sparse coding,State-of-the-art techniques},
  annotation = {11 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-25T22:25:31.344Z},
  file = {/home/leix/Documents/Zotero/storage/LB6I27GF/he2013.pdf.pdf}
}

@article{heFastKernelExtreme2018,
  title = {A Fast Kernel Extreme Learning Machine Based on Conjugate Gradient},
  author = {He, C. and Xu, F. and Liu, Y. and Zheng, J.},
  date = {2018},
  journaltitle = {Network: Computation in Neural Systems},
  shortjournal = {Netw. Comput. Neural Syst.},
  volume = {29},
  number = {1-4},
  pages = {70--80},
  publisher = {{Taylor and Francis Ltd}},
  issn = {0954898X (ISSN)},
  doi = {10.1080/0954898X.2018.1562247},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060987522&doi=10.1080%2f0954898X.2018.1562247&partnerID=40&md5=1fabfbf32384915bb9fabe042ebd2169},
  abstract = {Kernel extreme learning machine (KELM) introduces kernel leaning into extreme learning machine (ELM) in order to improve the generalization ability and stability. But the Penalty parameter in KELM is randomly set and it has a strong impact on the performance of KELM. A fast KELM combining the conjugate gradient method (CG-KELM) is presented in this paper. The CG-KELM computes the output weights of the neural network by the conjugate gradient iteration method. There is no penalty parameter to be set in CG-KELM. Therefore, the CG-KELM has good generalization ability and fast learning speed. The simulations in image restoration show that CG-KELM outperforms KELM. The CG-KELM provides a balanced method between KELM and ELM. © 2019, © 2019 Taylor \& Francis.},
  langid = {english},
  keywords = {algorithm,Algorithms,artificial neural network,conjugate gradient method,Generalization (Psychology),generalization ability,human,Humans,image restoration,Kernel extreme learning machine,learning,machine learning,Machine Learning,{Models, Theoretical},Neural Networks (Computer),physiology,theoretical model},
  annotation = {5 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T14:14:13.994Z},
  file = {/home/leix/Documents/Zotero/storage/SC6DUJTS/10.1080@0954898x.2018.1562247.pdf.pdf}
}

@inproceedings{heiligMemoryEfficientKernel2022,
  title = {Memory {{Efficient Kernel Approximation}} for {{Non-Stationary}} and {{Indefinite Kernels}}},
  booktitle = {Proc {{Int Jt Conf Neural Networks}}},
  author = {Heilig, S. and Munch, M. and Schleif, F.-M.},
  date = {2022},
  volume = {2022-July},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/IJCNN55064.2022.9892153},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140714964&doi=10.1109%2fIJCNN55064.2022.9892153&partnerID=40&md5=213080c00b1b8450e956731a88e82c6e},
  abstract = {Matrix approximations are a key element in large-scale algebraic machine learning approaches. The recently pro-posed method MEKA [1] effectively employs two common assumptions in Hilbert spaces: the low-rank property of an inner product matrix obtained from a shift-invariant kernel function and a data compactness hypothesis by means of an inherent block-cluster structure. In this work, we extend MEKA to be applicable not only for shift-invariant kernels but also for non-stationary kernels like polynomial kernels and an extreme learning kernel. We also address in detail how to handle non-positive semi-definite kernel functions within MEKA, either caused by the approximation itself or by the intentional use of general kernel functions. We present a Lanczos-based estimation of a spectrum shift to develop a stable positive semi-definite MEKA approximation, also usable in classical convex optimization frameworks. Furthermore, we support our findings with theoretical considerations and a variety of experiments on synthetic and real-world data. © 2022 IEEE.},
  isbn = {9781728186719 (ISBN)},
  langid = {english},
  keywords = {/unread,Convex optimization,Efficient kernels,Indefinite kernel,Indefinite learning,Indefinite Learning,Iterative methods,Kernel approximation,Kernel Approximation,Kernel function,Lanczos Iteration,Lanczos iterations,Large-Scale,Large-scales,Learning systems,Matrix algebra,Memory efficient,Nonstationary,Shift invariant,Structure (composition)},
  annotation = {0 citations (Crossref) [2023-02-21] Read\_Status: Not Reading Read\_Status\_Date: 2023-02-27T18:01:39.252Z}
}

@book{heusingerClassificationNonstationaryEnvironments2021,
  title = {Classification in {{Non-stationary Environments Using Coresets}} over {{Sliding Windows}}},
  author = {Heusinger, M. and Schleif, F.-M.},
  editorb = {{Rojas I.} and {Joya G.} and {Catala A.}},
  editorbtype = {redactor},
  date = {2021},
  journaltitle = {Lect. Notes Comput. Sci.},
  volume = {12861 LNCS},
  pages = {137},
  publisher = {{Springer Science and Business Media Deutschland GmbH}},
  doi = {10.1007/978-3-030-85030-2_11},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115144908&doi=10.1007%2f978-3-030-85030-2_11&partnerID=40&md5=87d6a48d0105a1f3c01991d5a1d753f7},
  abstract = {In non-stationary environments, several constraints require algorithms to be fast, memory-efficient, and highly adaptable. While there are several classifiers of the family of lazy learners and tree classifiers in the streaming context, the application of prototype-based classifiers has not found much attention. Prototype-based classifiers however have some interesting characteristics, which are also useful in streaming environments, in particular being highly interpretable. Hence, we propose a new prototype-based classifier, which is based on Minimum Enclosing Balls over sliding windows. We propose this algorithm as a linear version as well as kernelized. Our experiments show, that this technique can be useful and is comparable in performance to another popular prototype-based streaming classifier – the Adaptive Robust Soft Learning Vector Quantization but with an additional benefit of having a configurable window size to catch rapidly changing drift and the ability to use the internal mechanics for drift detection. © 2021, Springer Nature Switzerland AG.},
  isbn = {03029743 (ISSN); 9783030850296 (ISBN)},
  langid = {english},
  pagetotal = {126},
  keywords = {Intelligent computing,Lazy learners,Learning Vector Quantization,Memory efficient,Minimum enclosing ball,Neural networks,Non-stationary environment,Prototype-based classifier,Sliding Window,Tree classifiers},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-27T18:00:51.396Z},
  file = {/home/leix/Documents/Zotero/storage/JB6P357U/Heusinger and Schleif - 2021 - Classification in Non-stationary Environments Usin.pdf}
}

@inproceedings{heusingerReactiveConceptDrift2020,
  title = {Reactive {{Concept Drift Detection Using Coresets}} over {{Sliding Windows}}},
  booktitle = {{{IEEE Symp}}. {{Ser}}. {{Comput}}. {{Intell}}., {{SSCI}}},
  author = {Heusinger, M. and Schleif, F.-M.},
  date = {2020},
  pages = {1350--1355},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/SSCI47803.2020.9308521},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099711371&doi=10.1109%2fSSCI47803.2020.9308521&partnerID=40&md5=6746f074c6aca7075c070de400c2fe6c},
  abstract = {The change of underlying data is one of the biggest challenges in non-stationary environments. While several algorithms have been proposed to detect these changes, substantial problems remain in the case of higher dimensional data. Thus, we propose a novel Concept Drift detector based on Minimum Enclosing Balls, with the capability to quickly process higher dimensional data. Additionally a kernelized version of this detector is derived, to process non-linear streaming data. We also propose a method to measure the performance of drift detectors with a binary classification evaluation technique, the confusion matrix, which enables calculating statistics like the F1 score. Our experiments show, that this novel technique is superior to existing state-of-the-art Concept Drift detectors regarding its true positive detection rate. © 2020 IEEE.},
  isbn = {9781728125473 (ISBN)},
  langid = {english},
  keywords = {Binary classification,Clustering algorithms,Confusion matrices,Drift detectors,Higher-dimensional,Intelligent computing,Minimum enclosing ball,Non-stationary environment,Novel techniques,State of the art},
  annotation = {3 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T14:46:22.168Z},
  file = {/home/leix/Documents/Zotero/storage/4DS7T5I5/heusinger2020.pdf.pdf}
}

@article{hofmannKernelMethodsMachine2008,
  title = {Kernel Methods in Machine Learning},
  author = {Hofmann, Thomas and Schölkopf, Bernhard and Smola, Alexander J.},
  date = {2008-06-01},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  volume = {36},
  number = {3},
  eprint = {math/0701907},
  eprinttype = {arxiv},
  issn = {0090-5364},
  doi = {10.1214/009053607000000677},
  url = {http://arxiv.org/abs/math/0701907},
  urldate = {2023-08-19},
  abstract = {We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data. We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.},
  keywords = {/unread,30C40 (Primary) 68T05 (Secondary),Mathematics - Probability,Mathematics - Statistics Theory},
  annotation = {1141 citations (Crossref) [2023-08-19]},
  file = {/home/leix/Documents/Zotero_Papers/TFM/Introduction/Hofmann et al_2008_Kernel methods in machine learning.pdf;/home/leix/Documents/Zotero/storage/3L3PHUU7/0701907.html}
}

@article{hortaStreamBasedExtremeLearning2015,
  title = {Stream-{{Based Extreme Learning Machine Approach}} for {{Big Data Problems}}},
  author = {Horta, E.G. and Castro, C.L.D. and Braga, A.P.},
  date = {2015},
  journaltitle = {Mathematical Problems in Engineering},
  shortjournal = {Math. Probl. Eng.},
  volume = {2015},
  publisher = {{Hindawi Publishing Corporation}},
  issn = {1024123X (ISSN)},
  doi = {10.1155/2015/126452},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942775453&doi=10.1155%2f2015%2f126452&partnerID=40&md5=9ec541eabe5d8bdda90a868faaaa27cb},
  abstract = {Big Data problems demand data models with abilities to handle time-varying, massive, and high dimensional data. In this context, Active Learning emerges as an attractive technique for the development of high performance models using few data. The importance of Active Learning for Big Data becomes more evident when labeling cost is high and data is presented to the learner via data streams. This paper presents a novel Active Learning method based on Extreme Learning Machines (ELMs) and Hebbian Learning. Linearization of input data by a large size ELM hidden layer turns our method little sensitive to parameter setting. Overfitting is inherently controlled via the Hebbian Learning crosstalk term. We also demonstrate that a simple convergence test can be used as an effective labeling criterion since it points out to the amount of labels necessary for learning. The proposed method has inherent properties that make it highly attractive to handle Big Data: incremental learning via data streams, elimination of redundant patterns, and learning from a reduced informative training set. Experimental results have shown that our method is competitive with some large-margin Active Learning strategies and also with a linear SVM. © 2015 Euler Guimarães Horta et al.},
  langid = {english},
  keywords = {Active learning methods,Active learning strategies,Artificial intelligence,Big data,Clustering algorithms,Data communication systems,Data mining,Extreme learning machine (ELM),High dimensional data,Incremental learning,Knowledge acquisition,Learning systems,Parameter setting,Performance Model,Redundant patterns},
  annotation = {3 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-26T20:34:07.687Z},
  file = {/home/leix/Documents/Zotero/storage/69BAHT8H/horta2015.pdf.pdf}
}

@article{huangClusteringMethodBased2018,
  title = {A Clustering Method Based on Extreme Learning Machine},
  author = {Huang, J. and Yu, Z.L. and Gu, Z.},
  date = {2018},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {277},
  pages = {108--119},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2017.02.100},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028457075&doi=10.1016%2fj.neucom.2017.02.100&partnerID=40&md5=39cee59f526fd60ce5112df6b6738979},
  abstract = {Though many successful methods have been proposed for supervised learning tasks, such as support vector machines and extreme learning machines (ELM), it is still an open problem to extend the successful supervised learning methods to unsupervised learning tasks and obtain better results. In this paper, we propose to extend the ELM to an unsupervised learning version and propose a clustering method based on ELM (CM-ELM) for both binary class and multiple class problems, which aims to find a labeling that would yield an optimal ELM classifier. In the ELM feature space, we propose to combine the Gaussian hidden nodes and sigmoid hidden nodes in the hidden layer to combine their advantages. Then we propose to adopt the alternative direction method to solve the non-convex problems in CM-ELM simply. Furthermore, in order to make the results of the non-convex problems robust and satisfactory, we propose to initial the labels with cluster ensemble methods. Experiments on the artificial and benchmark data sets show that the CM-ELM is competitive to the state-of-the-art clustering methods. © 2017 Elsevier B.V.},
  langid = {english},
  keywords = {/unread,algorithm,Alternative direction optimization,Article,classifier,cluster analysis,Cluster analysis,Cluster ensembles,Clustering,Clustering methods,controlled study,Extreme learning machine (ELM),facial recognition,information processing,intermethod comparison,kernel method,Knowledge acquisition,Learning systems,machine learning,mathematical computing,mathematical parameters,Multiple class problem,Nonconvex problem,Optimal ELM classifier,priority journal,State of the art,Supervised learning methods,support vector machine,Support vector machines,Unsupervised learning},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.986Z}
}

@article{huangExtremeLearningMachine2006,
  title = {Extreme {{Learning Machine}}: {{Theory}} and {{Applications}}},
  shorttitle = {Extreme {{Learning Machine}}},
  author = {Huang, Guang-Bin and Zhu, Qin-Yu and Siew, Chee},
  date = {2006-12-01},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {70},
  doi = {10.1016/j.neucom.2005.12.126},
  abstract = {It is clear that the learning speed of feedforward neural networks is in general far slower than required and it has been a major bottleneck in their applications for past decades. Two key reasons behind may be: (1) the slow gradient-based learning algorithms are extensively used to train neural networks, and (2) all the parameters of the networks are tuned iteratively by using such learning algorithms. Unlike these conventional implementations, this paper proposes a new learning algorithm called extreme learning machine (ELM) for single-hidden layer feedforward neural networks (SLFNs) which randomly chooses hidden nodes and analytically determines the output weights of SLFNs. In theory, this algorithm tends to provide good generalization performance at extremely fast learning speed. The experimental results based on a few artificial and real benchmark function approximation and classification problems including very large complex applications show that the new algorithm can produce good generalization performance in most cases and can learn thousands of times faster than conventional popular learning algorithms for feedforward neural networks.1},
  annotation = {8773 citations (Crossref) [2023-02-22] Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:06.184Z},
  file = {/home/leix/Documents/Zotero/storage/3IXS3AC5/huang2006.pdf.pdf}
}

@article{huangExtremeLearningMachine2012,
  title = {Extreme Learning Machine for Regression and Multiclass Classification},
  author = {Huang, G.-B. and Zhou, H. and Ding, X. and Zhang, R.},
  date = {2012},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
  shortjournal = {IEEE Trans Syst Man Cybern Part B Cybern},
  volume = {42},
  number = {2},
  pages = {513--529},
  issn = {10834419 (ISSN)},
  doi = {10.1109/TSMCB.2011.2168604},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859007933&doi=10.1109%2fTSMCB.2011.2168604&partnerID=40&md5=f63897a64b4a58cfe4a2aa3768508662},
  abstract = {Due to the simplicity of their implementations, least square support vector machine (LS-SVM) and proximal support vector machine (PSVM) have been widely used in binary classification applications. The conventional LS-SVM and PSVM cannot be used in regression and multiclass classification applications directly, although variants of LS-SVM and PSVM have been proposed to handle such cases. This paper shows that both LS-SVM and PSVM can be simplified further and a unified learning framework of LS-SVM, PSVM, and other regularization algorithms referred to extreme learning machine (ELM) can be built. ELM works for the "generalized" single-hidden-layer feedforward networks (SLFNs), but the hidden layer (or called feature mapping) in ELM need not be tuned. Such SLFNs include but are not limited to SVM, polynomial network, and the conventional feedforward neural networks. This paper shows the following: 1) ELM provides a unified learning platform with a widespread type of feature mappings and can be applied in regression and multiclass classification applications directly; 2) from the optimization method point of view, ELM has milder optimization constraints compared to LS-SVM and PSVM; 3) in theory, compared to ELM, LS-SVM and PSVM achieve suboptimal solutions and require higher computational complexity; and 4) in theory, ELM can approximate any target continuous function and classify any disjoint regions. As verified by the simulation results, ELM tends to have better scalability and achieve similar (for regression and binary class cases) or much better (for multiclass cases) generalization performance at much faster learning speed (up to thousands times) than traditional SVM and LS-SVM. © 2006 IEEE.},
  langid = {english},
  keywords = {Extreme learning machine (ELM),feature mapping,Feature mapping,Feedforward neural networks,kernel,least square support vector machine (LS-SVM),Least square support vector machines,Mapping,Network layers,proximal support vector machine (PSVM),Proximal support vector machines,Regression analysis,regularization network,Regularization networks,Support vector machines},
  annotation = {Read\_Status: To Read Read\_Status\_Date: 2023-02-27T12:47:34.431Z},
  file = {/home/leix/Documents/Zotero/storage/TNGTXMWM/guang-binhuang2012.pdf.pdf}
}

@article{huangExtremeLearningMachines2011,
  title = {Extreme Learning Machines: {{A}} Survey},
  author = {Huang, G.-B. and Wang, D.H. and Lan, Y.},
  date = {2011},
  journaltitle = {International Journal of Machine Learning and Cybernetics},
  shortjournal = {Intl. J. Mach. Learn. Cybern.},
  volume = {2},
  number = {2},
  pages = {107--122},
  issn = {18688071 (ISSN)},
  doi = {10.1007/s13042-011-0019-y},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958178274&doi=10.1007%2fs13042-011-0019-y&partnerID=40&md5=d481e78197b9c380df8158a739fed019},
  abstract = {Computational intelligence techniques have been used in wide applications. Out of numerous computational intelligence techniques, neural networks and support vector machines (SVMs) have been playing the dominant roles. However, it is known that both neural networks and SVMs face some challenging issues such as: (1) slow learning speed, (2) trivial human intervene, and/or (3) poor computational scalability. Extreme learning machine (ELM) as emergent technology which overcomes some challenges faced by other techniques has recently attracted the attention from more and more researchers. ELM works for generalized single-hidden layer feedforward networks (SLFNs). The essence of ELM is that the hidden layer of SLFNs need not be tuned. Compared with those traditional computational intelligence techniques, ELM provides better generalization performance at a much faster learning speed and with least human intervene. This paper gives a survey on ELM and its variants, especially on (1) batch learning mode of ELM, (2) fully complex ELM, (3) online sequential ELM, (4) incremental ELM, and (5) ensemble of ELM. © 2011 Springer-Verlag.},
  langid = {english},
  keywords = {E-learning,ELM feature space,ELM kernel,Ensemble,Extreme learning machine (ELM),Feature space,Incremental learning,Network layers,Neural networks,Online sequential learning,Sequential learning,Support vector,Support vector machine,Support vector machines,Surveys},
  annotation = {1447 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-25T13:01:17.431Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/interesants/Huang et al_2011_Extreme learning machines.pdf}
}

@article{huangInsightExtremeLearning2014,
  title = {An {{Insight}} into {{Extreme Learning Machines}}: {{Random Neurons}}, {{Random Features}} and {{Kernels}}},
  author = {Huang, G.-B.},
  date = {2014},
  journaltitle = {Cognitive Computation},
  shortjournal = {Cognitive Comput.},
  volume = {6},
  number = {3},
  pages = {376--390},
  publisher = {{Springer New York LLC}},
  issn = {18669956 (ISSN)},
  doi = {10.1007/s12559-014-9255-2},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906948723&doi=10.1007%2fs12559-014-9255-2&partnerID=40&md5=efb8e0734e3b1726f1d95b3d6b54a5b0},
  abstract = {Extreme learning machines (ELMs) basically give answers to two fundamental learning problems: (1) Can fundamentals of learning (i.e., feature learning, clustering, regression and classification) be made without tuning hidden neurons (including biological neurons) even when the output shapes and function modeling of these neurons are unknown? (2) Does there exist unified framework for feedforward neural networks and feature space methods? ELMs that have built some tangible links between machine learning techniques and biological learning mechanisms have recently attracted increasing attention of researchers in widespread research areas. This paper provides an insight into ELMs in three aspects, viz: random neurons, random features and kernels. This paper also shows that in theory ELMs (with the same kernels) tend to outperform support vector machine and its variants in both regression and classification applications with much easier implementation. © 2014 Springer Science+Business Media New York.},
  langid = {english},
  keywords = {Artificial intelligence,Biological learning,ELM kernel,Elm kernels,Extreme learning machine (ELM),Feedforward neural networks,Function modeling,Knowledge acquisition,Least square support vector machine,Least square support vector machines,Machine learning techniques,Neurons,Random feature,Random features,Random neuron,Randomized matrix,Support vector machine,Support vector machines,Unified framework},
  annotation = {726 citations (Crossref) [2023-02-21] Read\_Status: To Read Read\_Status\_Date: 2023-02-25T13:01:44.861Z},
  file = {/home/leix/Documents/Zotero/storage/NAT37N3W/Huang - 2014 - An Insight into Extreme Learning Machines Random .pdf}
}

@article{huangLearningCapabilityStorage2003,
  title = {Learning Capability and Storage Capacity of Two-Hidden-Layer Feedforward Networks},
  author = {Huang, Guang-Bin},
  date = {2003-03},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {14},
  number = {2},
  pages = {274--281},
  issn = {1941-0093},
  doi = {10.1109/TNN.2003.809401},
  abstract = {The problem of the necessary complexity of neural networks is of interest in applications. In this paper, learning capability and storage capacity of feedforward neural networks are considered. We markedly improve the recent results by introducing neural-network modularity logically. This paper rigorously proves in a constructive method that two-hidden-layer feedforward networks (TLFNs) with 2/spl radic/(m+2)N (/spl Lt/N) hidden neurons can learn any N distinct samples (x/sub i/, t/sub i/) with any arbitrarily small error, where m is the required number of output neurons. It implies that the required number of hidden neurons needed in feedforward networks can be decreased significantly, comparing with previous results. Conversely, a TLFN with Q hidden neurons can store at least Q/sup 2//4(m+2) any distinct data (x/sub i/, t/sub i/) with any desired precision.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {/unread,Feedforward neural networks,Multi-layer neural network,Neural networks,Neurons,Upper bound},
  annotation = {586 citations (Crossref) [2023-08-20]},
  file = {/home/leix/Documents/Zotero/storage/52TWPQL4/1189626.html}
}

@article{huangOptimizationMethodBased2010,
  title = {Optimization Method Based Extreme Learning Machine for Classification},
  author = {Huang, G.-B. and Ding, X. and Zhou, H.},
  date = {2010},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {74},
  number = {1-3},
  pages = {155--163},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2010.02.019},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649492473&doi=10.1016%2fj.neucom.2010.02.019&partnerID=40&md5=0d3ccdca6c8e5ce832d5bda2be5e5749},
  abstract = {Extreme learning machine (ELM) as an emergent technology has shown its good performance in regression applications as well as in large dataset (and/or multi-label) classification applications. The ELM theory shows that the hidden nodes of the "generalized" single-hidden layer feedforward networks (SLFNs), which need not be neuron alike, can be randomly generated and the universal approximation capability of such SLFNs can be guaranteed. This paper further studies ELM for classification in the aspect of the standard optimization method and extends ELM to a specific type of "generalized" SLFNs-support vector network. This paper shows that: (1) under the ELM learning framework, SVM's maximal margin property and the minimal norm of weights theory of feedforward neural networks are actually consistent; (2) from the standard optimization method point of view ELM for classification and SVM are equivalent but ELM has less optimization constraints due to its special separability feature; (3) as analyzed in theory and further verified by the simulation results, ELM for classification tends to achieve better generalization performance than traditional SVM. ELM for classification is less sensitive to user specified parameters and can be implemented easily. © 2010 Elsevier B.V.},
  langid = {english},
  keywords = {analytical parameters,article,classification,Classification (of information),Computer simulation,data base,ELM feature space,ELM kernel,Equivalence between ELM and SVM,Extreme learning machine,Extreme learning machine (ELM),Feature space,Feedforward neural networks,learning algorithm,machine learning,mathematical computing,Maximal margin,medical parameters,Minimal norm of weights,Network layers,Optimization,Primal and dual ELM networks,priority journal,process optimization,regression analysis,simulation,standardization,support vector machine,Support vector machine,Support vector machines,Support vector network,technology,Vector spaces,Vectors},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-27T18:46:25.837Z},
  file = {/home/leix/Documents/Zotero/storage/XC6HCJP6/huang2010.pdf.pdf;/home/leix/Documents/Zotero/storage/FIZFSDSJ/S0925231210002225.html}
}

@article{huangTrendsExtremeLearning2015,
  title = {Trends in Extreme Learning Machines: {{A}} Review},
  author = {Huang, G. and Huang, G.-B. and Song, S. and You, K.},
  date = {2015},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Netw.},
  volume = {61},
  pages = {32--48},
  publisher = {{Elsevier Ltd}},
  issn = {08936080 (ISSN)},
  doi = {10.1016/j.neunet.2014.10.001},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908682236&doi=10.1016%2fj.neunet.2014.10.001&partnerID=40&md5=1e4b0d1ed4ec9aa4317d0d9d19129474},
  abstract = {Extreme learning machine (ELM) has gained increasing interest from various research fields recently. In this review, we aim to report the current state of the theoretical research and practical advances on this subject. We first give an overview of ELM from the theoretical perspective, including the interpolation theory, universal approximation capability, and generalization ability. Then we focus on the various improvements made to ELM which further improve its stability, sparsity and accuracy under general or specific conditions. Apart from classification and regression, ELM has recently been extended for clustering, feature selection, representational learning and many other learning tasks. These newly emerging algorithms greatly expand the applications of ELM. From implementation aspect, hardware implementation and parallel computation techniques have substantially sped up the training of ELM, making it feasible for big data processing and real-time reasoning. Due to its remarkable efficiency, simplicity, and impressive generalization performance, ELM have been applied in a variety of domains, such as biomedical engineering, computer vision, system identification, and control and robotics. In this review, we try to provide a comprehensive view of these advances in ELM together with its future perspectives. © 2014 Elsevier Ltd.},
  langid = {english},
  keywords = {/unread,accuracy,algorithm,Algorithms,artificial intelligence,Artificial Intelligence,Biomedical engineering,classification,Classification,Classification (of information),classification algorithm,Clustering,Computation theory,Computer control systems,computer model,computer prediction,Data handling,Extreme learning machine (ELM),Feature learning,Generalization performance,Hardware implementations,image processing,Implementation aspects,intermethod comparison,kernel method,Knowledge acquisition,learning algorithm,machine learning,Machine learning,Regression,regression analysis,Review,standards,support vector machine,trend study,trends,Universal approximation},
  annotation = {1209 citations (Crossref) [2023-02-21] Read\_Status: To Read Read\_Status\_Date: 2023-02-25T13:01:50.415Z},
  file = {/home/leix/Documents/Zotero/storage/A5I9MBXI/huang2015.pdf.pdf;/home/leix/Documents/Zotero/storage/UHH3DP5D/Huang et al. - 2015 - Trends in extreme learning machines A review.pdf}
}

@inproceedings{huHybridApproachPTSVM2014,
  title = {A Hybrid Approach of {{PTSVM}} and {{ELM}} Inspired by Samples' Geometric Distribution Structure},
  booktitle = {Proc. - {{IEEE Int}}. {{Conf}}. {{Granul}}. {{Comput}}., {{GrC}}},
  author = {Hu, H. and Ouyang, P. and Pang, L. and Shi, Z.},
  editor = {{Kudo Y.} and {Tsumoto S.}},
  date = {2014},
  pages = {100--105},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/GRC.2014.6982815},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920717505&doi=10.1109%2fGRC.2014.6982815&partnerID=40&md5=7f073c9204ab1b68f0fe8b680fb63b74},
  abstract = {In this paper, we try to hybrid projection twin support vector machine (PTSVM) and Extreme Learning Machine(ELM). The experiments shows that ELM generally out performs SVM/LS-SVM in various kinds of cases. PTELM tries to use ELM to overcome the shortness of PTSVM, which lacks of flexibility to change nonlinear kernel mapping for complex samples distribution regions. In order to overcome the shortness of ELMs, we try to maintain the geometric structure of the samples distribution in the initial setting of ELM parameters by starting with an auto-encoder ELM, a sufficient condition for such kind auto-encoder ELM to keep the equivalence of topological homology before and after nonlinear mapping is proved by us. For an auto-encoder, in order to do feature abstraction and find the exact manifold in which samples are located, the number of inner layers nodes should be small enough. For this purpose, we prove that the whole samples set can be at least replaced by a small subset which located on the boundary of distribution region. For more, the weights are modified by a ELM approach to make samples of a class move toward this classs hyperplane found by PTSVM. The experimental results on several UCI benchmark data sets show the feasibility and effectiveness of the proposed method. © 2014 IEEE.},
  isbn = {9781479954643 (ISBN)},
  langid = {english},
  keywords = {Extreme learning machine (ELM),Extreme Learning Machine (ELM),Feature abstraction,Geometric distribution,Geometric structure,Geometry,Knowledge acquisition,Learning systems,Mapping,Nonlinear kernel mapping,Nonlinear mappings,nonlinear support vector machine,Nonlinear Support Vector Machines,Probability distributions,projection twin support vector machine (PTSVM),Signal encoding,Support vector machines,Twin support vector machines},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-27T18:59:14.390Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/elm kernel scopus/Hu et al_2014_A hybrid approach of PTSVM and ELM inspired by samples' geometric distribution.pdf}
}

@incollection{jaganIntelligentModelsApplied2017,
  title = {Intelligent Models Applied to Elastic Modulus of Jointed Rock Mass},
  booktitle = {Handb. of {{Res}}. on {{Trends}} and {{Digit}}. {{Adv}}. in {{Eng}}. {{Geol}}.},
  author = {Jagan, J. and Samui, P. and Roy, S.S. and Kurup, P.},
  date = {2017},
  pages = {1--30},
  publisher = {{IGI Global}},
  doi = {10.4018/978-1-5225-2709-1.ch001},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028857235&doi=10.4018%2f978-1-5225-2709-1.ch001&partnerID=40&md5=715d1511f2209d9e4356daa1dddf2e54},
  abstract = {Elastic Modulus (Ej) of jointed rock mass is a key parameter for deformation analysis of rock mass. This chapter adopts three intelligent models (Extreme Learning Machine (ELM), Minimax Probability Machine Regression (MPMR) and Generalized Regression Neural Network (GRNN)) for determination of Ej of jointed rock mass. MPMR is derived in a probability framework. ELM is the modified version of Single Hidden Layer Feed forward network. GRNN approximates any arbitrary function between the input and output variables. Joint frequency (Jn), joint inclination parameter (n), joint roughness parameter (r), confining pressure (σ3) (MPa), and elastic modulus (Ei) (GPa) of intact rock have been taken as inputs of the ELM, GRNN and MPMR models. The output of ELM, GRNN and MPMR is Ej of jointed rock mass. In this study, ELM, GRNN and MPMR have been used as regression techniques. The developed GRNN, ELM and MPMR have been compared with the Artificial Neural Network (ANN) models. © 2018 by IGI Global. All rights reserved.},
  isbn = {9781522527107 (ISBN); 1522527095 (ISBN); 9781522527091 (ISBN)},
  langid = {english},
  keywords = {Artificial neural network models,Deformation analysis,Elastic moduli,Extreme learning machine (ELM),Feed-forward network,Generalized Regression Neural Network(GRNN),Learning systems,Minimax probability machine,Network layers,Neural networks,Regression analysis,Regression techniques,Rock mechanics,Rocks,Roughness parameters},
  annotation = {Read\_Status: Not Reading Read\_Status\_Date: 2023-02-27T13:57:02.895Z}
}

@article{jiangExtremeLearningMachine2018,
  title = {Extreme {{Learning Machine}} with {{Enhanced Composite Feature}} for {{Spectral-Spatial Hyperspectral Image Classification}}},
  author = {Jiang, M. and Cao, F. and Lu, Y.},
  date = {2018},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {6},
  pages = {22645--22654},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {21693536 (ISSN)},
  doi = {10.1109/ACCESS.2018.2825978},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045976779&doi=10.1109%2fACCESS.2018.2825978&partnerID=40&md5=6d78627a4241f00e27f7d9b427561a8b},
  abstract = {The applications of extreme learning machine (ELM) to the hyperspectral-image (HSI) classification have attracted a great deal of research attention because of its excellent performance and fast learning speed. However, conventional ELM is unable to achieve satisfactory accuracy since it only exploits the spectral information to conduct the HSI classification. To address the above issues, we propose a novel classification algorithm based on both spectral and multiscale spatial information, referred to as ELM with enhanced composite feature (ELM-ECF). To be specific, we adopt the original ELM, exploit a multiscale spatial weighted-mean-filtering-based approach to extract multiple spatial information, and use the majority vote method to select the final classification result. The proposed ELM-ECF significantly improves the classification accuracy of the original ELM. Experimental results on three public HSIs (i.e., Indian Pines data set, Pavia University data set, and Salinas data set) illustrate that the proposed ELM-ECF outperforms a variety of the state-of-the-art HSI classification counterparts in terms of classification accuracy. © 2013 IEEE.},
  langid = {english},
  keywords = {/unread,Classification (of information),Classification accuracy,Classification algorithm,Classification results,Composite features,Data mining,enhanced composite feature (CF),Extreme learning machine (ELM),Facsimile,Feature extraction,hyperspectral images (HSIs),Hyperspectral imaging,Image classification,Image enhancement,Independent component analysis,Information filtering,Kernel,Knowledge acquisition,Learning systems,Neurons,Personnel training,Spectral information,Spectroscopy,Weighted mean filtering},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.987Z}
}

@inproceedings{jiangRemoteSensingImage2019,
  title = {Remote Sensing Image Scene Classification Based on Densely Connected Multilayer {{Kernel ELM}}},
  booktitle = {{{ANZCC}} - {{Austr}}. {{New Zealand Control Conf}}.},
  author = {Jiang, X.W. and Yan, T.H. and Xu, Q. and He, B. and Du, H.P. and Li, W.H.},
  date = {2019},
  pages = {81--86},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/ANZCC.2018.8606551},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062417995&doi=10.1109%2fANZCC.2018.8606551&partnerID=40&md5=74ae520683a092ab6e97db25dfa50147},
  abstract = {The remote sensing images are captured by high-altitude satellites, and the coverage of each image scene is relatively large. It tends to have large intraclass differences and small differences between classes. Using conventional Hierarchical ELM (H-ELM) and multilayer kernel ELM (ML-KELM) to classify remote sensing image scenes, the network structure of the learning model is deep and the parameters are many. This leads to long training time and large memory consumption. In order to solve this problem, based on the ML-KELM, this paper proposes a densely connected kernel ELM (Dense-KELM) learning model, which is used to classify remote sensing image scenes. Experimental results show that at the same model depth, the Dense-KELM model has higher classification accuracy in remote sensing image scenes than the H-ELM and the ML-KELM. Its training time is slightly larger than the ML-KELM but much smaller than the H-ELM. This densely connected learning model can extract high-level features of remote sensing images more effectively, represent the details between remote sensing scenes, and improve the classification accuracy of remote sensing image scenes. Moreover, the densely connected network structure can effectively reduce the number of parameters of the depth model, improve the training speed of the model, and save the storage space of the model. © 2018 IEEE.},
  isbn = {9781538666173 (ISBN)},
  langid = {english},
  keywords = {/unread,Dense Connection,Extreme learning machine (ELM),Extreme Learning Machine (ELM),Feature extraction,Feature Extraction,Image classification,Image enhancement,Kernel function,Kernel Function,Learning systems,Multilayers,Remote sensing,Remote sensing images,Remote Sensing Images,Scene classification,Scene Classification,Space optics},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.999Z}
}

@article{jiaSARImageChange2016,
  title = {{{SAR Image Change Detection Based}} on {{Correlation Kernel}} and {{Multistage Extreme Learning Machine}}},
  author = {Jia, L. and Li, M. and Zhang, P. and Wu, Y.},
  date = {2016},
  journaltitle = {IEEE Transactions on Geoscience and Remote Sensing},
  shortjournal = {IEEE Trans Geosci Remote Sens},
  volume = {54},
  number = {10},
  pages = {5993--6006},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {01962892 (ISSN)},
  doi = {10.1109/TGRS.2016.2578438},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976312614&doi=10.1109%2fTGRS.2016.2578438&partnerID=40&md5=0a0bf85a6741ae79e205b408a43c08b8},
  abstract = {Designing a kernel function with good discriminating ability and a highly application-Adaptive kernelized classifier is the key of many kernel methods. However, not many kernel functions combining directly the bitemporal images' information are designed specifically for change detection tasks. In addition, extreme learning machine (ELM) has not found wide applications in change detection tasks, even though it is a potential kernel method possessing outstanding approximation and generalization capabilities as well as great classification accuracy and efficiency. Therefore, an approach relying on a difference correlation kernel (DCK) and a multistage ELM (MS-ELM) is proposed in this paper for synthetic aperture radar (SAR) image change detection. First, a DCK function is constructed specifically for change detection by measuring the 'distance' between any two pixels. The DCK function depicts the cross-Time similarities between couples of bitemporal image patches at any cyclic shifts with a kernel correlation operation and the high-order spatial distances between two differently located pixels with an algebraic subtraction. The DCK function possesses strong noise immunity and good identification of changed areas simultaneously. Second, an MS-ELM classifier is constructed for obtaining the change detection result. In MS-ELM, the hidden nodes and weights between the hidden and output layers are updated stage by stage by improving the kernel functions that compose them. Each stage of the MS-ELM is a standard kernel-ELM, and the DCK function is utilized in the first stage. The regenerative kernel functions incorporate the output spatial-neighborhood information of the previous stage for enhancing remarkably the MS-ELM's discriminating ability and noise resistance. The converged result at the last stage of MS-ELM is the final change detection result. Experiments on real SAR image change detection demonstrate the effectiveness of the DCK function and the MS-ELM algorithm, particularly its good identification of changed areas and strong robustness against noise in SAR images. © 1980-2012 IEEE.},
  langid = {english},
  keywords = {/unread,accuracy assessment,Change detection,Classification accuracy and efficiency,correlation,Correlation operation,data processing,detection method,difference correlation kernel (DCK) function,Discriminating abilities,efficiency measurement,Extreme learning machine (ELM),Generalization capability,image analysis,image classification,Knowledge acquisition,Learning systems,machine learning,multistage extreme learning machine (ELM) (MS-ELM),pixel,Pixels,Radar imaging,Signal detection,Spatial neighborhoods,spatial-neighborhood information,Strong-noise immunities,synthetic aperture radar,Synthetic aperture radar,synthetic aperture radar (SAR) image,Synthetic aperture radar (SAR) images},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.969Z}
}

@online{JLLPackagesBinaryBuilder,
  title = {{{JLL}} Packages · {{BinaryBuilder}}.Jl},
  url = {https://docs.binarybuilder.org/stable/jll/},
  urldate = {2023-03-17},
  keywords = {/unread},
  file = {/home/leix/Documents/Zotero/storage/797Z2ZQP/jll.html}
}

@article{JMLR:v21:19-348,
  title = {{{MFE}}: {{Towards}} Reproducible Meta-Feature Extraction},
  author = {Alcobaça, Edesio and Siqueira, Felipe and Rivolli, Adriano and Garcia, Luís P. F. and Oliva, Jefferson T. and family=Carvalho, given=André C. P. L. F., prefix=de, useprefix=true},
  date = {2020},
  journaltitle = {Journal of Machine Learning Research},
  volume = {21},
  number = {111},
  pages = {1--5},
  url = {http://jmlr.org/papers/v21/19-348.html},
  file = {/home/leix/Documents/Zotero/storage/V4Z3ZDYS/Alcobaça - MFE Towards reproducible meta-feature extraction.pdf}
}

@article{jomaaDataset2VecLearningDataset2021,
  title = {{{Dataset2Vec}}: Learning Dataset Meta-Features},
  shorttitle = {{{Dataset2Vec}}},
  author = {Jomaa, Hadi S. and Schmidt-Thieme, Lars and Grabocka, Josif},
  date = {2021-05-01},
  journaltitle = {Data Mining and Knowledge Discovery},
  shortjournal = {Data Min Knowl Disc},
  volume = {35},
  number = {3},
  pages = {964--985},
  issn = {1573-756X},
  doi = {10.1007/s10618-021-00737-9},
  url = {https://doi.org/10.1007/s10618-021-00737-9},
  urldate = {2023-08-27},
  abstract = {Meta-learning, or learning to learn, is a machine learning approach that utilizes prior learning experiences to expedite the learning process on unseen tasks. As a data-driven approach, meta-learning requires meta-features that represent the primary learning tasks or datasets, and are estimated traditonally as engineered dataset statistics that require expert domain knowledge tailored for every meta-task. In this paper, first, we propose a meta-feature extractor called Dataset2Vec that combines the versatility of engineered dataset meta-features with the expressivity of meta-features learned by deep neural networks. Primary learning tasks or datasets are represented as hierarchical sets, i.e., as a set of sets, esp. as a set of predictor/target pairs, and then a DeepSet architecture is employed to regress meta-features on them. Second, we propose a novel auxiliary meta-learning task with abundant data called dataset similarity learning that aims to predict if two batches stem from the same dataset or different ones. In an experiment on a large-scale hyperparameter optimization task for 120 UCI datasets with varying schemas as a meta-learning task, we show that the meta-features of Dataset2Vec outperform the expert engineered meta-features and thus demonstrate the usefulness of learned meta-features for datasets with varying schemas for the first time.},
  langid = {english},
  keywords = {Deep learning,Deep sets,Hyperparameter optimization,Meta-features,Meta-learning},
  file = {/home/leix/Documents/Zotero_Papers/TFM/meta-learning/Jomaa et al_2021_Dataset2Vec.pdf}
}

@online{JuliaV1Release,
  title = {Julia v1.9 {{Release Notes}} · {{The Julia Language}}},
  url = {https://docs.julialang.org/en/v1.9.3/NEWS/#Compiler/Runtime-improvements},
  urldate = {2023-08-27},
  file = {/home/leix/Documents/Zotero/storage/FTNNZY3V/NEWS.html}
}

@software{karatzoglouKernlabKernelBasedMachine2023,
  title = {Kernlab: {{Kernel-Based Machine Learning Lab}}},
  shorttitle = {Kernlab},
  author = {Karatzoglou, Alexandros and Smola, Alex and Hornik, Kurt and Australia (NICTA), National ICT and Maniscalco, Michael A. and Teo, Choon Hui},
  date = {2023-01-31},
  url = {https://CRAN.R-project.org/package=kernlab},
  urldate = {2023-03-10},
  abstract = {Kernel-based machine learning methods for classification, regression, clustering, novelty detection, quantile regression and dimensionality reduction. Among other methods 'kernlab' includes Support Vector Machines, Spectral Clustering, Kernel PCA, Gaussian Processes and a QP solver.},
  version = {0.9-32},
  keywords = {/unread,Cluster,MachineLearning,NaturalLanguageProcessing,Optimization}
}

@inproceedings{kastnerDifferentiableKernelsGeneralized2012,
  title = {Differentiable Kernels in Generalized Matrix Learning Vector Quantization},
  booktitle = {Proc. - {{Int}}. {{Conf}}. {{Mach}}. {{Learn}}. {{Appl}}., {{ICMLA}}},
  author = {Kastner, M. and Nebel, D. and Riedel, M. and Biehl, M. and Villmann, T.},
  date = {2012},
  volume = {1},
  pages = {132--137},
  doi = {10.1109/ICMLA.2012.231},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873600155&doi=10.1109%2fICMLA.2012.231&partnerID=40&md5=0aea68f8fb15fedc4307ab33a4670a8d},
  abstract = {In the present paper we investigate the application of differentiable kernel for generalized matrix learning vector quantization as an alternative kernel-based classifier, which additionally provides classification dependent data visualization. We show that the concept of differentiable kernels allows a prototype description in the data space but equipped with the kernel metric. Moreover, using the visualization properties of the original matrix learning vector quantization we are able to optimize the class visualization by inherent visualization mapping learning also in this new kernel-metric data space. © 2012 IEEE.},
  isbn = {9780769549132 (ISBN)},
  langid = {english},
  keywords = {Data space,Data visualization,Generalized matrix,Kernel based classifiers,Learning systems,Learning Vector Quantization,Vector quantization,Visualization},
  annotation = {7 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-25T13:12:58.105Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/citen scopus/Kastner et al_2012_Differentiable kernels in generalized matrix learning vector quantization.pdf}
}

@book{lalRobustFaceRecognition2021,
  title = {Robust {{Face Recognition Using Multi-scale Feature Pattern Sparse Representation}} with {{Extreme Learning Machine}}},
  author = {Lal, P.V. and Srilakshmi, U. and Venkateswarlu, D.},
  editorb = {{Jyothi S.} and {Mamatha D.M.} and {Zhang Y.-D.} and {Raju K.S.}},
  editorbtype = {redactor},
  date = {2021},
  journaltitle = {Lect. Notes Networks Syst.},
  volume = {215},
  pages = {150},
  publisher = {{Springer Science and Business Media Deutschland GmbH}},
  doi = {10.1007/978-981-16-1941-0_15},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116690491&doi=10.1007%2f978-981-16-1941-0_15&partnerID=40&md5=7cf499718c67c25513d41983074dff08},
  abstract = {Robust vision-assisted facial recognition is the top stimulating job in automation machines. Most of the face recognition techniques introduced till now considered sparse coding as an effective method, whereas the sparse coding prototypes are being hindered through the tremendously expensive evaluation cost in terms of execution. However, these sparse coding models could tolerate robust face recognition having high dimensionality. Therefore, it is essential to develop new face recognition approaches with reduced complexity and less computational time. Hence, novel robust face recognition approaches are presented in this paper. The recognition of robust facial images is performed through ELM classifier which is priory preprocessed by extracting the multi-scale sparse coding feature patterns from the sample images. Then finally the feature patterns dictionaries are obtained for every sample image that is employed with the primary function to work with the SLFN neural network. The experimental outcomes revealed that the suggested methodology has higher recognition rates under all three circumstances compared to other algorithms such as SRC, RSC, and MPRSC-ELM classification algorithms and has less recognition time in seconds. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
  isbn = {23673370 (ISSN); 9789811619403 (ISBN)},
  langid = {english},
  pagetotal = {135},
  keywords = {Classification,Extreme learning machine (ELM),Occluded facial images,Robust face recognition,SLFN,Sparse code representation},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-27T18:03:05.516Z},
  file = {/home/leix/Documents/Zotero/storage/MPHVADTG/Lal et al. - 2021 - Robust Face Recognition Using Multi-scale Feature .pdf}
}

@inproceedings{laoprachaImprovingVehicleDetection2014,
  title = {Improving Vehicle Detection by Adapting Parameters of {{HOG}} and Kernel Functions of {{SVM}}},
  booktitle = {Int. {{Comput}}. {{Sci}}. {{Eng}}. {{Conf}}., {{ICSEC}}},
  author = {Laopracha, N. and Thongkrau, T. and Sunat, K. and Songrum, P. and Chamchong, R.},
  date = {2014},
  pages = {372--377},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/ICSEC.2014.6978225},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988233514&doi=10.1109%2fICSEC.2014.6978225&partnerID=40&md5=dfefa36ca11c53f993be50205ee64462},
  abstract = {Currently, vehicle detection suffers from low performance in terms of accuracy and time costs in real-time application. Histograms Oriented of Gradients(HOG) and Support Vector Machine(SVM) are popular methods used to address these problems, however, while they can give high accuracy, detection is still too slow for real-time application. The V-HOG method has previously been proposed to reduce detection time in real-time application by adjusting HOG structures. Although V-HOG detection is faster than that of HOG, the accuracy is lower. Therefore, this paper proposed to improve accuracy and classification time by adjusting HOG parameters and SVM kernel functions. The experimental results showed that the proposed method results in 100\% accuracy and supports real-time application. © 2014 IEEE.},
  isbn = {9781479949632 (ISBN)},
  langid = {english},
  keywords = {Classification time,Detection time,Extreme learning machine (ELM),Extreme learning Machine(ELM),High-accuracy,Ittstograms of oriented gradients(HOG),Kernel function,Kernel function component,Learning systems,Object detection,Oriented gradients,Real-time application,Support vector machine(SVM),Support vector machines,Vehicle detection,Vehicles},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-27T19:05:03.353Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/elm kernel scopus/Laopracha et al_2014_Improving vehicle detection by adapting parameters of HOG and kernel functions.pdf}
}

@inproceedings{larochelleEmpiricalEvaluationDeep2007,
  title = {An Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variation},
  booktitle = {Proceedings of the 24th International Conference on {{Machine}} Learning},
  author = {Larochelle, Hugo and Erhan, Dumitru and Courville, Aaron and Bergstra, James and Bengio, Yoshua},
  date = {2007-06-20},
  pages = {473--480},
  publisher = {{ACM}},
  location = {{Corvalis Oregon USA}},
  doi = {10.1145/1273496.1273556},
  url = {https://dl.acm.org/doi/10.1145/1273496.1273556},
  urldate = {2023-04-20},
  eventtitle = {{{ICML}} '07 \& {{ILP}} '07: {{The}} 24th {{Annual International Conference}} on {{Machine Learning}} Held in Conjunction with the 2007 {{International Conference}} on {{Inductive Logic Programming}}},
  isbn = {978-1-59593-793-3},
  langid = {english},
  annotation = {490 citations (Crossref) [2023-04-20]},
  file = {/home/leix/Documents/Zotero_Papers/TFM/kernels/Larochelle et al_2007_An empirical evaluation of deep architectures on problems with many factors of.pdf}
}

@online{leeDeepNeuralNetworks2018,
  title = {Deep {{Neural Networks}} as {{Gaussian Processes}}},
  author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  date = {2018-03-02},
  eprint = {1711.00165},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1711.00165},
  url = {http://arxiv.org/abs/1711.00165},
  abstract = {It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:06.190Z},
  file = {/home/leix/Documents/Zotero/storage/YJ4JKRL7/Lee et al. - 2018 - Deep Neural Networks as Gaussian Processes.pdf;/home/leix/Documents/Zotero/storage/C8FIJBRE/1711.html}
}

@article{leiNovelApproachCirrhosis2017,
  title = {A Novel Approach for Cirrhosis Recognition via Improved {{LBP}} Algorithm and Dictionary Learning},
  author = {Lei, Y. and Zhao, X. and Wang, G. and Yu, K. and Guo, W.},
  date = {2017},
  journaltitle = {Biomedical Signal Processing and Control},
  shortjournal = {Biomed. Signal Process. Control},
  volume = {38},
  pages = {281--292},
  publisher = {{Elsevier Ltd}},
  issn = {17468094 (ISSN)},
  doi = {10.1016/j.bspc.2017.06.014},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022183237&doi=10.1016%2fj.bspc.2017.06.014&partnerID=40&md5=ef2a8947c80c37269f0588e589a89bac},
  abstract = {Early diagnosis of cirrhosis has been increasing the interest of medical specialists and engineers. Cirrhosis diagnosis is difficult to distinguish with naked eyes, and it depends on subjectivity of physicians largely. In this paper, the improved Local Binary Pattern(LBP) algorithm called T-LBP(total LBP) and its corresponding T-LBPs(T-LBP spectrum) feature were proposed to describe cirrhosis texture and to solve the edge blurring problem caused by cirrhosis effectively. We applied fusion of T-LBPs, two-dimensional Gabor transform and K-SVD(single value decomposition which generalizes K-means clustering process) based dictionary learning methods in cirrhosis recognition of ultrasound(US) images for the first time. Advantages of proposed algorithms include, firstly, to our best knowledge, proposed T-LBPs feature outperforms the traditional features using support vector machine(SVM), and it has also been proved that the consuming time of kernel extreme learning machine(kernel-ELM) is less than that of basic ELM in this issue; secondly, dictionary learning based recognition method through T-LBP has obtained the highest recognition rate of 99.69\% compared with state-of-the-art methods, and dictionary updating error decreased sharply via T-LBP. Therefore, the proposed algorithm will contribute to the clinical cirrhosis diagnosis. © 2017},
  langid = {english},
  keywords = {/unread,adaboostmm,Article,basic extreme learning machine,Bayesian learning,Cirrhosis,classification algorithm,classifier,Diagnosis,Dictionary learning,echography,ELM,Extreme learning machine (ELM),Improved local binary patterns,K-means clustering,K-SVD,kernel extreme learning machine,kernel method,learning algorithm,Learning algorithms,Learning systems,liver cirrhosis,local binary pattern algorithm,pattern recognition,principal component analysis,priority journal,single value decomposition,Single value decompositions,State-of-the-art methods,support vector machine,Support vector machines,T-LBP,T-LBPs,Textures},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.976Z}
}

@article{liangjunCorrentropybasedRobustMultilayer2018,
  title = {Correntropy-Based Robust Multilayer Extreme Learning Machines},
  author = {Liangjun, C. and Honeine, P. and Hua, Q. and Jihong, Z. and Xia, S.},
  date = {2018},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recogn.},
  volume = {84},
  pages = {357--370},
  publisher = {{Elsevier Ltd}},
  issn = {00313203 (ISSN)},
  doi = {10.1016/j.patcog.2018.07.011},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049905752&doi=10.1016%2fj.patcog.2018.07.011&partnerID=40&md5=9043cdc13eff755b5d3159aa20a2880f},
  abstract = {In extreme learning machines (ELM), the hidden node parameters are randomly generated and the output weights can be analytically computed. To overcome the bad feature extraction ability of the shallow architecture of ELM, the hierarchical ELM has been extensively studied as a deep architecture with multilayer neural network. However, the commonly used mean square error (MSE) criterion is very sensitive to outliers and impulsive noises, generally existing in real world data. In this paper, we investigate the correntropy to improve the robustness of the multilayer ELM and provide sparser representation. The correntropy, as a nonlinear measure of similarity, is robust to outliers and can approximate different norms (from ℓ0 to ℓ2). A new full correntropy based multilayer extreme learning machine (FC-MELM) algorithm is proposed to handle the classification of datasets which are corrupted by impulsive noises or outliers. The contributions of this paper are three-folds: (1) The MSE based reconstruction loss is replaced by the correntropy based loss function; In this way, the robustness of the ELM based multilayer algorithms is enhanced. (2) The traditional ℓ1-based sparsity penalty term is also replaced by a correntropy-based sparsity penalty term, which can further improve the performance of the proposed algorithm with a sparser representation of the data. The combination of (1) and (2) provides the correntropy-based ELM autoencoder. (3) The FC-MELM is proposed by using the correntropy-based ELM autoencoder as a building block. It is notable that the FC-MELM is trained in a forward manner, which means fine-tuning procedure is not required. Thus, the FC-MELM has great advantage in learning efficiently when compared with traditional deep learning algorithms. The good property of the proposed algorithm is confirmed by the experiments on well-known benchmark datasets, including the MNIST datasets, the NYU Object Recognition Benchmark dataset, and the Moore network traffic dataset. Finally, the proposed FC-MELM algorithm is applied to address Computer Aided Cancer Diagnosis. Experiments conducted on the well-known Wisconsin Breast Cancer Data (Diagnostic) dataset are presented and show that the proposed FC-MELM outperforms state-of-the-art methods in solving computer aided cancer diagnosis problems. © 2018},
  langid = {english},
  keywords = {Benchmark datasets,Cancer diagnosis,Classification (of information),Computer aided cancer diagnosis,Computer aided diagnosis,Computer aided instruction,Correntropy,Deep learning,Diseases,Extreme learning machine (ELM),Impulse noise,Knowledge acquisition,Learning algorithms,Mean square error,Mean square error criterions,Multilayer algorithms,Multilayer neural networks,Multilayers,Network architecture,Object recognition,State-of-the-art methods,Statistics,Unsupervised feature learning},
  annotation = {24 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T14:18:00.965Z},
  file = {/home/leix/Documents/Zotero/storage/RALQU7XA/liangjun2018.pdf.pdf}
}

@inproceedings{lianImprovedIndoorPositioning2019,
  title = {Improved {{Indoor}} Positioning Algorithm Using {{KPCA}} and {{ELM}}},
  booktitle = {Int. {{Conf}}. {{Wirel}}. {{Commun}}. {{Signal Process}}., {{WCSP}}},
  author = {Lian, L. and Xia, S. and Zhang, S. and Wu, Q. and Jing, C.},
  date = {2019},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/WCSP.2019.8928106},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077781892&doi=10.1109%2fWCSP.2019.8928106&partnerID=40&md5=65b9c2682edecc4b99e99f638deab284},
  abstract = {RSS (Received Signal Strength) values which are used in indoor position estimation based on fingerprinting are affected by noise. The RSS value received by the fixed line-of-sight condition points obeys the Gauss distribution and does not match the Gauss distribution. WIFI signal transmission attenuation is also a nonlinear attenuation. This paper presents a joint KPCA-ELM locating algorithm, the use of KPCA (Kernel Principal Component Analysis) of the nonlinear characteristics allow the original RSS data being replaced and dimension reduction, constructing new features. ELM (Extreme Learning Machine) is a fast and efficient single-layer feedforward neural network algorithm for training with new characteristics. KPCA-ELM positioning algorithm can effectively reduce the influence of noise on RSS value and improve the accuracy. The experimental results show that KPCA-ELM algorithm can effectively improve the accuracy of indoor positioning. © 2019 IEEE.},
  isbn = {9781728135557 (ISBN)},
  langid = {english},
  keywords = {/unread,Extreme learning machine (ELM),Feedforward neural networks,Indoor positioning systems,Kernel principal component analyses (KPCA),Kernel principal component analysis,Knowledge acquisition,Kpca (kernel principal component analysis),Machine learning,Multilayer neural networks,Network layers,Nonlinear characteristics,Principal component analysis,RSS (received signal strength),Signal processing,Single layer feed-forward neural networks,Wireless positioning},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.001Z}
}

@online{LIBSVMFAQ,
  title = {{{LIBSVM FAQ}}},
  url = {https://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html#f418},
  urldate = {2023-08-27},
  file = {/home/leix/Documents/Zotero/storage/IGNLDAT5/faq.html}
}

@software{LIBSVMJl2023,
  title = {{{LIBSVM}}.Jl},
  date = {2023-03-13T07:11:51Z},
  origdate = {2013-04-15T01:50:21Z},
  url = {https://github.com/JuliaML/LIBSVM.jl},
  urldate = {2023-03-21},
  abstract = {LIBSVM bindings for Julia},
  organization = {{JuliaML}},
  keywords = {/unread,classification,julia,libsvm,regression,svm}
}

@software{LibsvmJllJl2022,
  title = {Libsvm\_jll.Jl (v3.25.0+0)},
  date = {2022-03-20T22:40:10Z},
  origdate = {2020-06-02T14:13:53Z},
  url = {https://github.com/JuliaBinaryWrappers/libsvm_jll.jl},
  urldate = {2023-03-17},
  organization = {{JuliaBinaryWrappers}},
  keywords = {/unread}
}

@article{liCrossDomainExtremeLearning2019,
  title = {Cross-{{Domain Extreme Learning Machines}} for {{Domain Adaptation}}},
  author = {Li, S. and Song, S. and Huang, G. and Wu, C.},
  date = {2019},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  shortjournal = {IEEE Trans. Syst. Man Cybern. Syst.},
  volume = {49},
  number = {6},
  pages = {1194--1207},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {21682216 (ISSN)},
  doi = {10.1109/TSMC.2017.2735997},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040625343&doi=10.1109%2fTSMC.2017.2735997&partnerID=40&md5=7612c94f81a88d4fdfbaa47a8ea5cc10},
  abstract = {Extreme learning machines (ELMs), as 'generalized' single hidden layer feedforward networks, have been proved to be effective and efficient for classification and regression problems. Traditional ELMs assume that the training and testing data are drawn from the same distribution, which however is often violated in real-world applications. In this paper, we propose a unified cross-domain ELM (CDELM) framework to address domain adaptation problems, in which the distributions of training data (source domain) and testing data (target domain) are different but related. CDELM not only fully leverages labeled source data and unlabeled target data simultaneously to construct an adaptive target classifier but also maintains the computational efficiency of ELMs. Specifically, CDELM adapts the source classifier to target domain by matching the projected means of both domains, and explores the structure property of target domain by using manifold regularization to make the final classifier more adaptable to target data. Based on the framework, two algorithms CDELM-M and CDELM-C are proposed, which aim at minimizing the marginal and conditional distribution distance between source and target domains, respectively. Moreover, CDELM-C can further enhance the classification accuracy by multiple iterations. Comprehensive experimental studies on artificial datasets and public text and image datasets demonstrate that both CDELM-M and CDELM-C are competitive with several state-of-the-art domain adaptation learning methods in terms of the classification accuracy and efficiency. © 2013 IEEE.},
  langid = {english},
  keywords = {/unread,Algorithm design and analysis,Automobile engine manifolds,Classification (of information),Computational efficiency,Domain adaptation,Efficiency,extreme learning machine (ELM),Extreme learning machine (ELM),Kernel,Knowledge acquisition,Learning systems,manifold regularization,Manifold regularizations,Network layers,Personnel training,Prediction algorithms,projected maximum mean discrepancy (MMD),Support vector machines,Testing,Text processing,transfer learning,Transfer learning},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.999Z}
}

@book{liEpilepticSeizureDetection2017,
  title = {Epileptic {{Seizure Detection Using EEGs Based}} on {{Kernel Radius}} of {{Intrinsic Mode Functions}}},
  author = {Li, Q. and Ye, M. and Song, J.-L. and Zhang, R.},
  editorb = {{Klimenko S.} and {Huang Z.} and {Wang H.} and {Zhang Y.} and {Aickelin U.} and {Zhou R.} and {Siuly S.}},
  editorbtype = {redactor},
  date = {2017},
  journaltitle = {Lect. Notes Comput. Sci.},
  volume = {10594 LNCS},
  pages = {21},
  publisher = {{Springer Verlag}},
  doi = {10.1007/978-3-319-69182-4_2},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032484967&doi=10.1007%2f978-3-319-69182-4_2&partnerID=40&md5=1bacea6e5f49428568ed5e5dbf89a2df},
  abstract = {The study of automated epileptic seizure detection using EEGs has attracted more and more researchers in these decades. How to extract appropriate features in EEGs, which can be applied to differentiate non-seizure EEG from seizure EEG, is considered to be crucial in the successful realization. In this work, we proposed a novel kernel-radius-based feature extraction method from the perspective of nonlinear dynamics analysis. The given EEG signal is first decomposed into different numbers of intrinsic mode functions (IMFs) adaptively by using empirical mode decomposition. Then the three-dimensional phase space representation (3D-PSR) is reconstructed for each IMF according to the time delay method. At last, the kernel radius of the corresponding 3D-PSR is defined, which aims to characterize the concentration degree of all the points in 3D-PSR. With the extracted feature KRF, we employ extreme learning machine and support vector machine as the classifiers to achieve the task of the automate epileptic seizure detection. Performances of the proposed method are finally verified on the Bonn EEG database. © 2017, Springer International Publishing AG.},
  isbn = {03029743 (ISSN); 9783319691817 (ISBN)},
  langid = {english},
  pagetotal = {11},
  keywords = {/unread,Automatic seizure detection,Automatic seizure detections,Electro-encephalogram (EEG),Electroencephalogram (EEG),Electroencephalography,Empirical Mode Decomposition,Empirical mode decomposition (EMD),Extreme learning machine (ELM),Feature extraction,Functions,Kernel-radius-based feature,Knowledge acquisition,Learning systems,Molecular physics,Neurodegenerative diseases,Neurophysiology,Phase space methods,Phase space representation,Phase space representation (PSR),Signal processing,Support vector machine (SVM),Support vector machines,Vector spaces},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.976Z}
}

@article{liHybridSelfadaptiveLearning2013,
  title = {Hybrid Self-Adaptive Learning Based Particle Swarm Optimization and Support Vector Regression Model for Grade Estimation},
  author = {Li, X.-L. and Li, L.-H. and Zhang, B.-L. and Guo, Q.-J.},
  date = {2013},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {118},
  pages = {179--190},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2013.03.002},
  abstract = {Ore grade estimation is one of the key stages and the most complicated aspects in mining. Its complexity originates from scientific uncertainty. In this paper, a novel hybrid SLPSO-SVR model that hybridized the self-adaptive learning based particle swarm optimization (SLPSO) and support vector regression (SVR) is proposed for ore grade estimation. This hybrid SLPSO-SVR model searches for SVR's optimal parameters using self-adaptive learning based particle swarm optimization algorithms, and then adopts the optimal parameters to construct the SVR models. The SVR uses the 'Max-Margin' idea to search for an optimum hyperplane, and adopts the ε-insensitive loss function for minimizing the training error between the training data and identified function. The hybrid SLPSO-SVR grade estimation method has been tested on a number of real ore deposits. The result shows that method has advantages of rapid training, generality and accuracy grade estimation approach. It can provide with a very fast and robust alternative to the existing time-consuming methodologies for ore grade estimation. © 2013.},
  langid = {english},
  keywords = {/notrelevant,accuracy,Algorithms,article,artificial neural network,computer system,copper,Estimation,Grade estimations,hybrid,intermethod comparison,kernel method,learning algorithm,machine learning,mathematical model,nonlinear system,Optimal parameter,Ore deposits,Ore grade estimation,Ore grades,particle size,Particle swarm optimization (PSO),Particle swarm optimization algorithm,predictive value,priority journal,process optimization,Regression analysis,reliability,Scientific uncertainty,self adaptive learning based particle swarm optimization,Self-adaptive learning,Self-adaptive learning based particle swarm optimization,support vector machine,Support vector regression,Support vector regression (SVR),Support vector regression models,validation process},
  annotation = {28 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-25T13:17:14.663Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/citen scopus/Li et al_2013_Hybrid self-adaptive learning based particle swarm optimization and support.pdf}
}

@article{liInversefreeExtremeLearning2016,
  title = {Inverse-Free Extreme Learning Machine with Optimal Information Updating},
  author = {Li, S. and You, Z.-H. and Guo, H. and Luo, X. and Zhao, Z.-Q.},
  date = {2016},
  journaltitle = {IEEE Transactions on Cybernetics},
  shortjournal = {IEEE Trans. Cybern.},
  volume = {46},
  number = {5},
  pages = {1229--1241},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {21682267 (ISSN)},
  doi = {10.1109/TCYB.2015.2434841},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930468604&doi=10.1109%2fTCYB.2015.2434841&partnerID=40&md5=4a092e01c3c1b163cc06563cac4a50c1},
  abstract = {The extreme learning machine (ELM) has drawn insensitive research attentions due to its effectiveness in solving many machine learning problems. However, the matrix inversion operation involved in the algorithm is computational prohibitive and limits the wide applications of ELM in many scenarios. To overcome this problem, in this paper, we propose an inverse-free ELM to incrementally increase the number of hidden nodes, and update the connection weights progressively and optimally. Theoretical analysis proves the monotonic decrease of the training error with the proposed updating procedure and also proves the optimality in every updating step. Extensive numerical experiments show the effectiveness and accuracy of the proposed algorithm. © 2013 IEEE.},
  langid = {english},
  keywords = {Artificial intelligence,Connection weights,Extreme learning machine (ELM),Information updating,Inverse problems,inverse-free,Knowledge acquisition,Learning systems,Machine learning problem,Matrix inversions,Monotonic decrease,neural networks,Numerical experiments,optimal updates,Training errors},
  annotation = {90 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T13:05:28.641Z},
  file = {/home/leix/Documents/Zotero/storage/MURDPCUQ/li2016.pdf.pdf}
}

@article{liMultiplekernellearningbasedExtremeLearning2016,
  title = {Multiple-Kernel-Learning-Based Extreme Learning Machine for Classification Design},
  author = {Li, X. and Mao, W. and Jiang, W.},
  date = {2016},
  journaltitle = {Neural Computing and Applications},
  shortjournal = {Neural Comput. Appl.},
  volume = {27},
  number = {1},
  pages = {175--184},
  publisher = {{Springer-Verlag London Ltd}},
  issn = {09410643 (ISSN)},
  doi = {10.1007/s00521-014-1709-7},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953347931&doi=10.1007%2fs00521-014-1709-7&partnerID=40&md5=0010b835723b6b49dc3becccb985b244},
  abstract = {The extreme learning machine (ELM) is a new method for using single hidden layer feed-forward networks with a much simpler training method. While conventional kernel-based classifiers are based on a single kernel, in reality, it is often desirable to base classifiers on combinations of multiple kernels. In this paper, we propose the issue of multiple-kernel learning (MKL) for ELM by formulating it as a semi-infinite linear programming. We further extend this idea by integrating with techniques of MKL. The kernel function in this ELM formulation no longer needs to be fixed, but can be automatically learned as a combination of multiple kernels. Two formulations of multiple-kernel classifiers are proposed. The first one is based on a convex combination of the given base kernels, while the second one uses a convex combination of the so-called equivalent kernels. Empirically, the second formulation is particularly competitive. Experiments on a large number of both toy and real-world data sets (including high-magnification sampling rate image data set) show that the resultant classifier is fast and accurate and can also be easily trained by simply changing linear program. © 2014, The Natural Computing Applications Forum.},
  langid = {english},
  keywords = {Classification (of information),ELM kernel,Elm kernels,Extreme learning machine (ELM),Kernel learning,Knowledge acquisition,Learning systems,Linear programming,Minimal norm of weights,Multi-class ELM,Multiple-kernel learning (MKL),Network layers,QCQP,SILP,Virtual reality},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-03-09T21:44:02.684Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/elm kernel scopus/Li et al_2016_Multiple-kernel-learning-based extreme learning machine for classification.pdf}
}

@inproceedings{linComparativeStudySeveral2022,
  title = {A Comparative Study of Several Slfn-Based Classification Algorithms for Urban and Rural Land Use},
  booktitle = {{{ISPRS Ann}}. {{Photogramm}}. {{Remote Sens}}. {{Spat}}. {{Inf}}. {{Sci}}.},
  author = {Lin, Y. and Xie, G. and Zhang, T. and Yu, J. and Zhang, H. and Cai, J.},
  editor = {{Jiang J.} and {Shaker A.} and {Zhang H.}},
  date = {2022},
  volume = {5},
  number = {3},
  pages = {247--253},
  publisher = {{Copernicus GmbH}},
  doi = {10.5194/isprs-Annals-V-3-2022-247-2022},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132019876&doi=10.5194%2fisprs-Annals-V-3-2022-247-2022&partnerID=40&md5=349005c6ffc17713dce70a7af358d39d},
  abstract = {In the study of urban sustainable development, accurate classification of land use has become an important basis for monitoring urban dynamic changes. Hence it is necessary to develop the appropriate recognition model for urban-rural land use. Although deep learning algorithms have become a research hotspot in image classification tasks in recent years, and many good results have been achieved. But other machine learning algorithms are not going away. Compared deep learning with machine learning, there are some advantages and disadvantages in data dependence, hardware dependence, feature processing, problem solving methods, execution time, and interpretability, etc. Especially in the classification for remote sensing images, the continuous research and development of traditional machine learning algorithms is still of great significance. In this paper, the performances of several SLFN-based classification algorithms were studied and compared, including ELM, RBF K-ELM, mixed K-ELM, A-ELM and SVM. Extreme Learning Machine (ELM) is a new algorithm for single-hidden-layer feedforward neural network (SLFN). It has simple structure, fast speed and is easy to train. In some applications, however, standard ELM is prone to be overfitting and its performance will be affected seriously when outliers exist. In order to explore the performance of ELM and its improved algorithm for urban-rural land use classification, comparative experiments between three improved ELM algorithms (RBF K-ELM, mixed K-ELM and A-ELM), ELM and SVM with image data from several study areas were performed, and the classification accuracy and efficiency were analysed. The results show that the three improved ELM algorithms perform better than the standard ELM and SVM both in overall accuracy and Kappa coefficient. However, it is worth noting that the computation efficiency of RBF K-ELM and mixed K-ELM decreases greatly with larger image, the time cost is much more than other algorithms. Compared with other algorithms, A-ELM has the advantages of higher Overall Accuracy and less classification time. © Authors 2022.},
  isbn = {21949042 (ISSN)},
  langid = {english},
  keywords = {/unread,A-ELM,A-extreme learning machine,Classification algorithm,Deep learning,Efficiency,Extreme learning machine (ELM),Image classification,Image enhancement,kernel function,Kernel function,Knowledge acquisition,land use,Land use,Learning algorithms,Machine algorithm,Machine learning algorithms,machine learning.,Machine learning.,Multilayer neural networks,Network layers,Overall accuracies,Performance,Radial basis function networks,Remote sensing,Remote sensing image classification,remote-sensing image classification,Rural areas,Single-hidden layer feedforward neural networks,Support vector machines},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.024Z}
}

@article{linExtremeLearningMachine2015,
  title = {Is Extreme Learning Machine Feasible? {{A}} Theoretical Assessment ({{Part II}})},
  author = {Lin, S. and Liu, X. and Fang, J. and Xu, Z.},
  date = {2015},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  shortjournal = {IEEE Trans. Neural Networks Learn. Sys.},
  volume = {26},
  number = {1},
  pages = {21--34},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {2162237X (ISSN)},
  doi = {10.1109/TNNLS.2014.2336665},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919660197&doi=10.1109%2fTNNLS.2014.2336665&partnerID=40&md5=9a222bfb0091ddeed6990e48c6f5b16c},
  abstract = {An extreme learning machine (ELM) can be regarded as a two-stage feed-forward neural network (FNN) learning system that randomly assigns the connections with and within hidden neurons in the first stage and tunes the connections with output neurons in the second stage. Therefore, ELM training is essentially a linear learning problem, which significantly reduces the computational burden. Numerous applications show that such a computation burden reduction does not degrade the generalization capability. It has, however, been open that whether this is true in theory. The aim of this paper is to study the theoretical feasibility of ELM by analyzing the pros and cons of ELM. In the previous part of this topic, we pointed out that via appropriately selected activation functions, ELM does not degrade the generalization capability in the sense of expectation. In this paper, we launch the study in a different direction and show that the randomness of ELM also leads to certain negative consequences. On one hand, we find that the randomness causes an additional uncertainty problem of ELM, both in approximation and learning. On the other hand, we theoretically justify that there also exist activation functions such that the corresponding ELM degrades the generalization capability. In particular, we prove that the generalization capability of ELM with Gaussian kernel is essentially worse than that of FNN with Gaussian kernel. To facilitate the use of ELM, we also provide a remedy to such a degradation. We find that the well-developed coefficient regularization technique can essentially improve the generalization capability. The obtained results reveal the essential characteristic of ELM in a certain sense and give theoretical guidance concerning how to use ELM. © 2012 IEEE.},
  langid = {english},
  keywords = {Activation functions,algorithm,Algorithms,artificial intelligence,Artificial Intelligence,artificial neural network,biological model,Chemical activation,Coefficient regularizations,Computation theory,Computational burden,computer simulation,Computer Simulation,Essential characteristic,Extreme learning machine (ELM),Gaussian distribution,Gaussian kernel,Gaussian kernels,generalization capability,Generalization capability,human,Humans,Knowledge acquisition,learning,Learning,Learning systems,{Models, Neurological},nerve cell,neural networks,Neural networks,Neural Networks (Computer),Neurons,physiology,probability,Probability,Random processes,Uncertainty problems},
  annotation = {Read\_Status: To Read Read\_Status\_Date: 2023-02-26T20:42:01.655Z},
  file = {/home/leix/Documents/Zotero/storage/NNMQAA98/lin2015.pdf.pdf}
}

@article{linSpatiotemporalAnalysisOfwetland2018,
  title = {Spatio-Temporal Analysis Ofwetland Changes Using a Kernel Extreme Learning Machine Approach},
  author = {Lin, Y. and Yu, J. and Cai, J. and Sneeuw, N. and Li, F.},
  date = {2018},
  journaltitle = {Remote Sensing},
  shortjournal = {Remote Sens.},
  volume = {10},
  number = {7},
  publisher = {{MDPI AG}},
  issn = {20724292 (ISSN)},
  doi = {10.3390/rs10071129},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050478238&doi=10.3390%2frs10071129&partnerID=40&md5=2494b4adc281996adee339e97a7f783c},
  abstract = {Natural wetland ecosystems provide not only important habitats for many wildlife species, but also food for migratory and resident animals. In Shanghai, the Chongming Dongtan International Wetland, located at the mouth of the Yangtze River, plays an important role in maintaining both ecosystem health and ecological security of the island. Meanwhile it provides an especially important stopover and overwintering site for migratory birds, being located in the middle of the East Asian-Australasian Flyway. However, with the increase in development intensity and human activities, this wetland suffers from increasing environmental pressure. On the other hand, biological succession in the mudflat wetland makes Chongming Dongtan a rapidly developing and rare ecosystem in the world. Therefore, studying the wetland spatio-temporal change is an important precondition for analyzing the relationship between wetland evolution processes and human activities. This paper presents a novel method for analyzing land-use/cover changes (LUCC) on Chongming Dongtan wetland using multispectral satellite images. Our method mainly takes advantages of a machine learning algorithm, named the Kernel Extreme Learning Machine (K-ELM), which is applied to distinguish between different objects and extract their information from images. In the K-ELM, the kernel trick makes it more stable and accurate. The comparison between K-ELM and three other conventional classification methods indicates that the proposed K-ELM has the highest overall accuracy, especially for distinguishing between Spartina alternflora, Scirpus mariqueter, and Phragmites australis. Meanwhile, its efficiency is remarkable as well. Then a total of eight Landsat TM series images acquired from 1986 to 2013 were used for the LUCC analysis with K-ELM. According to the classification result, the change detection and spatio-temporal quantitative analysis were performed. The specific analysis of different objects are significant for learning about the historical changes to Chongming Dongtan and obtaining the evaluation rules. Generally, the rapid speed of Chongming Dongtan's urbanization brought about great influence with respect to natural resources and the environment. Integrating the results into the ecological analysis and ecological regional planning of Dongtan could provide a reliable scientific basis for rational planning, development, and the ecological balance and regional sustainability of the wetland area. © 2018 by the authors.},
  langid = {english},
  keywords = {/unread,Animals,Chongming Dongtan wetlands,Conventional classification methods,Dongtan wetland,Ecosystems,Environmental pressures,Extreme learning machine (ELM),Image analysis,Kernel extreme learning machine (K-ELM),Knowledge acquisition,Land use,Land use/cover change,Land-use/cover change (LUCC),LANDSAT TM,Landsat TM imagery,Learning algorithms,Learning systems,Multispectral satellite image,Neural networks,Planning,Regional planning,Spatio-temporal change analysis,Spatio-temporal changes,Sustainable development,Wetlands},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.988Z}
}

@inproceedings{liOptimizedMKELMBAS2020,
  title = {Optimized {{MK-ELM}} by {{BAS}} and Its {{Application}} in {{sEMG Gesture Recognition}}},
  booktitle = {Proc. - {{Int}}. {{Conf}}. {{Inf}}. {{Technol}}. {{Comput}}. {{Appl}}., {{ITCA}}},
  author = {Li, J. and Wu, Y. and Zhang, A. and Awudong, B. and Li, Q.},
  date = {2020},
  pages = {640--644},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/ITCA52113.2020.00139},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106039865&doi=10.1109%2fITCA52113.2020.00139&partnerID=40&md5=e2339b15084930031485525fcc0ad2fa},
  abstract = {In order to solve the problem of insufficient generalization performance and kernel parameter selection of the kernel extreme learning machine, a new model based on optimized multiple kernel extreme learning machine by beetle antennae search algorithm (BAS-MK-ELM) is proposed. In the model, the weighted combination of polynomial kernel function and Gaussian kernel function is selected as the kernel function of multiple kernel extreme learning machine (MK-ELM), and beetle antennae search algorithm is introduced to optimize the kernel function parameters to get the optimal kernel function parameters. This classification model is applied to sEMG gesture recognition, and four time-domain features of the pre-processed signalsl are extracted, and signal features are classified and recognized by BAS-MK-ELM. The experimental results show that the average recognition rate of the proposed classification model is higher than that of single-kernel extreme learning machine, multi-kernel support vector machine, MK-ELM and particle swarm optimization multi-kernel extreme learning machine, which verifies the effectiveness and superiority of the proposed classification model. © 2020 IEEE.},
  isbn = {9780738111414 (ISBN)},
  langid = {english},
  keywords = {/unread,BAS,Classification,Classification models,Extreme learning machine (ELM),Gaussian kernel functions,Generalization performance,Gesture recognition,kernel ELM,Kernel function parameters,Knowledge acquisition,Learning algorithms,Learning systems,Parameter estimation,Particle swarm optimization (PSO),Polynomial kernels,Search Algorithms,sEMG,Support vector machines,Time domain features},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.006Z}
}

@article{liSoftSensorTechnical2012,
  title = {Soft sensor of technical indices based on KPCA-ELM and application for flotation process},
  author = {Li, H. and Chai, T. and Yue, H.},
  date = {2012},
  journaltitle = {Huagong Xuebao/CIESC Journal},
  shortjournal = {Huagong Xuebao},
  volume = {63},
  number = {9},
  pages = {2892--2898},
  issn = {04381157 (ISSN)},
  doi = {10.3969/j.issn.0438-1157.2012.09.035},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867423289&doi=10.3969%2fj.issn.0438-1157.2012.09.035&partnerID=40&md5=0c6d757edf3fcbd2b5ad7deaf3f76fba},
  abstract = {In the flotation process, the concentrate grade and the tailing grade are crucial technical indices which can not be measured online continuously. They can hardly be described using accurate mathematical model for strong nonlinearities and uncertainties among technical indices and operating variables, mainly measured off-line by artificial laboratory. The long cycle of artificial laboratory is difficult to meet the control requirement of grade indices, so study of grade indices soft measurement method attracts more attention. By analyzing the relations between the technical indices and such boundary variables, a soft sensor model of technical indices based kernel principal component analysis(KPCA) and extreme learning machine(ELM) was proposed innovatively to estimate the concentrate grade and the tailing grade. To solve the outliers, missing data points of the outliers and deviation from normal values are detected. KPCA is applied to compress the input data, and select the nonlinear principle component. ELM is used to process regression modeling. The proposed model is successfully applied to the flotation process of a hematite ore processing plant in China. Industrial application results show that the soft sensor model has high accuracy and guidance to real production. © All Rights Reserved.},
  langid = {chinese},
  keywords = {/unread,Boundary variables,Concentrate grade,Control requirements,ELM,Extreme learning machine (ELM),Flotation,Flotation process,Industrial applications,Input datas,Kernel principal component analyses (KPCA),KPCA,Long cycles,Mathematical models,Missing data,Nonlinear principle components,Operating variables,Principal component analysis,Processing plants,Regression modeling,Sensors,Soft measurement method,Soft sensor,Soft sensor models,Soft sensors,Statistics,Strong nonlinearity,Tailing grade,Technical index},
  annotation = {Read\_Status: Not Reading Read\_Status\_Date: 2023-02-27T18:47:58.610Z}
}

@article{liTriangularHermiteKernel2016,
  title = {Triangular {{Hermite}} Kernel Extreme Learning Machine},
  author = {Li, Y. and Zhang, H. and Ji, X.},
  date = {2016},
  journaltitle = {International Journal of Innovative Computing, Information and Control},
  shortjournal = {Int. J. Innov. Comput. Inf. Control},
  volume = {12},
  number = {6},
  pages = {1893--1904},
  publisher = {{ICIC International}},
  issn = {13494198 (ISSN)},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995450975&partnerID=40&md5=92cce0ada2cbb5c0e7438f6c5a08ef8a},
  abstract = {The triangular Hermite kernel extreme learning machine methodology is presented based on Hermite polynomial. It introduces the triangular Hermite function which has been-proved as a valid kernel function into extreme learning machine as kernel function. The most significant advantages of proposed kernel are that it has only one parameter chosen from a small set of natural numbers, thus the parameter optimization is facilitated greatly, and more structure information of sample data is retained. Experiments were performed on bi-spiral benchmark data set as well as a number of regression datasets from the UCI benchmark repository. Similar or better robustness and generalization performance of the proposed method in comparison- to other extreme learning machine with different kernels and SVM (Support Vector Machine) methods demonstrates its effectiveness and usefulness. © 2016 ICIC INTERNATIONAL.},
  langid = {english},
  keywords = {Extreme learning machine (ELM),Hermite orthogonal polynomial,Hermite orthogonal polynomials,Kernel extreme learning machine,Kernel function,Kernel parameter,Kernel selection,Knowledge acquisition,Learning systems,Neural networks,Orthogonal functions,Polynomials,Support vector machines,Triangular Hermite kernel function},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-27T13:13:02.312Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/citen scopus/Li et al_2016_Triangular Hermite kernel extreme learning machine.pdf}
}

@article{liu2DDefectProfile2014,
  title = {2-{{D}} Defect Profile Reconstruction from Ultrasonic Guided Wave Signals Based on {{QGA-kernelized ELM}}},
  author = {Liu, B. and Tang, L. and Wang, J. and Li, A. and Hao, Y.},
  date = {2014},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {128},
  pages = {217--223},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2012.11.053},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893671804&doi=10.1016%2fj.neucom.2012.11.053&partnerID=40&md5=f24cd5e4eaabb69b3a266618cd651601},
  abstract = {The reconstruction of defect profiles based on ultrasonic guided waves means the acquisition of defect profiles and parameters from ultrasonic guided wave inspection signals, and it is the key for the inversion of ultrasonic guided waves. A method for the reconstruction of 2-D profiles based on kernelized extreme learning machine (ELM) is presented, and quantum genetic algorithm (QGA) is adopted to optimize the cost parameter C and kernel parameter γ of kernelized ELM. The input data sets of kernelized ELM are defect echo signals, and the output data sets are 2-D profile parameters. The mapping from defect echo signals to 2-D profiles is established. The sample database is achieved by practical experiments and numerical simulations. Then, 2-D profile reconstruction of artificial defects in ultrasonic guided wave testing is implemented with QGA-kernelized ELM. To compare the generalization performance and reconstruction results, another reconstruction model based on LS-SVM is designed simultaneously with the same kernel. Finally, experimental results indicate that proposed method possesses faster speed, lower computational complexity and better generalization performance, and it is a feasible and effective approach to reconstruct 2-D defect profiles. © 2013 Elsevier B.V.},
  langid = {english},
  keywords = {accuracy,article,controlled study,Defects,Extreme learning machine (ELM),Genetic algorithms,Guided electromagnetic wave propagation,intermethod comparison,Kernelized extreme learning machine,Knowledge acquisition,Least squares support vector machine,Least squares support vector machines,machine learning,mathematical analysis,mathematical and statistical procedures,mathematical computing,mathematical model,nonlinear system,priority journal,process optimization,Profile reconstruction,Profile Reconstruction,Quantum genetic algorithm,quantum genetic algorithm kernelized extreme learning machine,regression analysis,signal processing,support vector machine,Support vector machines,two dimensional defect profile reconstruction,Ultrasonic guided wave,Ultrasonic waves,ultrasound},
  annotation = {23 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-25T22:37:36.083Z},
  file = {/home/leix/Documents/Zotero/storage/7ZHLUPHE/liu2014.pdf.pdf}
}

@book{liu2DDefectProfile2016,
  title = {2-{{D}} Defect Profile Reconstruction from Ultrasonic Guided Wave Signals Based on Radial Wavelet Basis Function Neural Network with {{ELM}}},
  author = {Liu, B. and Tang, L. and Yu, X. and Wang, J. and Li, A.},
  date = {2016},
  journaltitle = {Key Eng Mat},
  series = {Key {{Engineering Materials}}},
  volume = {693},
  pages = {1561},
  publisher = {{Trans Tech Publications Ltd}},
  doi = {10.4028/www.scientific.net/KEM.693.1551},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969814708&doi=10.4028%2fwww.scientific.net%2fKEM.693.1551&partnerID=40&md5=0fe6f9ba50d143714332f20aeb0192cf},
  abstract = {The reconstruction of defect profiles based on ultrasonic guided waves means the acquisition of defect profiles and parameters from ultrasonic guided wave signals. To achieve multi-resolution approximation, this paper proposed a reconstruction approach based on Radial Wavelet Basis Function Neural Network (RWBFNN), which combines wavelet analysis and neural network. Gaussian radial basis functions and Mexican hat wavelet frames are used as scaling functions and wavelet functions respectively. The training and testing samples contain simulation data and experimental data. The input data sets are defect echo signals, and the output data sets are 2-D profile parameters. To reduce the training time and simplify the profile reconstruction procedure without losing accuracy, Extreme Learning Machine (ELM) is adopted simultaneously. The results indicate that significant advantages can be obtained over other defect profile reconstruction schemes, and the accuracy of the predicted defect profile can be controlled by the resolution of the network with the lower computational complexity. © 2016 Trans Tech Publications, Switzerland.},
  isbn = {10139826 (ISSN)},
  langid = {english},
  pagetotal = {1551},
  keywords = {Extreme Learning Machine,Extreme learning machine (ELM),Functions,Guided electromagnetic wave propagation,Knowledge acquisition,Machine learning,Multi-resolution Approximation,Neural Network,Neural networks,Profile Reconstruction,Radial basis function networks,Radial Wavelet Basis Function Neural Network,Radial wavelets,Signal reconstruction,Ultrasonic guided wave,Ultrasonic Guided Wave,Ultrasonic waves},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-27T13:46:25.289Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/citen scopus/Liu et al_2016_2-D defect profile reconstruction from ultrasonic guided wave signals based on.pdf}
}

@inproceedings{liuAwarenessCrimerelatedInformation2019,
  title = {Awareness of {{Crime-related Information}} and {{Concealed Information Detection}} Method},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. {{Cogn}}. {{Informatics Cogn}}. {{Comput}}., {{ICCI}}*{{CC}}},
  author = {Liu, Q. and Liu, H.-G. and Zhang, L.},
  editor = {{Soda P.} and {Fiorini R.A.} and {Wang Y.} and {Jacobs G.} and {Howard N.} and {Widrow B.} and {Feldman J.}},
  date = {2019},
  pages = {372--378},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/ICCICC46617.2019.9146100},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094649197&doi=10.1109%2fICCICC46617.2019.9146100&partnerID=40&md5=6c3359cc513dfd159be81507aac8a55e},
  abstract = {Concealed information test based on P300 and machine learning has become increasingly popular in the fields of cognitive psychology. Numerous studies have set up mock crime scenario to identify changes in EEG cognitive components. However, only two kinds of subjects are taken into account in most previous studies. Therefore, if an innocent person had been aware of case-related information, i.e., probe items, which is likely to happen in practice, recognition capability will be significantly compromised. In order to simulate practical cases, three kinds of subjects needed to be discriminated, including guilty, innocent and informed. 36 subjects went through a mock crime scenario, and EEG signals obtained on 8 electrodes were analyzed. After preprocessing, the discrete wavelet packet decomposition was used to extract EEG features. Subsequently, a multi-scale wavelet kernel extreme learning machine classifier is proposed to recognize the group to which a specific subject belongs. To further reduce computation, Cholesky decomposition is introduced during the calculation of the output weights. Our results demonstrate that the proposed algorithm can achieve good recognition performance and has low computational burden. © 2019 IEEE.},
  isbn = {9781728114194 (ISBN)},
  langid = {english},
  keywords = {/unread,Awareness of Crime-related Information,Cholesky decomposition,CIT,Cognitive components,Cognitive psychology,Computational burden,Congnitive machine learning,Crime,Discrete wavelets,ELM,Extreme learning machine (ELM),Information detection,Machine learning,Multi-scale wavelet,Multi-scale wavelet kernel,Wavelet decomposition},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.002Z}
}

@article{liuExtremeKernelSparse2017,
  title = {Extreme {{Kernel Sparse Learning}} for {{Tactile Object Recognition}}},
  author = {Liu, H. and Qin, J. and Sun, F. and Guo, D.},
  date = {2017},
  journaltitle = {IEEE Transactions on Cybernetics},
  shortjournal = {IEEE Trans. Cybern.},
  volume = {47},
  number = {12},
  pages = {4509--4520},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {21682267 (ISSN)},
  doi = {10.1109/TCYB.2016.2614809},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992093286&doi=10.1109%2fTCYB.2016.2614809&partnerID=40&md5=4d43353d20d79e8dc564bb13850dbae4},
  abstract = {Tactile sensors play very important role for robot perception in the dynamic or unknown environment. However, the tactile object recognition exhibits great challenges in practical scenarios. In this paper, we address this problem by developing an extreme kernel sparse learning methodology. This method combines the advantages of extreme learning machine and kernel sparse learning by simultaneously addressing the dictionary learning and the classifier design problems. Furthermore, to tackle the intrinsic difficulties which are introduced by the representer theorem, we develop a reduced kernel dictionary learning method by introducing row-sparsity constraint. A globally convergent algorithm is developed to solve the optimization problem and the theoretical proof is provided. Finally, we perform extensive experimental validations on some public available tactile sequence datasets and show the advantages of the proposed method. © 2013 IEEE.},
  langid = {english},
  keywords = {/unread,algorithm,article,classifier,Dictionary learning,Experimental validations,Extreme learning machine (ELM),Globally convergent algorithms,kernel dictionary learning,Learning systems,machine learning,Object recognition,Optimization,Optimization problems,Problem solving,recognition,Representer theorem,Sparsity constraints,tactile object recognition,theoretical study,Unknown environments},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.977Z}
}

@article{liuExtremeLearningMachine2015,
  title = {Is Extreme Learning Machine Feasible? {{A}} Theoretical Assessment ({{Part I}})},
  author = {Liu, X. and Lin, S. and Fang, J. and Xu, Z.},
  date = {2015},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  shortjournal = {IEEE Trans. Neural Networks Learn. Sys.},
  volume = {26},
  number = {1},
  pages = {7--20},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {2162237X (ISSN)},
  doi = {10.1109/TNNLS.2014.2335212},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919642556&doi=10.1109%2fTNNLS.2014.2335212&partnerID=40&md5=794d28f51bb82b77e0abf21a67553906},
  abstract = {An extreme learning machine (ELM) is a feedforward neural network (FNN) like learning system whose connections with output neurons are adjustable, while the connections with and within hidden neurons are randomly fixed. Numerous applications have demonstrated the feasibility and high efficiency of ELM-like systems. It has, however, been open if this is true for any general applications. In this two-part paper, we conduct a comprehensive feasibility analysis of ELM. In Part I, we provide an answer to the question by theoretically justifying the following: 1) for some suitable activation functions, such as polynomials, Nadaraya-Watson and sigmoid functions, the ELM-like systems can attain the theoretical generalization bound of the FNNs with all connections adjusted, i.e., they do not degrade the generalization capability of the FNNs even when the connections with and within hidden neurons are randomly fixed; 2) the number of hidden neurons needed for an ELM-like system to achieve the theoretical bound can be estimated; and 3) whenever the activation function is taken as polynomial, the deduced hidden layer output matrix is of full column-rank, therefore the generalized inverse technique can be efficiently applied to yield the solution of an ELM-like system, and, furthermore, for the nonpolynomial case, the Tikhonov regularization can be applied to guarantee the weak regularity while not sacrificing the generalization capability. In Part II, however, we reveal a different aspect of the feasibility of ELM: there also exists some activation functions, which makes the corresponding ELM degrade the generalization capability. The obtained results underlie the feasibility and efficiency of ELM-like systems, and yield various generalizations and improvements of the systems as well. © 2012 IEEE.},
  langid = {english},
  keywords = {Activation functions,algorithm,Algorithms,artificial intelligence,Artificial Intelligence,artificial neural network,biological model,Chemical activation,computer simulation,Computer Simulation,Extreme learning machine (ELM),feasibility,Feasibility analysis,Feedforward neural networks,Generalization (Psychology),Generalization bound,generalization capability,Generalization capability,human,Humans,Knowledge acquisition,learning,Learning,Learning systems,{Models, Neurological},nerve cell,nerve cell network,Nerve Net,Neural networks,Neural Networks (Computer),neural networks.,Neurons,Number of hidden neurons,physiology,Polynomials,Tikhonov regularization},
  annotation = {124 citations (Crossref) [2023-02-21] Read\_Status: In Progress Read\_Status\_Date: 2023-02-26T20:38:38.347Z},
  file = {/home/leix/Documents/Zotero/storage/3FZQPX98/liu2015.pdf.pdf}
}

@article{liuExtremeSpectralRegression2015,
  title = {Extreme Spectral Regression for Efficient Regularized Subspace Learning},
  author = {Liu, B. and Xia, S.-X. and Meng, F.-R. and Zhou, Y.},
  date = {2015},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {149},
  pages = {171--179},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2013.09.073},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922015998&doi=10.1016%2fj.neucom.2013.09.073&partnerID=40&md5=4020c2ae6b70ac8fb860d871e13876b8},
  abstract = {Traditional manifold learning algorithms, such as Locally Linear Embedding, Isomap and Laplacian Eigenmap, only provide the embedding results of training samples. Although many extensions of these approaches try to solve the out-of-sample extension problem, their computations cannot avoid eigen-decomposition of dense matrices which is expensive in both time and memory. To solve this problem, spectral regression (SR) casts the problem of learning an embedding function into a regression framework. Motivated by the effectiveness of extreme learning machine (ELM), in this paper, we solve the out-of-sample extension problem by seeking an embedding function in ELM feature space. An extreme spectral regression (ESR) algorithm is proposed to speed up kernel-based SR (KSR) further. In addition, it is proved that ESR is an approximation of KSR. Similar to SR, the proposed ESR algorithm can be performed in supervised, unsupervised and semi-supervised situation. Experimental results on classification and semi-supervised classification demonstrate the effectiveness and efficiency of our algorithm. © 2014 Elsevier B.V.},
  issue = {Part A},
  langid = {english},
  keywords = {Article,classifier,controlled study,Dimensionality reduction,Effectiveness and efficiencies,Electron spin resonance spectroscopy,Extreme learning machine (ELM),extreme spectral regression,facial expression,kernel based spectral regression,Knowledge acquisition,learning algorithm,Learning algorithms,Locally linear embedding,machine learning,Manifold learning algorithm,mathematical model,measurement accuracy,Out-of-sample extension,Problem solving,Regression analysis,Semi-supervised classification,Spectral regression,Spectral regressions,Supervised learning,theoretical study},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-03-09T21:31:07.738Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/elm kernel scopus/Liu et al_2015_Extreme spectral regression for efficient regularized subspace learning.pdf}
}

@article{liuFastFaultDiagnosis2019,
  title = {A Fast Fault Diagnosis Method of the {{PEMFC}} System Based on Extreme Learning Machine and Dempster-Shafer Evidence Theory},
  author = {Liu, J. and Li, Q. and Chen, W. and Yan, Y. and Wang, X.},
  date = {2019},
  journaltitle = {IEEE Transactions on Transportation Electrification},
  shortjournal = {IEEE Trans. Transp. Electrif.},
  volume = {5},
  number = {1},
  pages = {271--284},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {23327782 (ISSN)},
  doi = {10.1109/TTE.2018.2886153},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058878092&doi=10.1109%2fTTE.2018.2886153&partnerID=40&md5=6908ae10c17b3a8aae3897739910606b},
  abstract = {For purpose of solving the data-driven failure diagnosis problems of the proton exchange membrane fuel cell (PEMFC) system, improve the test accuracy and shorten the training time, a novel failure diagnosis method of the PEMFC systems based on data fusion is proposed, which combines extreme learning machine (ELM) and Dempster-Shafer (D-S) evidence theory. The characteristic vector extraction is carried out on the electrical quantities and the nonelectrical quantities of the PEMFC system under four different faults. The kernel ELM algorithm and online sequential ELM algorithm are, respectively, used to establish the failure diagnosis model of the PEMFC system based on electrical quantities and nonelectrical quantities. It is used for preliminary failure diagnosis of a PEMFC system. The diagnosis results of the above-mentioned two strategies are converted into the function values of the basic probability assignment by the squeeze function. The D-S evidence theory algorithm is used to fuse the diagnostic output at the decision level. The classification results of 154 samples of PEMFC system show that the novel model can diagnose four different degrees of high air stoichiometry failures. The average recognition rate is 98.70\% and the operation time is only 0.2011 s. At the same time, the comparisons with the back-propagation neural network and one-against-one support vector machine show that the data fusion algorithm can significantly improve the running speed while ensuring the correct recognition rate. It can be used for online failure diagnosis of the PEMFC systems. © 2018 IEEE.},
  langid = {english},
  keywords = {/unread,Back-propagation neural networks,Backpropagation algorithms,Basic probability assignment,Classification results,Coolants,Data fusion,Decision theory,Dempster Shafer evidence theory,Dempster-Shafer (D-S) evidence theory,E-learning,Extreme learning machine (ELM),Failure analysis,Fault detection,fault diagnosis,Hydrogen,kernel extreme learning machine(K-ELM),Knowledge acquisition,Neural networks,One-against-one support vector machines,online sequential ELM (OS-ELM),Online sequential extreme learning machine,Online systems,Personnel training,proton exchange membrane fuel cell (PEMFC),Proton exchange membrane fuel cell systems (PEMFC),Proton exchange membrane fuel cells (PEMFC),Support vector machines,Transportation},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.001Z}
}

@article{liuMultipleKernelExtreme2015,
  title = {Multiple Kernel Extreme Learning Machine},
  author = {Liu, X. and Wang, L. and Huang, G.-B. and Zhang, J. and Yin, J.},
  date = {2015},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {149},
  pages = {253--264},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2013.09.072},
  abstract = {Extreme learning machine (ELM) has been an important research topic over the last decade due to its high efficiency, easy-implementation, unification of classification and regression, and unification of binary and multi-class learning tasks. Though integrating these advantages, existing ELM algorithms pay little attention to optimizing the choice of kernels, which is indeed crucial to the performance of ELM in applications. More importantly, there is the lack of a general framework for ELM to integrate multiple heterogeneous data sources for classification. In this paper, we propose a general learning framework, termed multiple kernel extreme learning machines (MK-ELM), to address the above two issues. In the proposed MK-ELM, the optimal kernel combination weights and the structural parameters of ELM are jointly optimized. Following recent research on support vector machine (SVM) based MKL algorithms, we first design a sparse MK-ELM algorithm by imposing an ℓ1-norm constraint on the kernel combination weights, and then extend it to a non-sparse scenario by substituting the ℓ1-norm constraint with an ℓp-norm (p\&gt;1) constraint. After that, a radius-incorporated MK-ELM algorithm which incorporates the radius of the minimum enclosing ball (MEB) is introduced. Three efficient optimization algorithms are proposed to solve the corresponding kernel learning problems. Comprehensive experiments have been conducted on Protein, Oxford Flower17, Caltech101 and Alzheimer's disease data sets to evaluate the performance of the proposed algorithms in terms of classification accuracy and computational efficiency. As the experimental results indicate, our proposed algorithms can achieve comparable or even better classification performance than state-of-the-art MKL algorithms, while incurring much less computational cost. © 2014 Elsevier B.V.},
  issue = {Part A},
  langid = {english},
  keywords = {accuracy,algorithm,Alzheimer disease,Article,brain region,calculation,Classification (of information),Classification accuracy,Classification performance,Computational efficiency,controlled study,Efficiency,Extreme learning machine (ELM),gray matter,hippocampus,kernel method,Knowledge acquisition,Learning systems,machine learning,mathematical analysis,mild cognitive impairment,Minimum enclosing ball,Multi-class learning,Multiple heterogeneous data source,Multiple kernel learning,Multiple Kernel Learning,Neurodegenerative diseases,Optimization algorithms,prediction,process optimization,protein folding,support vector machine,Support vector machines,validity},
  annotation = {138 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T12:09:38.686Z},
  file = {/home/leix/Documents/Zotero/storage/66UMPEYL/liu2015.pdf.pdf}
}

@inproceedings{liuMultiscaleWaveletKernel2015,
  title = {Multi-Scale Wavelet Kernel Extreme Learning Machine for {{EEG}} Feature Classification},
  booktitle = {{{IEEE Int}}. {{Conf}}. {{Cyber Technol}}. {{Autom}}., {{Control}}, {{Intell}}. {{Syst}}., {{CYBER}}},
  author = {Liu, Q. and Zhao, X.-G. and Hou, Z.-G. and Liu, H.-G.},
  date = {2015},
  pages = {1546--1551},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/CYBER.2015.7288175},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962311512&doi=10.1109%2fCYBER.2015.7288175&partnerID=40&md5=9d6aeb634a93ce839a5be5536165ce66},
  abstract = {In this paper, the principle of the kernel extreme learning machine (ELM) is analyzed. Based on that, we introduce a kind of multi-scale wavelet kernel extreme learning machine classifier and apply it to electroencephalographic (EEG) signal feature classification. Experiments show that our classifier achieves excellent performance. © 2015 IEEE.},
  isbn = {9781479987290 (ISBN)},
  langid = {english},
  keywords = {Classification (of information),EEG classification,Electroencephalographic signals,Electroencephalography,ELM,Extreme learning machine (ELM),Feature classification,Intelligent systems,Knowledge acquisition,Learning systems,Multi-scale wavelet,multi-scale wavelet kernel,Neural networks},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-03-09T21:33:24.234Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/elm kernel scopus/Liu et al_2015_Multi-scale wavelet kernel extreme learning machine for EEG feature.pdf}
}

@article{liuRealTimeTrafficLight2017,
  title = {Real-{{Time Traffic Light Recognition Based}} on {{Smartphone Platforms}}},
  author = {Liu, W. and Li, S. and Lv, J. and Yu, B. and Zhou, T. and Yuan, H. and Zhao, H.},
  date = {2017},
  journaltitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  shortjournal = {IEEE Trans Circuits Syst Video Technol},
  volume = {27},
  number = {5},
  pages = {1118--1131},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {10518215 (ISSN)},
  doi = {10.1109/TCSVT.2016.2515338},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018905462&doi=10.1109%2fTCSVT.2016.2515338&partnerID=40&md5=30bd55f136977ce62682434bc354f500},
  abstract = {Traffic light recognition is of great significance for driver assistance or autonomous driving. In this paper, a traffic light recognition system based on smartphone platforms is proposed. First, an ellipsoid geometry threshold model in Hue Saturation Lightness color space is built to extract interesting color regions. These regions are further screened with a postprocessing step to obtain candidate regions that satisfy both color and brightness conditions. Second, a new kernel function is proposed to effectively combine two heterogeneous features, histograms of oriented gradients and local binary pattern, which is used to describe the candidate regions of traffic light. A kernel extreme learning machine (K-ELM) is designed to validate these candidate regions and simultaneously recognize the phase and type of traffic lights. Furthermore, a spatial-temporal analysis framework based on a finite-state machine is introduced to enhance the reliability of the recognition of the phase and type of traffic light. Finally, a prototype of the proposed system is implemented on a Samsung Note 3 smartphone. To achieve a real-time computational performance of the proposed K-ELM, a CPU-GPU fusion-based approach is adopted to accelerate the execution. The experimental results on different road environments show that the proposed system can recognize traffic lights accurately and rapidly. © 1991-2012 IEEE.},
  langid = {english},
  keywords = {/unread,Automobile drivers,Color,Computational performance,Extreme learning machine (ELM),Finite automata,Finite-state machine,geometry threshold model,Heterogeneous features,Histograms of oriented gradients,kernel extreme learning machine (K-ELM),Knowledge acquisition,Learning systems,Local binary patterns,Neural networks,Reliability analysis,smartphone,Smartphones,Spatial temporal analysis,Threshold model,Traffic light,traffic light recognition},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.978Z}
}

@book{liuSpectralDataModeling2012,
  title = {Spectral Data Modeling Based on Feature Extraction and Extreme Support Vector Regression},
  author = {Liu, S. and Yan, D. and Liu, Z. and Tang, J.},
  date = {2012},
  journaltitle = {Appl. Mech. Mater.},
  volume = {128--129},
  pages = {300},
  doi = {10.4028/www.scientific.net/AMM.128-129.297},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-81055144184&doi=10.4028%2fwww.scientific.net%2fAMM.128-129.297&partnerID=40&md5=cc151cc749aa652c4ffa45270ac09f26},
  abstract = {Spectral data such as near-infrared spectrum and frequency spectrum can simply the modeling of the difficulty-to-measured parameters. A novel modeling approach combined the feature extraction with extreme support vector regression (ESVR) is proposed. The latent variables space based feature extraction method can successfully complete the dimension reduction and independent variable extraction. The novel proposed ESVR leaning algorithm is realized by using extreme learning machine (ELM) kernel as SVR kernel, which is used to construct final models with better generalization. The experimental results based on the orange juice near-infrared spectra demonstrate that the proposed approach has better generalization performance and prediction accuracy.},
  isbn = {16609336 (ISSN); 9783037852842 (ISBN)},
  langid = {english},
  pagetotal = {297},
  keywords = {Dimension reduction,ELM kernel,Extreme learning machine (ELM),Feature extraction,Feature extraction methods,Frequency spectra,Fruit juices,Generalization performance,Independent variables,Infrared devices,Latent variable,Learning systems,Modeling approach,Near infrared spectra,Orange juice,Prediction accuracy,Regression analysis,Space-based,Spectral data,Spectroscopy,Support vector regression (SVR),Support vector regressions},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-27T18:48:39.112Z},
  file = {/home/leix/Documents/Zotero/storage/U8QCJYUI/Liu et al. - 2012 - Spectral data modeling based on feature extraction.pdf}
}

@article{liuSpectralRegressionBased2018,
  title = {Spectral Regression Based Marginal {{Fisher}} Analysis Dimensionality Reduction Algorithm},
  author = {Liu, B. and Zhou, Y. and Xia, Z.-G. and Liu, P. and Yan, Q.-Y. and Xu, H.},
  date = {2018},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {277},
  pages = {101--107},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2017.05.097},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029584453&doi=10.1016%2fj.neucom.2017.05.097&partnerID=40&md5=6aea914aa98567a1444281df4b792ddc},
  abstract = {Traditional nonlinear dimensionality reduction methods, such as multiple kernel dimensionality reduction and nonlinear spectral regression (SR), are generally regarded as extended versions of linear discriminant analysis (LDA) in the supervised case. As is well known, LDA has the restrictive assumption that the data of each class is of a Gaussian distribution. Thus, the performance of these methods will be degraded if such an assumption is not hold. Although some methods based on marginal Fisher analysis are proposed to overcome the drawback of LDA, they have to solve the problem of dense metrics generalized eigenvalue decomposition, which is very time-consuming. To address these issues, in this paper, marginal Fisher analysis criterion based on extreme learning machine (ELM) is proposed to improve spectral regression and kernel marginal Fisher analysis. It is proved that the proposed marginal Fisher analysis is a special case of traditional kernel marginal Fisher analysis. Based on the proposed criterion, a novel supervised dimensionality reduction algorithm is presented by virtue of ELM and spectral regression. Experimental results on benchmark datasets validate that the proposed algorithm outperforms the state-of-the-art nonlinear dimensionality reduction methods in supervised scenarios. © 2017 Elsevier B.V.},
  langid = {english},
  keywords = {/unread,analytic method,Article,Benchmark datasets,clinical effectiveness,conceptual framework,data base,Dimensionality reduction,Dimensionality reduction algorithms,discriminant analysis,Discriminant analysis,Eigenvalues and eigenfunctions,Extreme learning machine (ELM),Generalized eigenvalue decomposition,Image recognition,kernel marginal Fisher analysis,Knowledge acquisition,Learning systems,Linear discriminant analysis,machine learning,Marginal fisher analysis,Marginal Fisher analysis,mathematical analysis,mathematical parameters,Nonlinear analysis,Nonlinear dimensionality reduction,nonlinear regression analysis,priority journal,Spectral regression,Spectral regressions},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.989Z}
}

@article{liuUniversalConsistencyExtreme2018,
  title = {The Universal Consistency of Extreme Learning Machine},
  author = {Liu, Xia and Xu, Lin},
  date = {2018-10},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {311},
  pages = {176--182},
  issn = {09252312},
  doi = {10.1016/j.neucom.2018.05.066},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231218306519},
  urldate = {2023-03-10},
  langid = {english},
  annotation = {8 citations (Crossref) [2023-03-10] Read\_Status: Read Read\_Status\_Date: 2023-03-10T15:44:27.988Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/citen scopus/Liu_Xu_2018_The universal consistency of extreme learning machine.pdf}
}

@article{liwangELMKernelWavelet2019,
  title = {{{ELM}}\_{{Kernel}} and {{Wavelet Packet Decomposition Based EEG Classification Algorithm}}},
  author = {{Li Wang} and Lan, Z. and Wang, Q. and Yang, R. and Li, H.},
  date = {2019},
  journaltitle = {Automatic Control and Computer Sciences},
  shortjournal = {Autom. Control Comput. Sci.},
  volume = {53},
  number = {5},
  pages = {452--460},
  publisher = {{Pleiades Publishing}},
  issn = {01464116 (ISSN)},
  doi = {10.3103/S0146411619050079},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075167461&doi=10.3103%2fS0146411619050079&partnerID=40&md5=0bfc6c96f9474340ae27e877f00f7931},
  abstract = {Abstract: Rehabilitation technology based on brain-computer interface (BCI) has become a promising approach for patients with dyskinesia to regain movement. In this paper, a novel classification algorithm is proposed based on the characteristic of electroencephalogram (EEG) signals. Specifically wavelet packet decomposition (WPD) and Extreme learning machine with kernel (ELM\_Kernel) algorithm are studied. In view of the existence of cross-banding of WPD, the average energy of the wavelet packets of the corresponding frequency bands which belong to the mu and beta rhythm are used to form the feature vectors that are classified by the ELM\_Kernel algorithm. Simulation results demonstrate that the proposed algorithm produces a high probability of correct classification of 97.8\% and outperforms state-of-the-art algorithms such as ELM, BP and SVM in terms of both training time and classification accuracy. © 2019, Allerton Press, Inc.},
  langid = {english},
  keywords = {/unread,Brain computer interface,Classification,Classification (of information),Classification algorithm,EEG,Electroencephalogram signals,Electroencephalography,ELM\_Kernel,Extreme learning machine (ELM),Feature extraction,Feature Extraction,Machine learning,Medical computing,Patient rehabilitation,Rehabilitation technology,State-of-the-art algorithms,Support vector machines,Wavelet analysis,Wavelet decomposition,Wavelet Packet Decomposition,Wavelet packet decompositions,WPD},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.000Z}
}

@article{luKernelExtremeLearning2020,
  title = {Kernel Extreme Learning Machine with Iterative Picking Scheme for Failure Diagnosis of a Turbofan Engine},
  author = {Lu, J. and Huang, J. and Lu, F.},
  date = {2020},
  journaltitle = {Aerospace Science and Technology},
  shortjournal = {Aerosp Sci Technol},
  volume = {96},
  publisher = {{Elsevier Masson SAS}},
  issn = {12709638 (ISSN)},
  doi = {10.1016/j.ast.2019.105539},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075864273&doi=10.1016%2fj.ast.2019.105539&partnerID=40&md5=1fe98ee531e3c0a2542de740681b3f6a},
  abstract = {The kernel extreme learning machine (KELM) has attracted attention for failure diagnosis of turbofan engines, but its application for time-sensitive scenarios is inherently limited by its lack of sparseness. The original KELM constructs the hidden layer using all the training samples; thus, the real-time performance may be seriously degraded for large datasets. To address this limitation, a novel iterative picking scheme for improving the sparseness of KELM is proposed in this study. It has two noteworthy features, a compact structure and a sparse solution, which gives better real-time performance. The proposed scheme improves the sparseness of the KELM algorithm by two alternating components, an insertion strategy to expand the network and an elimination strategy to reduce the scale of the network. Validation on regression and classification benchmark datasets demonstrates that the proposed algorithm can produce a sparse network structure with fewer hidden nodes without sacrificing the model's accuracy. Finally, simulations of failure diagnosis of a turbofan engine show that the proposed algorithm can perform at an accuracy comparable to that of KELM with a much faster testing speed, which is crucial in real-time and onboard applications. © 2019 Elsevier Masson SAS},
  langid = {english},
  keywords = {Benchmark datasets,Classification (of information),Compact structures,Engines,Extreme learning machine (ELM),Failure (mechanical),Failure analysis,Failure diagnosis,Failure Diagnosis,Iterative methods,ITS applications,Kernel method,Kernel methods,Knowledge acquisition,Large dataset,Machine learning,Real time performance,Sparseness,Turbofan engine,Turbofan engines},
  annotation = {15 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T16:08:04.078Z},
  file = {/home/leix/Documents/Zotero/storage/EU3TXDEE/lu2019.pdf.pdf}
}

@article{luoJointSparseRegularization2015,
  title = {Joint Sparse Regularization Based {{Sparse Semi-Supervised Extreme Learning Machine}} ({{S3ELM}}) for Classification},
  author = {Luo, X. and Liu, F. and Yang, S. and Wang, X. and Zhou, Z.},
  date = {2015},
  journaltitle = {Knowledge-Based Systems},
  shortjournal = {Knowl Based Syst},
  volume = {73},
  pages = {149--160},
  publisher = {{Elsevier}},
  issn = {09507051 (ISSN)},
  doi = {10.1016/j.knosys.2014.09.014},
  abstract = {Extreme Learning Machine (ELM) has received increasing attention for its simple principle, low computational cost and excellent performance. However, a large number of labeled instances are often required, and the number of hidden nodes should be manually tuned, for better learning and generalization of ELM. In this paper, we propose a Sparse Semi-Supervised Extreme Learning Machine (S3ELM) via joint sparse regularization for classification, which can automatically prune the model structure via joint sparse regularization technology, to achieve more accurate, efficient and robust classification, when only a small number of labeled training samples are available. Different with most of greedy-algorithms based model selection approaches, by using ℓ2,1-norm, S3ELM casts a joint sparse constraints on the training model of ELM and formulate a convex programming. Moreover, with a Laplacian, S3ELM can make full use of the information from both the labeled and unlabeled samples. Some experiments are taken on several benchmark datasets, and the results show that S3ELM is computationally attractive and outperforms its counterparts. © 2014 Elsevier B.V.All rights reserved.},
  langid = {english},
  keywords = {Benchmark datasets,Computational costs,Convex programming,Extreme learning machine (ELM),Joint sparse regularization,Knowledge acquisition,ℓ 2 1-Norm,Laplace transforms,Laplacian,Laplacians,Robust classification,Semi- supervised learning,Sparse regularizations,Sparse semi-supervised learning,Supervised learning,Unlabeled samples},
  annotation = {25 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T12:11:38.644Z},
  file = {/home/leix/Documents/Zotero/storage/R7HZEYGI/luo2015.pdf.pdf}
}

@article{luoRobustMultilayerExtreme2020,
  title = {A Robust Multilayer Extreme Learning Machine Using Kernel Risk-Sensitive Loss Criterion},
  author = {Luo, X. and Li, Y. and Wang, W. and Ban, X. and Wang, J.-H. and Zhao, W.},
  date = {2020},
  journaltitle = {International Journal of Machine Learning and Cybernetics},
  shortjournal = {Intl. J. Mach. Learn. Cybern.},
  volume = {11},
  number = {1},
  pages = {197--216},
  publisher = {{Springer}},
  issn = {18688071 (ISSN)},
  doi = {10.1007/s13042-019-00967-w},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067232216&doi=10.1007%2fs13042-019-00967-w&partnerID=40&md5=e45c907e5177ee199005c39c7aaea4aa},
  abstract = {More recently, extreme learning machine (ELM) has emerged as a novel computing paradigm that enables the neural network (NN) based learning to be achieved with fast training speed and good generalization performance. However, the single hidden layer NN using ELM may be not effective in addressing some large-scale problems with more computational efforts. To avoid such limitation, we utilize the multilayer ELM architecture in this article to reduce the computational complexity, without the physical memory limitation. Meanwhile, it is known to us all that there are a lot of noises in the practical applications, and the traditional ELM may not perform well in this instance. Considering the existence of noises or outliers in training dataset, we develop a more practical approach by incorporating the kernel risk-sensitive loss (KRSL) criterion into ELM, on the basis of the efficient performance surface of KRSL with high accuracy while still maintaining the robustness to outliers. A robust multilayer ELM, i.e., the stacked ELM using the minimum KRSL criterion (SELM-MKRSL), is accordingly proposed in this article to enhance the outlier robustness on large-scale and complicated dataset. The simulation results on some synthetic datasets indicate that the proposed approach SELM-MKRSL can achieve higher classification accuracy and is more robust to the noises compared with other state-of-the-art algorithms related to multilayer ELM. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.},
  langid = {english},
  keywords = {/unread,Classification (of information),Classification accuracy,Computational effort,Deep learning,Extreme learning machine (ELM),Generalization performance,Kernel risk-sensitive loss (KRSL),Knowledge acquisition,Large dataset,Large-scale problem,Multilayer neural networks,Multilayer perceptron,Multilayers,Neural network (nn),State-of-the-art algorithms,Statistics},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.007Z}
}

@article{luProbabilisticRegularizedExtreme2018,
  title = {Probabilistic Regularized Extreme Learning Machine for Robust Modeling of Noise Data},
  author = {Lu, X. and Ming, L. and Liu, W. and Li, H.-X.},
  date = {2018},
  journaltitle = {IEEE Transactions on Cybernetics},
  shortjournal = {IEEE Trans. Cybern.},
  volume = {48},
  number = {8},
  pages = {2368--2377},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {21682267 (ISSN)},
  doi = {10.1109/TCYB.2017.2738060},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028460503&doi=10.1109%2fTCYB.2017.2738060&partnerID=40&md5=da75ada09d12548091c3b6caab1ad5a6},
  abstract = {The extreme learning machine (ELM) has been extensively studied in the machine learning field and has been widely implemented due to its simplified algorithm and reduced computational costs. However, it is less effective for modeling data with non-Gaussian noise or data containing outliers. Here, a probabilistic regularized ELM is proposed to improve modeling performance with data containing non-Gaussian noise and/or outliers. While traditional ELM minimizes modeling error by using a worst-case scenario principle, the proposed method constructs a new objective function to minimize both mean and variance of this modeling error. Thus, the proposed method considers the modeling error distribution. A solution method is then developed for this new objective function and the proposed method is further proved to be more robust when compared with traditional ELM, even when subject to noise or outliers. Several experimental cases demonstrate that the proposed method has better modeling performance for problems with non-Gaussian noise or outliers. © 2013 IEEE.},
  langid = {english},
  keywords = {article,Computational model,Errors,Extreme learning machine (ELM),Gaussian distribution,Gaussian noise (electronic),Integrated circuit modeling,Knowledge acquisition,Learning systems,Linear programming,machine learning,modeling,Models,noise,outlier,Probabilistic logics,Reactive power,robustness,Robustness (control systems),Statistics},
  annotation = {17 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T14:19:52.075Z},
  file = {/home/leix/Documents/Zotero/storage/867UCBYU/1b5f87dc8badc44facd39101aa41fdc5.pdf.pdf}
}

@inproceedings{lvHyperspectralImageClassification2016,
  title = {Hyperspectral Image Classification via Kernel Extreme Learning Machine Using Local Receptive Fields},
  booktitle = {Proc. {{Int}}. {{Conf}}. {{Image Process}}. {{ICIP}}},
  author = {Lv, Q. and Niu, X. and Dou, Y. and Wang, Y. and Xu, J. and Zhou, J.},
  date = {2016},
  volume = {2016-August},
  pages = {256--260},
  publisher = {{IEEE Computer Society}},
  doi = {10.1109/ICIP.2016.7532358},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006802872&doi=10.1109%2fICIP.2016.7532358&partnerID=40&md5=2eaa9ed17034fe7924be13c7f359aab5},
  abstract = {This paper proposes a classification approach for hyperspectral image (HSI) using the local receptive fields based kernel extreme learning machine. Extreme learning machine (ELM) has drawn increasing attention in the pattern recognition filed due to its simpleness, speediness and good generalization ability. A kernel method is often used to promote ELM's performance, which is known as kernel ELM. The local receptive field concept originates from research in neuroscience. Considering the local correlations of spectral features, it is promising to improve the performance of HSI classification by combining local receptive fields with kernel ELM. Experimental results on the Pavia University dataset confirm the effectiveness of the proposed HSI classification method. © 2016 IEEE.},
  isbn = {15224880 (ISSN); 9781467399616 (ISBN)},
  langid = {english},
  keywords = {/unread,Classification (of information),Classification approach,Classification methods,Extreme learning machine,Extreme learning machine (ELM),Generalization ability,Hyper-spectral images,Hyperspectral image,Hyperspectral image classification,Image classification,Image processing,Kernel method,Kernel methods,Knowledge acquisition,Learning systems,Local receptive field (LRF),Neural networks,Pattern recognition,Receptive fields,Spectroscopy},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.970Z}
}

@article{lvReduceTrainingError2021,
  title = {Reduce {{Training Error}} of {{Extreme Learning Machine}} by {{Selecting Appropriate Hidden Layer Output Matrix}}},
  author = {Lv, Y. and Li, B. and Yu, J. and Ding, Y.},
  date = {2021},
  journaltitle = {Journal of Systems Science and Systems Engineering},
  shortjournal = {J. Syst. Sci. Syst. Eng.},
  volume = {30},
  number = {5},
  pages = {552--571},
  publisher = {{Systems Engineering Society of China}},
  issn = {10043756 (ISSN)},
  doi = {10.1007/s11518-021-5502-8},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107630811&doi=10.1007%2fs11518-021-5502-8&partnerID=40&md5=6194852af3e736cae63da363cc50bec5},
  abstract = {Extreme learning machine(ELM) is a feedforward neural network with a single layer of hidden nodes, where the weight and the bias connecting input to hidden nodes are randomly assigned. The output weight between hidden nodes and outputs are learned by a linear model. It is interesting to ask whether the training error of ELM is significantly affected by the hidden layer output matrix H, because a positive answer will enable us obtain smaller training error from better H. For single hidden layer feedforward neural network(SLFN) with one input neuron, there is significant difference between the training errors of different Hs. We find there is a reliable strong negative rank correlation between the training errors and some singular values of the Moore-Penrose generalized inverse of H. Based on the rank correlation, a selection algorithm is proposed to choose robust appropriate H to achieve smaller training error among numerous Hs. Extensive experiments are carried out to validate the selection algorithm, including tests on real data set. The results show that it achieves better performance in validity, speed and robustness. © 2021, Systems Engineering Society of China and Springer-Verlag GmbH Germany.},
  langid = {english},
  keywords = {ELM,Errors,Extreme learning machine (ELM),Feedforward neural networks,Knowledge acquisition,Linear modeling,Machine learning,Matrix algebra,Moore-Penrose generalized inverse,Multilayer neural networks,Network layers,Rank correlation,selection algorithm,Selection algorithm,Single-hidden layer feedforward neural networks,Singular values,SLFNs,Statistical tests,Systems engineering,training error,Training errors},
  annotation = {0 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T18:06:01.992Z},
  file = {/home/leix/Documents/Zotero/storage/X253MP44/lv2021.pdf.pdf}
}

@article{maChebyshevFunctionalLink2018,
  title = {Chebyshev {{Functional Link Artificial Neural Network Based}} on {{Correntropy Induced Metric}}},
  author = {Ma, W. and Duan, J. and Zhao, H. and Chen, B.},
  date = {2018},
  journaltitle = {Neural Processing Letters},
  shortjournal = {Neural Process Letters},
  volume = {47},
  number = {1},
  pages = {233--252},
  publisher = {{Springer New York LLC}},
  issn = {13704621 (ISSN)},
  doi = {10.1007/s11063-017-9646-y},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020696205&doi=10.1007%2fs11063-017-9646-y&partnerID=40&md5=84f30cf4a9fff4d972a77d337ca95568},
  abstract = {In this paper, the Correntropy Induced Metric (CIM) as an alternative to the well-known mean square error (MSE) is employed in Chebyshev functional link artificial neural network (CFLANN) to deal with the noisy training data set and enhance the generalization performance. The MSE performs well under Gaussian noise but it is sensitive to large outliers. The CIM as a local similarity measure, however, can improve significantly the anti-noise ability of CFLANN. The convergence of the proposed algorithm, namely the CFLANN based on CIM (CFLANNCIM), has been analyzed. Simulation results on nonlinear channel identification show that CFLANNCIM can perform much better than the traditional CFLANN and multiple-layer perceptron (MLP) neural networks trained under MSE criterion. © 2017, Springer Science+Business Media, LLC.},
  langid = {english},
  keywords = {Chebyshev basis,Chebyshev basis function,Correntropy Induced Metric (CIM),Functional link artificial neural network,Functional link artificial neural networks,Gaussian noise (electronic),Generalization performance,Induced metric,Local similarity measure,Mean square error,Multiple layer perceptron,Neural networks,Nonlinear channel identification,Training data sets},
  annotation = {7 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T14:24:56.788Z},
  file = {/home/leix/Documents/Zotero/storage/MJZU3BXK/ma2017.pdf.pdf}
}

@article{mackayBayesianMethodsAdaptive1999,
  title = {Bayesian {{Methods}} for {{Adaptive Models}}},
  author = {Mackay, David and Bridle, John and Cheeseman, Peter and Fels, Sidney and Gull, Steve and Herz, Andreas and Hopfield, John and Kerns, Doug and Knutsen, Allen and Koerner, David and Lewicki, Mike and Loredo, Tom and Luttrell, Steve and Rose, Ken and Sibisi, Sibusiso and Skilling, John and Sompolinsky, Haim and Weir, Nick},
  date = {1999-05-28},
  abstract = {The Bayesian framework for model comparison and regularisation is demonstrated by studying interpolation and classification problems modelled with both linear and non--linear models. This framework quantitatively embodies `Occam's razor'. Over--complex and under-- regularised models are automatically inferred to be less probable, even though their flexibility allows them to fit the data better. When applied to `neural networks', the Bayesian framework makes possible (1) objective comparison of solutions using alternative network architectures; (2) objective stopping rules for network pruning or growing procedures; (3) objective choice of type of weight decay terms (or regularisers); (4) on--line techniques for optimising weight decay (or regularisation constant) magnitude; (5) a measure of the effective number of well--determined parameters in a model; (6) quantified estimates of the error bars on network parameters and on network output. In the case of classification models, it is sh...},
  file = {/home/leix/Documents/Zotero_Papers/TFM/Introduction/Mackay et al_1999_Bayesian Methods for Adaptive Models.pdf}
}

@article{mackayBayesianNeuralNetworks1995,
  title = {Bayesian Neural Networks and Density Networks},
  author = {MacKay, David J. C},
  date = {1995-01-15},
  journaltitle = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  shortjournal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  series = {Proceedings of the {{Third Workshop}} on {{Neutron Scattering Data Analysis}}},
  volume = {354},
  number = {1},
  pages = {73--80},
  issn = {0168-9002},
  doi = {10.1016/0168-9002(94)00931-7},
  url = {https://www.sciencedirect.com/science/article/pii/0168900294009317},
  urldate = {2023-08-19},
  abstract = {This paper reviews the Bayesian approach to learning in neural networks, then introduces a new adaptive model, the density network. This is a neural network for which target outputs are provided, but the inputs are unspecified. When a probability distribution is placed on the unknown inputs, a latent variable model is defined that is capable of discovering the underlying dimensionality of a data set. A Bayesian learning algorithm for these networks is derived and demonstrated.},
  annotation = {66 citations (Crossref) [2023-08-19]},
  file = {/home/leix/Documents/Zotero_Papers/TFM/Introduction/MacKay_1995_Bayesian neural networks and density networks.pdf;/home/leix/Documents/Zotero/storage/NPKRBYVZ/0168900294009317.html}
}

@article{maiorovApproximationNeuralNetworks2006,
  title = {Approximation by Neural Networks and Learning Theory},
  author = {Maiorov, V.},
  date = {2006-02-01},
  journaltitle = {Journal of Complexity},
  shortjournal = {Journal of Complexity},
  series = {Special {{Issue}}},
  volume = {22},
  number = {1},
  pages = {102--117},
  issn = {0885-064X},
  doi = {10.1016/j.jco.2005.09.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0885064X05000889},
  urldate = {2023-03-10},
  abstract = {We consider the problem of Learning Neural Networks from samples. The sample size which is sufficient for obtaining the almost-optimal stochastic approximation of function classes is obtained. In the terms of the accuracy confidence function, we show that the least-squares estimator is almost-optimal for the problem. These results can be used to solve Smale's network problem.},
  langid = {english},
  keywords = {Entropy,Learning theory,Neural networks,Stochastic approximation},
  annotation = {33 citations (Crossref) [2023-03-10]},
  file = {/home/leix/Documents/Zotero_Papers/Maiorov_2006_Approximation by neural networks and learning theory.pdf;/home/leix/Documents/Zotero/storage/XV7HCTCG/Maiorov - 2006 - Approximation by neural networks and learning theo.pdf;/home/leix/Documents/Zotero/storage/TZ5ETD6S/S0885064X05000889.html}
}

@article{majumderSolarPowerForecasting2020,
  title = {Solar Power Forecasting Using Robust Kernel Extreme Learning Machine and Decomposition Methods},
  author = {Majumder, I. and Bisoi, R. and Nayak, N. and Hannoon, N.},
  date = {2020},
  journaltitle = {International Journal of Power and Energy Conversion},
  shortjournal = {Int. J. Power Energy Convers.},
  volume = {11},
  number = {3},
  pages = {260--290},
  publisher = {{Inderscience Publishers}},
  issn = {17571154 (ISSN)},
  doi = {10.1504/IJPEC.2020.107958},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089100692&doi=10.1504%2fIJPEC.2020.107958&partnerID=40&md5=074f7dee923294f1de1f29b7c8bf8540},
  abstract = {This paper proposes empirical mode decomposition (EMD)-based robust kernel extreme learning machine (RKELM) to achieve a precise predicted value of solar power generation in a smart grid environment. The non-stationary historical solar power data is initially decomposed into various intrinsic mode functions (IMFs) using EMD, which are subsequently passed through the proposed robust Morlet wavelet kernel extreme learning machine (RWKELM) for solar power prediction at different time horizons. Further a reduced kernel matrix version of RWKELM is used to decrease the training time significantly without appreciable loss of forecasting accuracy. By implementing the real time data for validation of the proposed method for short term solar power prediction it can be observed that the proposed EMD-based RWKELM outperforms various other methods, in terms of different performance matrices and execution time. The solar power prediction results on experimental data show the lowest error which proves the highest prediction accuracy. Copyright © 2020 Inderscience Enterprises Ltd.},
  langid = {english},
  keywords = {/unread,Decomposition methods,Electric power transmission networks,ELM,EMD,Empirical mode decomposition,Empirical Mode Decomposition,Extreme learning machine (ELM),Forecasting,Forecasting accuracy,Intrinsic Mode functions,Knowledge acquisition,Machine learning,Matrix algebra,Performance matrices,Power predictions,Prediction accuracy,Reduced kernel matrix,RKELM,Robust kernel extreme learning machine,Signal processing,Smart power grids,Solar power forecasting,Solar power generation,Solar power plants,Wavelet transform decomposition,WD},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.008Z}
}

@article{malihaExtremeLearningMachine2018,
  title = {Extreme Learning Machine for Structured Output Spaces},
  author = {Maliha, A. and Yusof, R. and Shapiai, M.I.},
  date = {2018},
  journaltitle = {Neural Computing and Applications},
  shortjournal = {Neural Comput. Appl.},
  volume = {30},
  number = {4},
  pages = {1251--1264},
  publisher = {{Springer London}},
  issn = {09410643 (ISSN)},
  doi = {10.1007/s00521-016-2754-1},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006409377&doi=10.1007%2fs00521-016-2754-1&partnerID=40&md5=c1d3cfdda099203a139dc3bb9324612d},
  abstract = {Recently, extreme learning machine (ELM) has attracted increasing attention due to its successful applications in classification, regression, and ranking. Normally, the desired output of the learning system using these machine learning techniques is a simple scalar output. However, there are many applications in machine learning which require more complex output rather than a simple scalar one. Therefore, structured output is used for such applications where the system is trained to predict structured output instead of simple one. Previously, support vector machine (SVM) has been introduced for structured output learning in various applications. However, from machine learning point of view, ELM is known to offer better generalization performance compared to other learning techniques. In this study, we extend ELM to more generalized framework to handle complex outputs where simple outputs are considered as special cases of it. Besides the good generalization property of ELM, the resulting model will possesses rich internal structure that reflects task-specific relations and constraints. The experimental results show that structured ELM achieves similar (for binary problems) or better (for multi-class problems) generalization performance when compared to ELM. Moreover, as verified by the simulation results, structured ELM has comparable or better precision performance with structured SVM when tested for more complex output such as object localization problem on PASCAL VOC2006. Also, the investigation on parameter selections is presented and discussed for all problems. © 2016, The Natural Computing Applications Forum.},
  langid = {english},
  keywords = {Extreme learning machine (ELM),Generalization performance,Generalization properties,Knowledge acquisition,Learning algorithms,Machine learning techniques,Multi-class problems,Object detection,Object recognition,Parameter selection,Quadratic programming,Structured learning,Structured output spaces,Support vector machines},
  annotation = {8 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T14:29:20.272Z},
  file = {/home/leix/Documents/Zotero/storage/XH5X88HT/maliha2016.pdf.pdf}
}

@article{maRecognitionMethodsThreshing2022,
  title = {Recognition Methods of Threshing Load Conditions Based on Machine Learning Algorithms},
  author = {Ma, Z. and Jiang, S. and Li, Y. and Xu, L. and Zhu, Y. and Shi, M. and Nfamoussa Traore, S.},
  date = {2022},
  journaltitle = {Computers and Electronics in Agriculture},
  shortjournal = {Comput. Electron. Agric.},
  volume = {200},
  publisher = {{Elsevier B.V.}},
  issn = {01681699 (ISSN)},
  doi = {10.1016/j.compag.2022.107250},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134883188&doi=10.1016%2fj.compag.2022.107250&partnerID=40&md5=4fc0164ffa4997e15fdf64700cac8278},
  abstract = {In order to identify the load conditions of threshing and separating device of combine harvester quickly and accurately, this experiment was conducted to collect vibration acceleration signals of the outer surface of threshing and separating device under different load conditions by field test in 2020 with the new rice variety Nanjing Jinggu as the research object. Firstly, based on the statistical analysis and signal analysis method, time domain, frequency domain, and time–frequency domain characteristics were extracted and fused into total domain characteristics to characterize the overall signal attributes of load conditions of the threshing and separation device so as to reduce the difficulty of data decision. Secondly, Principal Component Analysis (PCA) and Kernel Principal Component Analysis (KPCA) were used to remove the correlation and nonlinearity among the extracted characteristics, reduce the dimensionality of the characteristic vector and improve the accuracy of the diagnostic model. Finally, Support Vector Machine (SVM), Extreme Learning Machine (ELM) and Random Forest (RF) were used to diagnose the load conditions of the dimension reduction collection of total domains, and the accuracy rate and recognition time were used as evaluation indexes for the comparative analysis of model recognition. The results showed that the KPCA clustering separation effect is significant. RF has the highest recognition accuracy, which the accuracy of training set and prediction set are 100\% and 98\% respectively. The accuracy of ELM-KPCA model training set and prediction set is 100\% and 90\% respectively, and the analysis time is 6.206 s. This model accuracy is high, and the analysis time is the shortest, then ELM-KPCA model can be the best model for load conditions recognition of combine harvester. © 2022},
  langid = {english},
  keywords = {/unread,Agriculture,China,Combine harvester,Combine harvesters,Decision trees,ELM,Extreme learning machine (ELM),Frequency domain analysis,Harvesters,instrumentation,Jinggu,Kernel principal component analyses (KPCA),Kernel principal component analysis,Learning algorithms,Learning machines,Load condition,Load recognition,machine learning,Nanjing [Jiangsu],principal component analysis,Principal component analysis,Random forests,RF,statistical analysis,Support vector machines,Support vectors machine,SVM,Threshing and separating,Time domain analysis,Training sets,Yunnan},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.024Z}
}

@book{masriSoilPropertyPrediction2015,
  title = {Soil Property Prediction: {{An}} Extreme Learning Machine Approach},
  author = {Masri, D. and Woon, W.L. and Aung, Z.},
  editorb = {{Lai W.K.} and {Liu Q.} and {Huang T.} and {Arik S.}},
  editorbtype = {redactor},
  date = {2015},
  journaltitle = {Lect. Notes Comput. Sci.},
  volume = {9490},
  pages = {27},
  publisher = {{Springer Verlag}},
  doi = {10.1007/978-3-319-26535-3_3},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951800845&doi=10.1007%2f978-3-319-26535-3_3&partnerID=40&md5=aed69fa66fea47ab5bc5a975ea373e28},
  abstract = {In this paper, we propose a method for predicting functional properties of soil samples from a number of measurable spatial and spectral features of those samples. Our method is based on Savitzky-Golay filter for preprocessing and a relatively recent evolution of single hidden layer feed-forward network (SLFN) learning technique called extreme learning machine (ELM) for prediction. We tested our method with Africa Soil Property Prediction dataset, and observed that the results were promising. © Springer International Publishing Switzerland 2015.},
  isbn = {03029743 (ISSN); 9783319265346 (ISBN)},
  langid = {english},
  pagetotal = {18},
  keywords = {/unread,Extreme learning machine (ELM),Feed-forward network,Forecasting,Functional properties,Information science,Kernel-based ELM,Knowledge acquisition,Learning systems,Learning techniques,Network layers,Neural network,Neural networks,Prediction,Savitzky-Golay filter,Soil property,Soils,Spectral feature},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.960Z}
}

@inproceedings{mengqiHybridKernelPCA2015,
  title = {A Hybrid Kernel {{PCA}}, Hypersphere {{SVM}} and Extreme Learning Machine Approach for Nonlinear Process Online Fault Detection},
  booktitle = {Annu. {{Conf}}. {{IEEE Industrial Electron}}. {{Soc}}., {{IECON}}},
  author = {Mengqi, N. and Jingjing, D. and Tianzhen, W. and Diju, G. and Jingang, H. and Benbouzid, M.E.H.},
  date = {2015},
  pages = {2106--2111},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/IECON.2015.7392412},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973130396&doi=10.1109%2fIECON.2015.7392412&partnerID=40&md5=69fe51b887681c041846768fcdf30855},
  abstract = {This paper presents a hybrid approach for online fault detection in nonlinear processes. To solve the possible monitoring difficulties caused by nonlinear characteristics of industrial process data, two applications of the Kernel Method: Hypersphere Support Vector Machine (HSSVM) and Kernel Principal Component Analysis (KPCA) are used as fault detection methods. On top of that, to obtain the adaptive models for online monitoring and fault detection in unsteady-stage conditions, instead of the static ones established by traditional HSSVM and KPCA, multiple methods are adopted, including Recursive KPCA, Adaptive Control Limit (ACL) and Online Sequential Extreme Learning Machine (OS-ELM), all of which update the detection model in real time with dynamically adjusting. The T2 control limit of Recursive KPCA, the classification hyperspheres of HSSVM and the single hidden layer feedforward network (SLFN) trained with OS-ELM work collaboratively in monitoring the real time process data to detect the possible faults. The proposed approach was tested and validated via a set of experimental data collected from a bearing test rig. Experimental results show that this approach is adequate for fault detection while meets the needs of real time performance. © 2015 IEEE.},
  isbn = {9781479917624 (ISBN)},
  langid = {english},
  keywords = {ELM,Extreme learning machine (ELM),Fault detection,HSSVM,Industrial electronics,Kernel principal component analyses (KPCA),Knowledge acquisition,KPCA,Learning systems,Network layers,Nonlinear characteristics,Nonlinear process,On-line fault detection,Online fault detection,Online sequential extreme learning machine,Principal component analysis,Support vector machines},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-03-09T21:35:22.572Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/elm kernel scopus/Mengqi et al_2015_A hybrid kernel PCA, hypersphere SVM and extreme learning machine approach for.pdf}
}

@software{meyer[autE1071MiscFunctions2023,
  title = {E1071: {{Misc Functions}} of the {{Department}} of {{Statistics}}, {{Probability Theory Group}} ({{Formerly}}: {{E1071}}), {{TU Wien}}},
  shorttitle = {E1071},
  author = {Meyer [aut, David and {cre} and Dimitriadou, Evgenia and Hornik, Kurt and Weingessel, Andreas and Leisch, Friedrich and C++-code), Chih-Chung Chang (libsvm and C++-code), Chih-Chen Lin (libsvm},
  date = {2023-02-01},
  url = {https://CRAN.R-project.org/package=e1071},
  urldate = {2023-04-16},
  abstract = {Functions for latent class analysis, short time Fourier transform, fuzzy clustering, support vector machines, shortest path computation, bagged clustering, naive Bayes classifier, generalized k-nearest neighbour ...},
  version = {1.7-13},
  keywords = {/unread,Cluster,Distributions,Environmetrics,MachineLearning,Psychometrics}
}

@article{mhaskarNeuralNetworksApproximation1996,
  title = {Neural Networks and Approximation Theory},
  author = {Mhaskar, H.N.},
  date = {1996-06},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {9},
  number = {4},
  pages = {721--722},
  issn = {08936080},
  doi = {10.1016/0893-6080(95)00125-5},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0893608095001255},
  urldate = {2023-03-10},
  langid = {english},
  keywords = {/unread},
  annotation = {7 citations (Crossref) [2023-03-10]},
  file = {/home/leix/Documents/Zotero_Papers/TFM/citen scopus/Mhaskar_1996_Neural networks and approximation theory.pdf}
}

@article{micheOPELMOptimallyPruned2010,
  title = {{{OP-ELM}}: {{Optimally Pruned Extreme Learning Machine}}},
  shorttitle = {{{OP-ELM}}},
  author = {Miche, Yoan and Sorjamaa, Antti and Bas, Patrick and Simula, Olli and Jutten, Christian and Lendasse, Amaury},
  date = {2010-01},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {21},
  number = {1},
  pages = {158--162},
  issn = {1941-0093},
  doi = {10.1109/TNN.2009.2036259},
  abstract = {In this brief, the optimally pruned extreme learning machine (OP-ELM) methodology is presented. It is based on the original extreme learning machine (ELM) algorithm with additional steps to make it more robust and generic. The whole methodology is presented in detail and then applied to several regression and classification problems. Results for both computational time and accuracy (mean square error) are compared to the original ELM and to three other widely used methodologies: multilayer perceptron (MLP), support vector machine (SVM), and Gaussian process (GP). As the experiments for both regression and classification illustrate, the proposed OP-ELM methodology performs several orders of magnitude faster than the other algorithms used in this brief, except the original ELM. Despite the simplicity and fast performance, the OP-ELM is still able to maintain an accuracy that is comparable to the performance of the SVM. A toolbox for the OP-ELM is publicly available online.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {Classification,Computational complexity,Computer science,extreme learning machine (ELM),Feedforward neural networks,least angle regression (LARS),Machine learning,Machine learning algorithms,Multi-layer neural network,Neural networks,optimally pruned extreme learning machine (OP-ELM),regression,Robustness,Support vector machine classification,Support vector machines,variable selection},
  annotation = {612 citations (Crossref) [2023-05-08]},
  file = {/home/leix/Documents/Zotero_Papers/TFM/kernels/Miche et al_2010_OP-ELM.pdf;/home/leix/Documents/Zotero/storage/LYJFJ92N/5350449.html}
}

@article{micheSOMELMSelfOrganizedClusteringUsing2015,
  title = {{{SOM-ELM-Self-Organized Clustering}} Using {{ELM}}},
  author = {Miche, Y. and Akusok, A. and Veganzones, D. and Björk, K.-M. and Séverin, E. and family=Jardin, given=P., prefix=du, useprefix=true and Termenon, M. and Lendasse, A.},
  date = {2015},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {165},
  pages = {238--254},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2015.03.014},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929965891&doi=10.1016%2fj.neucom.2015.03.014&partnerID=40&md5=b918f66b65e5a21895665a0dac63fe6e},
  abstract = {This paper presents two new clustering techniques based on Extreme Learning Machine (ELM). These clustering techniques can incorporate a priori knowledge (of an expert) to define the optimal structure for the clusters, i.e. the number of points in each cluster. Using ELM, the first proposed clustering problem formulation can be rewritten as a Traveling Salesman Problem and solved by a heuristic optimization method. The second proposed clustering problem formulation includes both a priori knowledge and a self-organization based on a predefined map (or string). The clustering methods are successfully tested on 5 toy examples and 2 real datasets. © 2015 Elsevier B.V.},
  langid = {english},
  keywords = {algorithm,Article,cluster analysis,Cluster analysis,Clustering,Clustering problems,Clustering techniques,computer interface,ELM,Extreme learning machine (ELM),Heuristic methods,Heuristic optimization method,information processing,kernel method,Learning systems,machine learning,mathematical computing,nerve cell,priority journal,Self organizations,self organizing maps extreme learning machine,Self-Organized,Self-organized clustering,SOM,Structural optimization,Traveling salesman problem},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-03-09T21:38:09.074Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/elm kernel scopus/Miche et al_2015_SOM-ELM-Self-Organized Clustering using ELM.pdf}
}

@inproceedings{minNovelKernelbasedExtreme2020,
  title = {A Novel Kernel-Based Extreme Learning Machine with Incremental Hidden Layer Nodes},
  booktitle = {{{IFAC-PapersOnLine}}},
  author = {Min, M. and Chen, X. and Lei, Y. and Chen, Z. and Xie, Y.},
  date = {2020},
  volume = {53},
  number = {2},
  pages = {11836--11841},
  publisher = {{Elsevier B.V.}},
  doi = {10.1016/j.ifacol.2020.12.695},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105067325&doi=10.1016%2fj.ifacol.2020.12.695&partnerID=40&md5=c82ae5a8697f35604c894543074aae08},
  abstract = {Extreme learning machine (ELM) is widely used in various fields because of its advantages such as short training time and good generalization performance. The input weights and bias of hidden layer of traditional ELM are generated randomly, and the number of hidden layer nodes is determined by artificial experience. Only by adjusting parameters manually can an appropriate network structure be found. This training method is complex and time-consuming, which increases the workload of workers. To solve this problem, the incremental extreme learning machine (I-ELM) is used to determine the appropriate number of hidden layer nodes and construct a compact network structure in this paper. At the same time, a new hidden layer activation function STR is proposed, which avoids the disadvantages of incomplete output information of hidden layer due to uneven distribution of sample data. The proposed algorithm is evaluated by public data sets and applied to the classification of superheat degree (SD) in aluminum electrolysis industry. The experimental results show that STR activation function has a good learning speed, and the proposed algorithm is superior to the existing SD identification algorithm in terms of accuracy and robustness. Copyright © 2020 The Authors. This is an open access article under the CC BY-NC-ND license},
  isbn = {24058963 (ISSN)},
  langid = {english},
  keywords = {/unread,Activation functions,Chemical activation,Classification (of information),Electrolysis,ELM,Extreme learning machine (ELM),Hidden layer nodes,Hidden layers,I-ELM,Incremental extreme learning machine,Kernel function,Knowledge acquisition,Learning algorithms,Learning machines,Machine learning,Network structures,SD classification,Superheat degree classification,Training time},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.009Z}
}

@article{mishraMultiobjectiveAutoencoderDeep2021,
  title = {Multi-Objective Auto-Encoder Deep Learning-Based Stack Switching Scheme for Improved Battery Life Using Error Prediction of Wind-Battery Storage Microgrid},
  author = {Mishra, S.P. and Krishna Rayi, V. and Dash, P.K. and Bisoi, R.},
  date = {2021},
  journaltitle = {International Journal of Energy Research},
  shortjournal = {Int. J. Energy Res.},
  volume = {45},
  number = {14},
  pages = {20331--20355},
  publisher = {{John Wiley and Sons Ltd}},
  issn = {0363907X (ISSN)},
  doi = {10.1002/er.7117},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112283266&doi=10.1002%2fer.7117&partnerID=40&md5=b095c63fccec21d9d97d154bb1bdffe6},
  abstract = {For any wind power generation system, battery energy storage is a suitable backup power unit for ensuring greater functionality by compensating the prediction error due to the variable nature of generation. For designing local energy management system, the hierarchical operation of distributed generation controllers, BES life, etc. is directly influenced by the prediction error profile. To design an effective local energy management system, it is imperative to obtain wind power generator control reference influencing the overall system stability and reducing the deterioration of life and power loss of the integrated battery energy storage system due to increased temperature. Thus, an efficient local energy management system for the doubly fed induction generator battery energy storage system is proposed here for the robust minimization of the prediction error and to increase controller responses and the life of the battery. A novel deep learning robust multilayer multi-kernel extreme learning machine autoencoder algorithm is proposed here in order to obtain an improvement in the prediction error profile. This prediction algorithm produces optimal model generalization along with the minimization of reconstruction errors and uses simple matrix inversion for prediction in comparison with more complex deep learning neural networks. A new secondary controller is proposed to address the degradation of the life of the battery energy storage system due to increased temperature/power loss profile under prediction error. A model reference-based battery temperature model, associated with multi-objective optimization-based temperature tolerance dynamic stack reconfiguration of battery stacks is incorporated in the proposed secondary controller. This secondary controller-based local energy management system operation is targeted towards satisfying local load demand and grid dispatch, while ensuring an optimized battery stack performance under wind power generation discrepancies. In addition, a new adaptive slope-based primary controller is designed for effective power sharing between the battery energy storage system and wind power generator based on \% state of the charge of the energy storage system. Also, an independent distributed generator controller is studied in an elaborate manner for the DC link stability of the DC-DC converter and voltage source converter for grid synchronization control. © 2021 John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {/unread,Adaptive control systems,Asynchronous generators,battery energy storage,Battery energy storage systems,battery management system,Battery storage,Controllers,DC-DC converters,deep learning,Deep learning,Deep neural networks,Deterioration,Distributed power generation,Doubly fed induction generator (DFIG),Electric batteries,Electric load dispatching,Electric machine control,Electric power system control,Energy efficiency,Energy management,Energy management systems,Errors,Extreme learning machine (ELM),Forecasting,Grid synchronization controls,Learning algorithms,Learning neural networks,Learning systems,Local energy management systems,Microgrids,multi-objective optimization,Multiobjective optimization,robust multilayer multi-kernel ELM autoencoder,Signal encoding,Storage management,System stability,Voltage source converters,Wind power,Wind power generation systems},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.019Z}
}

@software{mouselimisElmNNRcppExtremeLearning2022,
  title = {{{elmNNRcpp}}: {{The Extreme Learning Machine Algorithm}}},
  shorttitle = {{{elmNNRcpp}}},
  author = {Mouselimis, Lampros and Gosso, Alberto and Contributor), Edwin de Jonge (Github},
  date = {2022-01-28},
  url = {https://CRAN.R-project.org/package=elmNNRcpp},
  urldate = {2023-03-10},
  abstract = {Training and predict functions for Single Hidden-layer Feedforward Neural Networks (SLFN) using the Extreme Learning Machine (ELM) algorithm. The ELM algorithm differs from the traditional gradient-based algorithms for very short training times (it doesn't need any iterative tuning, this makes learning time very fast) and there is no need to set any other parameters like learning rate, momentum, epochs, etc. This is a reimplementation of the 'elmNN' package using 'RcppArmadillo' after the 'elmNN' package was archived. For more information, see "Extreme learning machine: Theory and applications" by Guang-Bin Huang, Qin-Yu Zhu, Chee-Kheong Siew (2006), Elsevier B.V, {$<$}doi:10.1016/j.neucom.2005.12.126{$>$}.},
  version = {1.0.4},
  keywords = {/unread}
}

@article{nayakAutomatedDiagnosisPathological2020,
  title = {Automated {{Diagnosis}} of {{Pathological Brain Using Fast Curvelet Entropy Features}}},
  author = {Nayak, D.R. and Dash, R. and Chang, X. and Majhi, B. and Bakshi, S.},
  date = {2020},
  journaltitle = {IEEE Transactions on Sustainable Computing},
  shortjournal = {IEETrans. Sust. Comp.},
  volume = {5},
  number = {3},
  pages = {416--427},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {23773782 (ISSN)},
  doi = {10.1109/TSUSC.2018.2883822},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121937609&doi=10.1109%2fTSUSC.2018.2883822&partnerID=40&md5=2b8976985e89ebd0f74b8bf14e408880},
  abstract = {Automated diagnosis of pathological brain not only reduces the diagnostic error significantly but also improves the patient's quality of life, thereby addressing the sustainability issues. The last few decades have witnessed an intensive research on binary classification of brain magnetic resonance (MR) images. Multiclass classification of pathological brain MR images is a more challenging task and the literature on this problem is still in its infancy. In this paper, we propose a new automated diagnosis system to classify the brain MR images into five different categories. Texture features within MR images play a significant role in accurate and efficient pathological brain detection. This work presents the extraction of such vital texture features by calculating the entropy over the curvelet subbands. Two faster and simpler strategies of fast curvelet transform are separately employed for feature extraction and the derived features are termed as FCEntF-I and FCEntF-II. The features are finally subjected to kernel extreme learning machine (K-ELM) for classification. The effectiveness of the proposed scheme is evaluated on multiclass as well as binary brain MR datasets. Comparisons with state-of-the-art methods indicate the superiority of the proposed scheme. The discriminatory potential of FCEntF-I and FCEntF-II features is found better than its counterparts. © 2016 IEEE.},
  langid = {english},
  keywords = {/unread,Automated diagnosis system,Automation,Diagnosis,Discrete wavelet transforms,Diseases,entropy,Entropy,Extraction,Extreme learning machine (ELM),Fast curvelet transform,Feature extraction,Image classification,kernel ELM,Learning systems,magnetic resonance imaging,Magnetic resonance imaging,Multi-class classification,Pathology,State-of-the-art methods,Sustainability issues,texture features,Texture features,Textures},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.010Z}
}

@book{nealBayesianLearningNeural1996,
  title = {Bayesian {{Learning}} for {{Neural Networks}}},
  author = {Neal, Radford M.},
  editorb = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S.},
  editorbtype = {redactor},
  date = {1996},
  series = {Lecture {{Notes}} in {{Statistics}}},
  volume = {118},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4612-0745-0},
  url = {http://link.springer.com/10.1007/978-1-4612-0745-0},
  urldate = {2023-03-10},
  abstract = {Two features distinguish the Bayesian approach to learning models from data. First, beliefs derived from background knowledge are used to select a prior probability distribution for the model parameters. Second, predictions of future observations are made by integrating the model's predictions with respect to the posterior parameter distribution obtained by updating this prior to take account of the data. For neural network models, both these aspects present di culties | the prior over network parameters has no obvious relation to our prior knowledge, and integration over the posterior is computationally very demanding.},
  isbn = {978-0-387-94724-2 978-1-4612-0745-0},
  langid = {english},
  annotation = {Read\_Status: To Read Read\_Status\_Date: 2023-03-10T18:26:07.845Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/kernels/Neal_1996_Bayesian Learning for Neural Networks.pdf}
}

@inproceedings{nealBayesianTrainingBackpropagation1992,
  title = {Bayesian Training of Backpropagation Networks by the Hybrid {{Monte-Carlo}} Method},
  author = {Neal, Radford M.},
  date = {1992},
  url = {https://www.semanticscholar.org/paper/Bayesian-training-of-backpropagation-networks-by-Neal/fc053fd3feade79df85fd0612d7f817f5ae3cd44},
  urldate = {2023-08-18},
  abstract = {It is shown that Bayesian training of backpropagation neural networks can feasibly be performed by the \textbackslash Hybrid Monte Carlo" method. This approach allows the true predictive distribution for a test case given a set of training cases to be approximated arbitrarily closely, in contrast to previous approaches which approximate the posterior weight distribution by a Gaussian. In this work, the Hybrid Monte Carlo method is implemented in conjunction with simulated annealing, in order to speed relaxation to a good region of parameter space. The method has been applied to a test problem, demonstrating that it can produce good predictions, as well as an indication of the uncertainty of these predictions. Appropriate weight scaling factors are found automatically. By applying known techniques for calculation of \textbackslash free energy" diierences, it should also be possible to compare the merits of diierent network architectures. The work described here should also be applicable to a wide variety of statistical models other than neural networks.},
  file = {/home/leix/Documents/Zotero_Papers/TFM/Introduction/Neal_1992_Bayesian training of backpropagation networks by the hybrid Monte-Carlo method.pdf}
}

@incollection{nealPriorsInfiniteNetworks1996,
  title = {Priors for {{Infinite Networks}}},
  booktitle = {Bayesian {{Learning}} for {{Neural Networks}}},
  author = {Neal, Radford M.},
  editor = {Neal, Radford M.},
  date = {1996},
  series = {Lecture {{Notes}} in {{Statistics}}},
  pages = {29--53},
  publisher = {{Springer}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4612-0745-0_2},
  url = {https://doi.org/10.1007/978-1-4612-0745-0_2},
  abstract = {In this chapter, I show that priors over network parameters can be defined in such a way that the corresponding priors over functions computed by the network reach reasonable limits as the number of hidden units goes to infinity. When using such priors,there is thus no need to limit the size of the network in order to avoid “overfitting”. The infinite network limit also provides insight into the properties of different priors. A Gaussian prior for hidden-to-output weights results in a Gaussian process prior for functions,which may be smooth, Brownian, or fractional Brownian. Quite different effects can be obtained using priors based on non-Gaussian stable distributions. In networks with more than one hidden layer, a combination of Gaussian and non-Gaussian priors appears most interesting.},
  isbn = {978-1-4612-0745-0},
  langid = {english},
  keywords = {/unread,Gaussian Process,Hide Layer,Hide Unit,Output Unit,Prior Distribution},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:06.179Z},
  file = {/home/leix/Documents/Zotero/storage/H2IHDN6W/Neal - 1996 - Priors for Infinite Networks.pdf}
}

@article{nianExtremeLearningMachine2014,
  title = {Extreme Learning Machine towards Dynamic Model Hypothesis in Fish Ethology Research},
  author = {Nian, R. and He, B. and Zheng, B. and family=Heeswijk, given=M., prefix=van, useprefix=true and Yu, Q. and Miche, Y. and Lendasse, A.},
  date = {2014},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {128},
  pages = {273--284},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2013.03.054},
  abstract = {In this paper, we present one dynamic model hypothesis to perform fish trajectory tracking in the fish ethology research and develop the relevant mathematical criterion on the basis of the Extreme Learning Machine (ELM). It is shown that the proposed scheme can conduct the non-linear and non Gaussian tracking process by multiple historical cues and current predictions - the state vector motion, the color distribution and the appearance recognition, all of which can be extracted from the single-hidden layer feedforward neural network (SLFN) at diverse levels with ELM. The strategy of the hierarchical hybrid ELM ensemble then combines the individual SLFN of the tracking cues for the performance improvements. The simulation results have shown the excellent performance in both robustness and accuracy of the developed approach. © 2013 Elsevier B.V.},
  langid = {english},
  keywords = {article,Bayes theorem,Biology,calculation,Color distribution,controlled study,Dynamic models,Dynamic state space,ethology,Extreme learning machine (ELM),fish,Fish,Fish ethology,Knowledge acquisition,learning algorithm,Learning systems,machine learning,mathematical computing,mathematical model,Non-Gaussian,Object recognition,priority journal,Security systems,simulation,Single-hidden layer feedforward neural networks,State vector,Tracking process,Trajectory tracking,Video surveillance system,Video surveillance systems},
  annotation = {43 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-26T19:54:02.288Z},
  file = {/home/leix/Documents/Zotero/storage/KM68VNVV/nian2014.pdf.pdf}
}

@online{NixNixOSReproducible,
  title = {Nix \& {{NixOS}} | {{Reproducible}} Builds and Deployments},
  url = {https://nixos.org/},
  urldate = {2023-08-27},
  abstract = {Nix is a tool that takes a unique approach to package management and system configuration. Learn how to make reproducible, declarative and reliable systems.},
  langid = {english},
  file = {/home/leix/Documents/Zotero/storage/H5AAQ98A/nixos.org.html}
}

@video{nixosMichielLeenaarsSignificance2022,
  entrysubtype = {video},
  title = {Michiel {{Leenaars}} - {{The Significance}} of {{Reproducible Software}} in {{International R}}\&{{D}} ({{SoN2022}})},
  editor = {{NixOS}},
  editortype = {director},
  date = {2022-08-30},
  url = {https://www.youtube.com/watch?v=TM5zpCn4piM},
  urldate = {2023-08-27},
  abstract = {Technology is impacting or even driving ever more aspects of our society, and publicly funded research was behind many of the key breakthroughs in recent decades: the internet, the web, and of course Nix itself. From a policy angle "public money" may officially mean "public code", but that copyright licensing imperative says nothing just yet about deployability or sustainability. Lots of technology developed with public funding ungracefully floats into oblivion, simply because it is buried inside a fragile technical context and was never made easy to deploy. Even if people find out about some interesting technology through luck or perseverance, bitrot often sets in almost immediately - change of a single dependency turns the prospective user into an amateur software archeologist by necessity even if they just want to have a look how it works.    Nix offers a different path. By its own or combined with open data, it allows for reproducible research in a cost-effective and non-discriminatory way. For development, it makes collaboration and usage easy - making technology something you can share, adjust and build on at scale. In this talk, Michiel Leenaars from NLnet will explain how Nix is part of the larger ambitions of the Next Generation Internet (NGI), and how nix helps to bring reusable technological building blocks towards the most exotic corners of the internet.    Special thanks to the NLnet Foundation, the European Commission, the NixOS Foundation, and Tweag for making this event a reality!    The continued discussion for the lecture series is happening over here:   https://matrix.to/\#/\#son2022-lectures:matrix.org    More information about the Summer of Nix can be found on the website:  https://summer.nixos.org}
}

@article{onetoDatadrivenPhotovoltaicPower2018,
  title = {Data-Driven Photovoltaic Power Production Nowcasting and Forecasting for Polygeneration Microgrids},
  author = {Oneto, L. and Laureri, F. and Robba, M. and Delfino, F. and Anguita, D.},
  date = {2018},
  journaltitle = {IEEE Systems Journal},
  shortjournal = {IEEE Syst. J.},
  volume = {12},
  number = {3},
  pages = {2842--2853},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {19328184 (ISSN)},
  doi = {10.1109/JSYST.2017.2688359},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018521775&doi=10.1109%2fJSYST.2017.2688359&partnerID=40&md5=0fcf79fccad257fdf6dd19e298c92bbc},
  abstract = {In this paper, we deal with the problem of nowcasting and forecasting the photovoltaic power production (PvPP) on the basis of real data available for the Savona Campus and coming from the energy management systems (EMSs) of the smart polygeneration microgrid that feeds buildings in the University area. In this paper, we show how PvPP nowcast and forecast problems can be solved with the state-of-the-art data-driven techniques, which use the historical data collected by the EMS. In particular, we compare the performance of the kernelized regularized least squares, the extreme learning machines, and the random forests. In the machine learning field, these algorithms are the best choice in three different families of techniques: kernel methods, neural networks, and ensemble methods. Results show that our proposal can improve of almost one order of magnitude to the actual prediction system used in the EMS of the Savona Campus, which is based on the knowledge of the physical problem. Finally, by using the EMS installed at the Savona Campus, it has been possible to quantify the saving in costs and CO 2 emissions due to the new nowcasting and forecasting models. © 2018 IEEE.},
  langid = {english},
  keywords = {/unread,Data driven technique,Decision trees,Energy management system,Energy management systems,Ensemble methods,Extreme learning machine (ELM),Extreme learning machines (ELM),Forecasting,Forecasting models,Information management,Kernel methods (KMs),Learning systems,Microgrids,Model selection (MS),Nowcasting,Photovoltaic cells,Photovoltaic power,Photovoltaic power production (PvPP),Prediction systems,Random forests (RFs),Regularized least squares,State of the art},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.990Z}
}

@online{pandeyGoDeepWide2014,
  title = {To Go Deep or Wide in Learning?},
  author = {Pandey, Gaurav and Dukkipati, Ambedkar},
  date = {2014-02-23},
  eprint = {1402.5634},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1402.5634},
  url = {http://arxiv.org/abs/1402.5634},
  urldate = {2023-03-10},
  abstract = {To achieve acceptable performance for AI tasks, one can either use sophisticated feature extraction methods as the first layer in a two-layered supervised learning model, or learn the features directly using a deep (multi-layered) model. While the first approach is very problem-specific, the second approach has computational overheads in learning multiple layers and fine-tuning of the model. In this paper, we propose an approach called wide learning based on arc-cosine kernels, that learns a single layer of infinite width. We propose exact and inexact learning strategies for wide learning and show that wide learning with single layer outperforms single layer as well as deep architectures of finite width for some benchmark datasets.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Machine Learning},
  file = {/home/leix/Documents/Zotero_Papers/TFM/Pandey_Dukkipati_2014_To go deep or wide in learning.pdf;/home/leix/Documents/Zotero/storage/CQFQGRJW/1402.html}
}

@online{pandeyGoDeepWide2014a,
  title = {To Go Deep or Wide in Learning?},
  author = {Pandey, Gaurav and Dukkipati, Ambedkar},
  date = {2014-02-23},
  eprint = {1402.5634},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1402.5634},
  url = {http://arxiv.org/abs/1402.5634},
  urldate = {2023-03-10},
  abstract = {To achieve acceptable performance for AI tasks, one can either use sophisticated feature extraction methods as the first layer in a two-layered supervised learning model, or learn the features directly using a deep (multi-layered) model. While the first approach is very problem-specific, the second approach has computational overheads in learning multiple layers and fine-tuning of the model. In this paper, we propose an approach called wide learning based on arc-cosine kernels, that learns a single layer of infinite width. We propose exact and inexact learning strategies for wide learning and show that wide learning with single layer outperforms single layer as well as deep architectures of finite width for some benchmark datasets.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  annotation = {Read\_Status: To Read Read\_Status\_Date: 2023-03-10T18:26:11.138Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/kernels/Pandey_Dukkipati_2014_To go deep or wide in learning.pdf;/home/leix/Documents/Zotero/storage/QI89YVI5/1402.html}
}

@inproceedings{parikhNoreferenceImageQuality2016,
  title = {No-Reference Image Quality Assessment Using Extreme Learning Machines},
  booktitle = {Proc. {{Int}}. {{Conf}}. {{Inven}}. {{Comput}}. {{Technol}}., {{ICICT}}},
  author = {Parikh, N. and Chapaneri, S. and Shah, G.},
  date = {2016},
  volume = {2},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/INVENTIVE.2016.7824891},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011072146&doi=10.1109%2fINVENTIVE.2016.7824891&partnerID=40&md5=3b718994b648023e15e6075b2c028b0b},
  abstract = {In this paper, a No-Reference Image Quality Assessment (NR-IQA) algorithm is implemented with the help of Extreme Learning Machine (ELM) using spatial and spectral features. ELMs are single hidden layer feed-forward neural networks that provides optimum solution in a single iteration, hence ELMs can be used for performing classification and regression at high speeds. Proposed NR-IQA algorithm can quantify the amount of distortion for images caused by JPEG compression, JPEG2000 compression, Additive White Gaussian Noise, Gaussian Blurring effect and Rayleigh's Fast Fading effects. The proposed algorithm is evaluated using LIVE IQA database via Spearman's Ranked Ordered Correlation Coefficient (SROCC) and Root Mean Square Error (RMSE). The proposed algorithm outperforms existing NR-IQA methods. © 2016 IEEE.},
  isbn = {9781509012855 (ISBN)},
  langid = {english},
  keywords = {/unread,Additive White Gaussian noise,Correlation coefficient,ELM,Extreme learning machine,Feedforward neural networks,Gaussian noise (electronic),Image compression,Image quality,IQA,Iterative methods,Kernel,Knowledge acquisition,Machine learning,Mean square error,Multilayer neural networks,No references,No-reference,No-reference image quality assessments,Quality,Root mean square errors,Single-hidden layer feed-forward neural network,White noise},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.971Z}
}

@book{parviainenConnectionExtremeLearning2013,
  title = {A Connection between Extreme Learning Machine and Neural Network Kernel},
  author = {Parviainen, E. and Riihimäki, J.},
  date = {2013},
  journaltitle = {Commun. Comput. Info. Sci.},
  volume = {272 CCIS},
  pages = {135},
  publisher = {{Springer Verlag}},
  doi = {10.1007/978-3-642-29764-9_8},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873922670&doi=10.1007%2f978-3-642-29764-9_8&partnerID=40&md5=29d6106f13386aa9ef97da82a3569bfb},
  abstract = {We study a connection between extreme learning machine (ELM) and neural network kernel (NNK). NNK is derived from a neural network with an infinite number of hidden units. We interpret ELM as an approximation to this infinite network. We show that ELM and NNK can, to certain extent, replace each other. ELM can be used to form a kernel, and NNK can be decomposed into feature vectors to be used in the hidden layer of ELM. The connection reveals possible importance of weight variance as a parameter of ELM. Based on our experiments, we recommend that model selection on ELM should consider not only the number of hidden units, as is the current practice, but also the variance of weights. We also study the interaction of variance and the number of hidden units, and discuss some properties of ELM, that may have been too strongly interpreted previously. © 2013 Springer-Verlag Berlin Heidelberg.},
  isbn = {18650929 (ISSN); 9783642297632 (ISBN)},
  langid = {english},
  pagetotal = {122},
  keywords = {Current practices,ELM,Extreme learning machine (ELM),Feature vectors,Hidden layers,Hidden units,Infinite numbers,Knowledge acquisition,Knowledge management,Machine learning,Model Selection,Neural network kernel,Neural networks},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-27T18:55:55.281Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/elm kernel scopus/Parviainen_Riihimäki_2013_A connection between extreme learning machine and neural network kernel.pdf}
}

@inproceedings{parviainenInterpretingExtremeLearning2010,
  title = {Interpreting Extreme Learning Machine as an Approximation to an Infinite Neural Network},
  booktitle = {{{KDIR}} - {{Proc}}. {{Int}}. {{Conf}}. {{Knowl}}. {{Discov}}. {{Inf}}. {{Retr}}.},
  author = {Parviainen, E. and Riihimäki, J. and Miche, Y. and Lendasse, A.},
  date = {2010},
  pages = {65--73},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651426442&partnerID=40&md5=3cc32251f98ad35fecc319c238761b44},
  abstract = {Extreme Learning Machine (ELM) is a neural network architecture in which hidden layer weights are randomly chosen and output layer weights determined analytically. We interpret ELM as an approximation to a network with infinite number of hidden units. The operation of the infinite network is captured by neural network kernel (NNK). We compare ELM and NNK both as part of a kernel method and in neural network contextsights gained from this analysis lead us to strongly recommend model selection also on the variance of ELM hidden layer weights, and not only on the number of hidden units, as is usually done with ELM. We also discuss some properties of ELM, which may have been too strongly interpreted in previous works.},
  isbn = {9789898425287 (ISBN)},
  langid = {english},
  keywords = {Extreme learning machine (ELM),Hidden layers,Hidden units,Infinite numbers,Information retrieval,Kernel methods,Learning systems,Model Selection,Network architecture,Neural network kernel,Neural networks,Output layer},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-25T13:01:10.854Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/Parviainen et al_2010_Interpreting extreme learning machine as an approximation to an infinite neural.pdf}
}

@article{petrosyanNeuralNetworkIntegral,
  title = {Neural Network Integral Representations with the {{ReLU}} Activation Function},
  author = {Petrosyan, Armenak and Dereventsov, Anton and Webster, Clayton G},
  abstract = {In this effort, we derive a formula for the integral representation of a shallow neural network with the ReLU activation function. We assume that the outer weighs admit a finite L1-norm with respect to Lebesgue measure on the sphere. For univariate target functions we further provide a closed-form formula for all possible representations. Additionally, in this case our formula allows one to explicitly solve the least L1-norm neural network representation for a given function.},
  langid = {english},
  keywords = {/unread},
  file = {/home/leix/Documents/Zotero/storage/XJJ9ISQQ/Petrosyan et al. - Neural network integral representations with the R.pdf}
}

@online{PkgJuliaLanguage,
  title = {Pkg · {{The Julia Language}}},
  url = {https://docs.julialang.org/en/v1/stdlib/Pkg/},
  urldate = {2023-08-27},
  file = {/home/leix/Documents/Zotero/storage/WMK8KRS8/Pkg.html}
}

@article{rathodReviewOptimizationImproving2021,
  title = {Review of {{Optimization}} in {{Improving Extreme Learning Machine}}},
  author = {Rathod, N. and Wankhade, S.},
  date = {2021},
  journaltitle = {EAI Endorsed Transactions on Industrial Networks and Intelligent Systems},
  shortjournal = {EAI. Endorsed. Trans. Ind. Netw. Intell. Syst.},
  volume = {8},
  number = {28},
  pages = {1--13},
  publisher = {{European Alliance for Innovation}},
  issn = {24100218 (ISSN)},
  doi = {10.4108/EAI.17-9-2021.170960},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117161583&doi=10.4108%2fEAI.17-9-2021.170960&partnerID=40&md5=eed0b1711b21d889c4b3dfcaf0e4d496},
  abstract = {Now a days Extreme Learning Machine has gained a lot of interest because of its noteworthy qualities over single hidden-layer feedforward neural networks and the kernel functions. Even if ELM has many advantages, it has some potential shortcomings such as performance sensitivity to the underlying state of the hidden neurons, input weights and the choice of functions of activation. To overcome the limitations of traditional ELM, analysts have devised numerical methods to optimise specific parts of ELM in order to enhance ELM performance for a variety of complicated difficulties and applications. Hence through this study, we intend to study the different algorithms developed for optimizing the ELM to enhance its performance in the aspects of survey criteria such as datasets, algorithm, objectives, training time, accuracy, error rate and the hidden neurons. This study will help other researchers to find out the research issues that lowering the performance of the ELM. © 2021. Nilesh Rathod et al.,. All Rights Reserved.},
  langid = {english},
  keywords = {/unread,Extreme learning machine (ELM),Feedforward neural networks,Hidden neurons,Input weights,Input weights and Activation bias,Kernel function,Kernel functions,Knowledge acquisition,Machine learning,Multilayer neural networks,Network layers,Numerical methods,Performance sensitivity,Research issues,Sensitivity,Single-feedforward neural networks,Single-hidden layer feedforward neural networks,Training time},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.019Z}
}

@article{reddyExploitingMachineLearning2021,
  title = {Exploiting {{Machine Learning Algorithms}} to {{Diagnose Foot Ulcers}} in {{Diabetic Patients}}},
  author = {Reddy, S.S. and Mahesh, G. and Preethi, N.M.},
  date = {2021},
  journaltitle = {EAI Endorsed Transactions on Pervasive Health and Technology},
  shortjournal = {EAI Endorsed Trans. Pervasive Health Technol.},
  volume = {7},
  number = {29},
  publisher = {{European Alliance for Innovation}},
  issn = {24117145 (ISSN)},
  doi = {10.4108/eai.24-8-2021.170752},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131548523&doi=10.4108%2feai.24-8-2021.170752&partnerID=40&md5=86d545a7a1c7a77a13b03ebe1e8d592f},
  abstract = {INTRODUCTION: Diabetic foot ulcer (DFU) is a complication of diabetes that affects most of the diabetic patients. It will cause open wounds on the foot. Untreated DFU will lead to amputation and infection, which results in removal of foot or leg. As diabetes is the major health problem faced by people of all age groups, identifying foot ulcers at an early stage is essential. In this context, an efficient model to predict the foot ulcer accurately was proposed in this work. OBJECTIVES: To predict DFU using an effective neural network algorithm on a suitable dataset that consists of risk factors and clinical outcomes of the disease. METHODS: In recent days, ML techniques are most commonly used for predicting various diseases. To achieve the objectives a neural network technique, namely extreme learning machine (ELM) is proposed to predict DFU accurately. In addition, three existing algorithms, namely KNN, SVM with Gaussian kernel and ANN are also considered. These are implemented in R programming. RESULTS: Algorithms compared in terms of five evaluation metrics accuracy, zero-one loss, threat score/critical success index (TS/CSI), false omission rate (FOR) and false discovery rate (FDR). The values of accuracy, 0-1 loss, TS/CSI, FOR and FDR obtained for ELM are 96.15\%, 0.0385, 0.95, 0 and 0.05 respectively. CONCLUSION: After comparison, it was discovered that ELM had outperformed other algorithms in terms of all the metrics. Thus, it was recommended to use ELM over other algorithms while predicting diabetic foot ulcers. © European Alliance for Innovation. All rights reserved.},
  langid = {english},
  keywords = {/unread,accuracy,Accuracy,Artificial neural network,artificial neural network (ANN),Critical success index,critical success index (CSI),Diabetic foot ulcer,Diseases,extreme learning machine (ELM),Extreme learning machine (ELM),False discovery rate,false omission rate (FOR) and false discovery rate (FDR),False omission rate and false discovery rate,Forecasting,Gaussian kernels,KNN,Knowledge acquisition,Learning algorithms,Neural networks,Support vector machines,SVM with gaussian kernel,SVM with Gaussian kernel,Zero-one,zero-one loss,Zero-one loss},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.020Z}
}

@article{rivolliMetafeaturesMetalearning2022,
  title = {Meta-Features for Meta-Learning},
  author = {Rivolli, Adriano and Garcia, Luís P. F. and Soares, Carlos and Vanschoren, Joaquin and family=Carvalho, given=André C. P. L. F., prefix=de, useprefix=true},
  date = {2022-03-15},
  journaltitle = {Knowledge-Based Systems},
  shortjournal = {Knowledge-Based Systems},
  volume = {240},
  pages = {108101},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2021.108101},
  url = {https://www.sciencedirect.com/science/article/pii/S0950705121011631},
  urldate = {2023-06-03},
  abstract = {Meta-learning is increasingly used to support the recommendation of machine learning algorithms and their configurations. These recommendations are made based on meta-data, consisting of performance evaluations of algorithms and characterizations on prior datasets. These characterizations, also called meta-features, describe properties of the data which are predictive for the performance of machine learning algorithms trained on them. Unfortunately, despite being used in many studies, meta-features are not uniformly described, organized and computed, making many empirical studies irreproducible and hard to compare. This paper aims to deal with this by systematizing and standardizing data characterization measures for classification datasets used in meta-learning. Moreover, it presents an extensive list of meta-features and characterization tools, which can be used as a guide for new practitioners. By identifying particularities and subtle issues related to the characterization measures, this survey points out possible future directions that the development of meta-features for meta-learning can assume.},
  langid = {english},
  keywords = {Characterization measures,Classification problems,Meta-features,Meta-learning},
  annotation = {17 citations (Crossref) [2023-06-03]},
  file = {/home/leix/Documents/Zotero_Papers/TFM/meta-learning/Rivolli et al_2022_Meta-features for meta-learning.pdf}
}

@article{ronoudEvolutionaryDeepBelief2019,
  title = {An Evolutionary Deep Belief Network Extreme Learning-Based for Breast Cancer Diagnosis},
  author = {Ronoud, S. and Asadi, S.},
  date = {2019},
  journaltitle = {Soft Computing},
  shortjournal = {Soft Comput.},
  volume = {23},
  number = {24},
  pages = {13139--13159},
  publisher = {{Springer Verlag}},
  issn = {14327643 (ISSN)},
  doi = {10.1007/s00500-019-03856-0},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062027551&doi=10.1007%2fs00500-019-03856-0&partnerID=40&md5=75acad33d54551ddc5c07853642c4d68},
  abstract = {Cancer is one of the leading causes of morbidity and mortality worldwide with increasing prevalence. Breast cancer is the most common type among women, and its early diagnosis is crucially important. Cancer diagnosis is a classification problem, where its nature requires very high classification accuracy. As artificial neural networks (ANNs) have a high capability in modeling nonlinear relationships in data, they are frequently used as good global approximators in prediction and classification problems. However, in complex problems such as diagnosing breast cancer, shallow ANNs may cause certain problems due to their limited capacity of modeling and representation. Therefore, deep architectures are essential for extracting the complicated structure of cancer data. Under such circumstances, deep belief networks (DBNs) are appropriate choice whose application involves two major challenges: (1) the method of fine-tuning the network weights and biases and (2) the number of hidden layers and neurons. The present study suggests two novel evolutionary methods, namely E(T)-DBN-BP-ELM and E(T)-DBN-ELM-BP, that address the first challenge via combining DBN with extreme learning machine (ELM) classifier. In the proposed methods, because of the very large solution space of DBN topologies, the genetic algorithm (GA), which is able to search globally in the solutions space wondrously, has been applied for architecture optimization to tackle the second challenge. The third proposed method in this paper, E(TW)-DBN, uses GA to solve both challenges, in which DBN topology and weights evolve simultaneously. The proposed models are tested using two breast cancer datasets and compared with the state-of-the-art methods in the literature in terms of classification performance metrics and area under ROC (AUC) curves. According to the results, the proposed methods exhibit very high diagnostic performance in classification of breast cancer. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.},
  langid = {english},
  keywords = {Architecture optimization,Bayesian networks,Breast cancer diagnosis,Classification (of information),Classification performance,Computer aided diagnosis,Decision support systems,Deep belief network,Deep belief networks,Deep learning,Diseases,Extreme learning machine (ELM),Genetic algorithms,Knowledge acquisition,Medical decision support system,Network architecture,Neural networks,Non-linear relationships,State-of-the-art methods,Topology},
  annotation = {41 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T14:39:23.522Z},
  file = {/home/leix/Documents/Zotero/storage/BV8HPLCI/10.1007@s00500-019-03856-0.pdf.pdf}
}

@article{rubio-solisMultilayerIntervalType22020,
  title = {A {{Multilayer Interval Type-2 Fuzzy Extreme Learning Machine}} for the Recognition of Walking Activities and Gait Events Using Wearable Sensors},
  author = {Rubio-Solis, A. and Panoutsos, G. and Beltran-Perez, C. and Martinez-Hernandez, U.},
  date = {2020},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {389},
  pages = {42--55},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2019.11.105},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078495810&doi=10.1016%2fj.neucom.2019.11.105&partnerID=40&md5=c48c9f37f7183db46df4f9b1e75e5de2},
  abstract = {In this paper, a novel Multilayer Interval Type-2 Fuzzy Extreme Learning Machine (ML-IT2-FELM) for the recognition of walking activities and Gait events is presented. The ML-IT2-FELM uses a hierarchical learning scheme that consists of multiple layers of IT2 Fuzzy Autoencoders (FAEs), followed by a final classification layer based on an IT2-FELM architecture. The core building block in the ML-IT2-FELM is an IT2-FELM, which is a generalised model of the Interval Type-2 Radial Basis Function Neural Network (IT2-RBFNN) and that is functionally equivalent to a class of simplified IT2 Fuzzy Logic Systems (FLSs). Each FAE in the ML-IT2-FELM employs an output layer with a direct-defuzzification process based on the Nie-Tan algorithm, while the IT2-FELM classifier includes a Karnik-Mendel type-reduction method (KM). Real data was collected using three inertial measurements units attached to the thigh, shank and foot of twelve healthy participants. The validation of the ML-IT2-FELM method is performed with two different experiments. The first experiment involves the recognition of three different walking activities: Level-Ground Walking (LGW), Ramp Ascent (RA) and Ramp Descent (RD). The second experiment consists of the recognition of stance and swing phases during the gait cycle. In addition, to compare the efficiency of the ML-IT2-FELM with other ML fuzzy methodologies, a kernel-based ML-IT2-FELM that is inspired by kernel learning and called KML-IT2-FELM is also implemented. The results from the recognition of walking activities and gait events achieved an average accuracy of 99.98\% and 99.84\% with a decision time of 290.4ms and 105ms, respectively, by the ML-IT2-FELM, while the KML-IT2-FELM achieved an average accuracy of 99.98\% and 99.93\% with a decision time of 191.9ms and 94ms. The experiments demonstrate that the ML-IT2-FELM is not only an effective Fuzzy Logic-based approach in the presence of sensor noise, but also a fast extreme learning machine for the recognition of different walking activities. © 2020},
  langid = {english},
  keywords = {/unread,accuracy,algorithm,Article,artificial neural network,Autoencoders,Computer circuits,controlled study,decision making task,Defuzzification method,Direct-defuzzification method,Electroplating shops,Equivalence classes,Extreme learning machine (ELM),Fuzzy Autoencoders (FAEs),Fuzzy inference,Fuzzy logic,Fuzzy neural networks,fuzzy system,gait,ground walking,human,human experiment,intermethod comparison,Interval Type-2 fuzzy logic system (IT2 FLSs),Interval type-2 fuzzy logic systems,kernel method,Kernel-based ELM,Knowledge acquisition,Machine learning,mathematical computing,multilayer interval type 2 fuzzy extreme learning machine,Multilayer neural networks,Multilayer Neural Networks (ML-NNs),Multilayers,normal human,priority journal,Radial basis function networks,ramp ascent,ramp descent,recognition,standing,walking,walking parameters,Wearable sensors},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.011Z}
}

@inproceedings{sarangiMultikernelBasedRandom2021,
  title = {Multi-Kernel Based {{Random Vector Functional Link Neural Network}} for {{Short-term Prediction}} of {{Wind Speed}}},
  booktitle = {Int. {{Conf}}. {{Adv}}. {{Power}}, {{Signal}}, {{Inf}}. {{Technol}}., {{APSIT}}},
  author = {Sarangi, S. and Dash, P.K. and Sahoo, B.N. and Bisoi, R.},
  editor = {{Nayak N.} and {Dash T.} and {Parida T.}},
  date = {2021},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/APSIT52773.2021.9641314},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123941518&doi=10.1109%2fAPSIT52773.2021.9641314&partnerID=40&md5=424f1118675c746c94e120ff1d98f333},
  abstract = {This work provides a wind speed prediction technique which is the combination of kernel functions and the random vector functional link neural network (RVFLN). The nonlinear kernel functions used in RVFLN called as MKRVFLN replace the traditional trial and error method to decide the number of neurons in hidden layer and also their appropriate activation functions. The MATLAB results demonstrates a comparison between ELM, RVFLN and MKRVFLN model. From comparison, the MKRVFLN forecasting model shows greater prediction accuracy. For wind seed prediction, the samples are collected at 10 minute, 30 minute, 1 hour and 3hour intervals of time from the wind farm named Sotavento locate in Spain. © 2021 IEEE.},
  isbn = {9781665425063 (ISBN)},
  langid = {english},
  keywords = {/unread,Empirical mode decomposition,Empirical Mode Decomposition,Empirical mode decomposition (EMD),Extreme learning machine (ELM),Forecasting,Functional link neural network,Learning machines,Multi kernel random vector functional link neural network,Multi Kernel RVFLN (MKRVFLN),Multi-kernel,Random vector functional link neural network,Random vector functional link neural network (RVFLN),Random vectors,Support vector machine,Support vector machine (SVM),Support vector machines,Support vectors machine,Vectors,Wind power,Wind speed},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.021Z}
}

@book{sauravFacialExpressionRecognition2020,
  title = {Facial {{Expression Recognition Using Improved Adaptive Local Ternary Pattern}}},
  author = {Saurav, S. and Singh, S. and Saini, R. and Yadav, M.},
  editorb = {{Chaudhuri B.B.} and {Nakagawa M.} and {Khanna P.} and {Kumar S.}},
  editorbtype = {redactor},
  date = {2020},
  journaltitle = {Adv. Intell. Sys. Comput.},
  volume = {1024},
  pages = {52},
  publisher = {{Springer Science and Business Media Deutschland GmbH}},
  doi = {10.1007/978-981-32-9291-8_4},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075595083&doi=10.1007%2f978-981-32-9291-8_4&partnerID=40&md5=8a81fc44698b2379d2918ffa6f181feb},
  abstract = {Recently, there has been a huge demand for assistive technology for industrial, commercial, automobile, and societal applications. In some of these applications, there is a requirement of an efficient and accurate system for automatic facial expression recognition (FER). Therefore, FER has gained enormous interest among computer vision researchers. Although there has been a plethora of work available in the literature, an automatic FER system has not yet reached the desired level of robustness and performance. In most of these works, there has been the dominance of appearance-based methods primarily consisting of local binary pattern (LBP), local directional pattern (LDP), local ternary pattern (LTP), gradient local ternary pattern (GLTP), and improved local ternary pattern (IGLTP). Keeping in view the popularity of appearance-based methods, in this paper, we have proposed an appearance-based descriptor called Improved Adaptive Local Ternary Pattern (IALTP) for automatic FER. This new descriptor is an improved version of ALTP, which has been proved to be effective in face recognition. We have investigated ALTP in more details and have proposed some improvements like the use of uniform patterns and dimensionality reduction via principal component analysis (PCA). The reduced features are then classified using kernel extreme learning machine (K-ELM) classifier. In order to validate the performance of the proposed method, experiments have been conducted on three different FER datasets using well-known evaluation measures such as accuracy, precision, recall, and F1-Score. The proposed approach has also been compared with some of the state-of-the-art works in literature and found to be more accurate and efficient. © 2020, Springer Nature Singapore Pte Ltd.},
  isbn = {21945357 (ISSN); 9789813292901 (ISBN)},
  langid = {english},
  pagetotal = {39},
  keywords = {/unread,Adaptive local ternary pattern (ALTP),Appearance-based methods,Automatic facial expression recognition,Computer vision,Dimensionality reduction,Extreme learning machine (ELM),Face recognition,Facial expression recognition,Facial expression recognition (FER),Kernel extreme learning machine (K-ELM),Knowledge acquisition,Learning systems,Local binary patterns,Local directional patterns (LDP),Local ternary patterns,Local ternary patterns (LTP),Petroleum reservoir evaluation,Principal component analysis,Principal component analysis (PCA)},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.011Z}
}

@book{sauravImageBasedFacialExpression2020,
  title = {Image-{{Based Facial Expression Recognition Using Local Neighborhood Difference Binary Pattern}}},
  author = {Saurav, S. and Singh, S. and Yadav, M. and Saini, R.},
  editorb = {{Chaudhuri B.B.} and {Chaudhuri B.B.} and {Nakagawa M.} and {Khanna P.} and {Kumar S.}},
  editorbtype = {redactor},
  date = {2020},
  journaltitle = {Adv. Intell. Sys. Comput.},
  volume = {1022 AISC},
  pages = {470},
  publisher = {{Springer Science and Business Media Deutschland GmbH}},
  doi = {10.1007/978-981-32-9088-4_38},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106739979&doi=10.1007%2f978-981-32-9088-4_38&partnerID=40&md5=3fa469777d698c93d0ee96786663f6a7},
  abstract = {Automatic facial expression recognition (FER) has gained enormous interest among the computer vision researchers in recent years because of its potential deployment in many industrial, consumer, automobile, and societal applications. There are a number of techniques available in the literature for FER; among them, many appearance-based methods such as local binary pattern (LBP), local directional pattern (LDP), local ternary pattern (LTP), gradient local ternary pattern (GLTP), and improved local ternary pattern (IGLTP) have been shown to be very efficient and accurate. In this paper, we propose a new descriptor called local neighborhood difference binary pattern (LNDBP). This new descriptor is motivated by the recent success of local neighborhood difference pattern (LNDP) which has been proven to be very effective in image retrieval. The basic characteristic of LNDP as compared with the traditional LBP is that it generates binary patterns based on a mutual relationship of all neighboring pixels. Therefore, in order to use the benefit of both LNDP and LBP, we have proposed LNDBP descriptor. Moreover, since the extracted LNDBP features are of higher dimension, therefore a dimensionality reduction technique has been used to reduce the dimension of the LNDBP features. The reduced features are then classified using the kernel extreme learning machine (K-ELM) classifier. In order to, validate the performance of the proposed method, experiments have been conducted on two different FER datasets. The performance has been observed using well-known evaluation measures, such as accuracy, precision, recall, and F1-score. The proposed method has been compared with some of the state-of-the-art works available in the literature and found to be very effective and accurate. © 2020, Springer Nature Singapore Pte Ltd.},
  isbn = {21945357 (ISSN); 9789813290877 (ISBN)},
  langid = {english},
  pagetotal = {457},
  keywords = {/unread,Appearance-based methods,Automatic facial expression recognition,Computer vision,Dimensionality reduction,Dimensionality reduction techniques,Extreme learning machine (ELM),Face recognition,Facial expression recognition,Facial expression recognition (FER),Image retrieval,Kernel extreme learning machine (K-ELM),Learning systems,Local directional patterns (LDP),Local neighborhood difference pattern (LNDP),Local ternary patterns,Local ternary patterns (LTP),Optical character recognition,Petroleum reservoir evaluation,Principal component analysis (PCA)},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.012Z}
}

@article{scardapaneOnlineSequentialExtreme2015,
  title = {Online {{Sequential Extreme Learning Machine}} with {{Kernels}}},
  author = {Scardapane, S. and Comminiello, D. and Scarpiniti, M. and Uncini, A.},
  date = {2015},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  shortjournal = {IEEE Trans. Neural Networks Learn. Sys.},
  volume = {26},
  number = {9},
  pages = {2214--2220},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {2162237X (ISSN)},
  doi = {10.1109/TNNLS.2014.2382094},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939797937&doi=10.1109%2fTNNLS.2014.2382094&partnerID=40&md5=eda98b859693862f410d5ea1d3205b3b},
  abstract = {The extreme learning machine (ELM) was recently proposed as a unifying framework for different families of learning algorithms. The classical ELM model consists of a linear combination of a fixed number of nonlinear expansions of the input vector. Learning in ELM is hence equivalent to finding the optimal weights that minimize the error on a dataset. The update works in batch mode, either with explicit feature mappings or with implicit mappings defined by kernels. Although an online version has been proposed for the former, no work has been done up to this point for the latter, and whether an efficient learning algorithm for online kernel-based ELM exists remains an open problem. By explicating some connections between nonlinear adaptive filtering and ELM theory, in this brief, we present an algorithm for this task. In particular, we propose a straightforward extension of the well-known kernel recursive least-squares, belonging to the kernel adaptive filtering (KAF) family, to the ELM framework. We call the resulting algorithm the kernel online sequential ELM (KOS-ELM). Moreover, we consider two different criteria used in the KAF field to obtain sparse filters and extend them to our context. We show that KOS-ELM, with their integration, can result in a highly efficient algorithm, both in terms of obtained generalization error and training time. Empirical evaluations demonstrate interesting results on some benchmarking datasets. © 2015 IEEE.},
  langid = {english},
  keywords = {Adaptive filtering,Algorithms,E-learning,Extreme learning machine (ELM),kernel,Kernel adaptive filtering,Kernel recursive least squares,Knowledge acquisition,Learning algorithms,Learning systems,Mapping,Nonlinear adaptive filtering,online learning,Online learning,Online sequential extreme learning machine,Position control,Recursive least square (RLS),recursive least square (RLS).,Social networking (online)},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-03-09T21:36:37.349Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/elm kernel scopus/Scardapane et al_2015_Online Sequential Extreme Learning Machine with Kernels.pdf}
}

@book{schleifDiscriminativeFastSoft2014,
  title = {Discriminative Fast Soft Competitive Learning},
  author = {Schleif, F.-M.},
  date = {2014},
  journaltitle = {Lect. Notes Comput. Sci.},
  volume = {8681 LNCS},
  pages = {88},
  publisher = {{Springer Verlag}},
  doi = {10.1007/978-3-319-11179-7_11},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958528446&doi=10.1007%2f978-3-319-11179-7_11&partnerID=40&md5=1b9cbf057a2e4794b9f7bda61797d190},
  abstract = {Proximity matrices like kernels or dissimilarity matrices provide non-standard data representations common in the life science domain. Here we extend fast soft competitive learning to a discriminative and vector labeled learning algorithm for proximity data. It provides a more stable and consistent integration of label information in the cost function solely based on a give proximity matrix without the need of an explicite vector space. The algorithm has linear computational and memory requirements and performs favorable to traditional techniques. © 2014 Springer International Publishing Switzerland.},
  isbn = {03029743 (ISSN); 9783319111780 (ISBN)},
  langid = {english},
  pagetotal = {81},
  keywords = {Consistent integrations,Data representations,Label information,Learning algorithms,Life-sciences,Matrix algebra,Memory requirements,Neural networks,Proximity matrix,Soft competitive learning,Traditional techniques},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-26T19:55:09.809Z},
  file = {/home/leix/Documents/Zotero/storage/3M6WET3Y/Schleif - 2014 - Discriminative fast soft competitive learning.pdf}
}

@inproceedings{schleifIncrementalProbabilisticClassification2015,
  title = {Incremental Probabilistic Classification Vector Machine with Linear Costs},
  booktitle = {Proc {{Int Jt Conf Neural Networks}}},
  author = {Schleif, F.-M. and Chen, H. and Tino, P.},
  date = {2015},
  volume = {2015-September},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/IJCNN.2015.7280377},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951164985&doi=10.1109%2fIJCNN.2015.7280377&partnerID=40&md5=67c1bd2daba1c781e2eda5b86ccf8824},
  abstract = {The probabilistic classification vector machine is a very effective and generic probabilistic and sparse classifier. A recently published incremental version improved the runtime complexity to quadratic costs. We derive the Nyström approximation for asymmetric matrices to obtain linear runtime and memory complexity for the incremental probabilistic classification vector machine while keeping similar prediction performance. © 2015 IEEE.},
  isbn = {9781479919604 (ISBN); 9781479919604 (ISBN); 9781479919604 (ISBN); 9781479919604 (ISBN)},
  langid = {english},
  keywords = {Complex networks,Memory complexity,Prediction performance,Probabilistic classification,Quadratic costs,Run time complexity,Runtimes,Sparse classifiers,Support vector machines,Vector machines,Vectors,Xenon},
  annotation = {6 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T12:13:10.891Z},
  file = {/home/leix/Documents/Zotero/storage/VA4HLELB/schleif2015.pdf.pdf}
}

@inproceedings{schleifProbabilisticClassificationVector2015,
  title = {Probabilistic {{Classification Vector Machine}} at Large Scale},
  booktitle = {Eur. {{Symp}}. {{Artif}}. {{Neural Networks}}, {{ESANN}}},
  author = {Schleif, F.-M. and Gisbrecht, A. and Tino, P.},
  date = {2015},
  pages = {555--560},
  publisher = {{i6doc.com publication}},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961775789&partnerID=40&md5=f25d55262c8c67439a846184383b1422},
  abstract = {Probabilistic kernel classifiers are effective approaches to solve classification problems but only few of them can be applied to indefinite kernels as typically observed in life science problems and are often limited to rather small scale problems. We provide a novel batch formulation of the Probabilistic Classification Vector Machine for large scale metric and non-metric data.},
  isbn = {9782875870148 (ISBN)},
  langid = {english},
  keywords = {Artificial intelligence,Effective approaches,Indefinite kernel,Kernel classifiers,Learning systems,Life-sciences,Neural networks,Non-Metric,Probabilistic classification,Problem solving,Small scale,Vector machines},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-27T12:14:47.904Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/citen scopus/Schleif et al_2015_Probabilistic Classification Vector Machine at large scale.pdf}
}

@book{schleifSparsePrototypeRepresentation2013,
  title = {Sparse Prototype Representation by Core Sets},
  author = {Schleif, F.-M. and Zhu, X. and Hammer, B.},
  date = {2013},
  journaltitle = {Lect. Notes Comput. Sci.},
  volume = {8206 LNCS},
  pages = {309},
  doi = {10.1007/978-3-642-41278-3_37},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890891015&doi=10.1007%2f978-3-642-41278-3_37&partnerID=40&md5=72f196e6a8370403028b2b106c55915a},
  abstract = {Due to the increasing amount of large data sets, efficient learning algorithms are necessary. Also the interpretation of the final model is desirable to draw efficient conclusions from the model results. Prototype based learning algorithms have been extended recently to proximity learners to analyze data given in non-standard data formats. The supervised methods of this type are of special interest but suffer from a large number of optimization parameters to model the prototypes. In this contribution we derive an efficient core set based preprocessing to restrict the number of model parameters to O(n/ε2) with n as the number of prototypes. Accordingly, the number of model parameters gets independent of the size of the data sets but scales with the requested precision ε of the core sets. Experimental results show that our approach does not significantly degrade the performance while significantly reducing the memory complexity. © 2013 Springer-Verlag.},
  isbn = {03029743 (ISSN); 9783642412776 (ISBN)},
  langid = {english},
  pagetotal = {302},
  keywords = {/unread,Core set,Engineering education,Large datasets,Learning algorithms,Memory complexity,Model parameters,Model results,Optimization,Optimization parameter,Prototype-based learning,Supervised methods},
  annotation = {Read\_Status: Not Reading Read\_Status\_Date: 2023-02-25T13:04:27.900Z}
}

@software{ScikitlearnScikitlearn2023,
  title = {Scikit-Learn/Scikit-Learn},
  date = {2023-04-16T15:21:02Z},
  origdate = {2010-08-17T09:43:38Z},
  url = {https://github.com/scikit-learn/scikit-learn},
  urldate = {2023-04-16},
  abstract = {scikit-learn: machine learning in Python},
  organization = {{scikit-learn}},
  keywords = {/unread}
}

@online{segaldan@seg.alLIBSVMJl,
  title = {{{LIBSVM}}.Jl},
  author = {Segal (dan@seg.al), Dan},
  url = {https://juliapackages.com/p/libsvm},
  urldate = {2023-03-17},
  abstract = {LIBSVM bindings for Julia},
  langid = {english},
  organization = {{Julia Packages}},
  keywords = {/unread},
  file = {/home/leix/Documents/Zotero/storage/LYQ4RNG4/libsvm.html}
}

@inproceedings{shangHYPERSPECTRALSUPERVISEDCLASSIFICATION2018,
  title = {{{HYPERSPECTRAL SUPERVISED CLASSIFICATION USING MEAN FILTERING BASED KERNEL EXTREME LEARNING MACHINE}}},
  booktitle = {Int. {{Workshop Earth Obs}}. {{Remote Sens}}. {{Appl}}., {{EORSA}} - {{Proc}}.},
  author = {Shang, W. and Wu, Z. and Xu, Y. and Zhang, Y. and Wei, Z.},
  editor = {{Weng Q.} and {Gamba P.} and {Chang N.-B.} and {Wang G.} and {Yao W.}},
  date = {2018},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/EORSA.2018.8598594},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061775311&doi=10.1109%2fEORSA.2018.8598594&partnerID=40&md5=f74eee8a1cf6450c508d602306c10a1f},
  abstract = {Extreme learning machine (ELM) is a single hidden layer neural network, and has a faster learning speed than the traditional neural networks. However, the kernel ELM (KELM) is increasingly gaining attentions in hyperspectral image(HSI) classification, due to its robustness. The widely used RBF kernel in KELM has achieved promising classification performance in hyperspectral image(HSI) classification, but the underlying data structure of the hyperspectral image is not taken into consideration. In this paper, we propose a novel spectral-spatial KELM method by incorporating the mean filtering (MF) kernel into the KELM model, which can properly compute the average of the spatial neighboring pixels in the kernel space. Experimental results on real hyperspectral datasets show that, the proposed method outperforms others kernel based KELM method, in terms of both computational efficiency and classification accuracy in HSI supervised classification. © 2018 IEEE.},
  isbn = {9781538666425 (ISBN)},
  langid = {english},
  keywords = {/unread,Classification (of information),Classification accuracy,Classification performance,Computational efficiency,Extreme learning machine (ELM),hyperspectral image(HSI) classification,Image classification,kernel ELM (KELM),Knowledge acquisition,Learning speed,mean filtering (MF) kernel,Network layers,Neural networks,Observatories,Remote sensing,Single-hidden-layer neural networks,Space optics,Spectroscopy,Supervised classification,Supervised learning},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.991Z}
}

@book{shawe-taylorKernelMethodsPattern2004,
  title = {Kernel Methods for Pattern Analysis},
  author = {Shawe-Taylor, John},
  editora = {Cristianini, Nello},
  editoratype = {collaborator},
  date = {2004},
  publisher = {{University Press}},
  location = {{Cambridge}},
  isbn = {978-0-521-81397-6},
  langid = {english},
  pagetotal = {xiv+462},
  keywords = {Algorismes,Aprenentatge automàtic,{Kernel, Funcions de},Mètodes estadístics,Percepció de formes,Processament de dades,Reconeixement de formes},
  annotation = {Read\_Status: In Progress Read\_Status\_Date: 2023-02-24T23:22:00.917Z},
  file = {/home/leix/Documents/Zotero/storage/XUEE95N4/Shawe-Taylor - 2004 - Kernel methods for pattern analysis.pdf}
}

@inproceedings{shenSpectralQuantitativeAnalysis2019,
  title = {Spectral Quantitative Analysis Based on {{AdaBoost}} Kernel Extreme Learning Machine for Gas Component Prediction of Underground Cable Channel},
  booktitle = {{{ICARM}} - {{Int}}. {{Conf}}. {{Adv}}. {{Robot}}. {{Mechatronics}}},
  author = {Shen, G. and Han, R. and Wu, Z. and Qiu, H. and Fan, H. and Ye, J.},
  date = {2019},
  pages = {195--200},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/ICARM.2018.8610714},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061479330&doi=10.1109%2fICARM.2018.8610714&partnerID=40&md5=e1e27ac9c779eb9a209203c1723f80a0},
  abstract = {Quantitative analysis of channel gas is of great significant for the safety of the maintenance staff. This paper proposes a spectral quantitative analysis method based on adaptive boosting kernel ELM for gas component prediction of underground cable channel. The proposed method uses the ensemble learning strategy based on some base models. Each base model are built by the conjunction process, namely, each base model are trained with the weights of training samples which are adjusted based on the performance of the last base model. Then, the outputs of every base models are combined into a weighted sum to obtain the final output of the proposed method. Moreover, the kernel ELM is adopted as the base model, where the kernel function is used to replace the random matrix in ELM for dealing with the nonlinearity of spectral data. A real gas spectral dataset including the methane, the carbon monoxide and the carbon dioxide is used in the experiments.The experiments results verify that the proposed model has higher effectiveness. © 2018 IEEE.},
  isbn = {9781538670668 (ISBN)},
  langid = {english},
  keywords = {/unread,Adaboost,Adaptive boosting,Cables,Carbon dioxide,Carbon monoxide,Chemical analysis,ELM,Ensemble learning,Extreme learning machine (ELM),Gas component,Kernel function,Learning systems,Maintenance staff,Neural networks,quantitative analysis,Random Matrix,Robotics,Spectral data,Training sample,Underground cables},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.003Z}
}

@book{shiRecognitionModelBased2014,
  title = {Recognition Model Based Feature Extraction and Kernel Extreme Learning Machine for High Dimensional Data},
  author = {Shi, Y. and Zhao, L.J. and Tang, J.},
  date = {2014},
  journaltitle = {Adv. Mater. Res.},
  volume = {875--877},
  pages = {2024},
  publisher = {{Trans Tech Publications}},
  doi = {10.4028/www.scientific.net/AMR.875-877.2020},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896273670&doi=10.4028%2fwww.scientific.net%2fAMR.875-877.2020&partnerID=40&md5=a1a0dcd92d03b6b518c1fb2b515a767a},
  abstract = {High dimensional data such as mass-spectrometric and near-infrared spectrum are always used in disease diagnosis and product quality monitoring. Aim at the nonlinear feature extraction and low learning speed problems, a novel modeling approach combined principal component analysis (PCA) with kernel extreme learning machine (KELM) is proposed. The extracted features using PCA algorithms are fed into nonlinear classification based KELM with fast learning speed. The numbers of the features are selected according the classification performance. The experimental results based on the mass-spectrometric data in the benchmark demonstrate that the proposed approach has better performance. This approach can also be used to target recognition based on radar data. © (2014) Trans Tech Publications, Switzerland.},
  isbn = {10226680 (ISSN); 9783037859933 (ISBN)},
  langid = {english},
  pagetotal = {2020},
  keywords = {/unread,Benchmarking,Classification performance,Diagnosis,Extreme learning machine (ELM),Feature extraction,High dimensional data,Kernel extreme learning machine (ELM),Learning systems,Near infrared spectra,Neural networks,Nonlinear classification,Nonlinear feature extraction,Principal component analysis,Product quality monitoring,Recognition models,Spectrometry},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.955Z}
}

@article{singhDetectionCoronaryArtery2018,
  title = {Detection of Coronary Artery Disease by Reduced Features and Extreme Learning Machine},
  author = {Singh, R.S. and Saini, B.S. and Sunkaria, R.K.},
  date = {2018},
  journaltitle = {Clujul Medical},
  shortjournal = {Clujul Med.},
  volume = {91},
  number = {2},
  pages = {166--175},
  publisher = {{Universitatea de Medicina si Farmacie Iuliu Hatieganu}},
  issn = {12222119 (ISSN)},
  doi = {10.15386/cjmed-882},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046011828&doi=10.15386%2fcjmed-882&partnerID=40&md5=ca20f54edac61717667fe87d04e57367},
  abstract = {Objective. Cardiovascular diseases generate the highest mortality in the globe population, mainly due to coronary artery disease (CAD) like arrhythmia, myocardial infarction and heart failure. Therefore, an early identification of CAD and diagnosis is essential. For this, we have proposed a new approach to detect the CAD patients using heart rate variability (HRV) signals. This approach is based on subspaces decomposition of HRV signals using multiscale wavelet packet (MSWP) transform and entropy features extracted from decomposed HRV signals. The detection performance was analyzed using Fisher ranking method, generalized discriminant analysis (GDA) and binary classifier as extreme learning machine (ELM). The ranking strategies designate rank to the available features extracted by entropy methods from decomposed heart rate variability (HRV) signals and organize them according to their clinical importance. The GDA diminishes the dimension of ranked features. In addition, it can enhance the classification accuracy by picking the best discerning of ranked features. The main advantage of ELM is that the hidden layer does not require tuning and it also has a fast rate of detection. Methodology. For the detection of CAD patients, the HRV data of healthy normal sinus rhythm (NSR) and CAD patients were obtained from a standard database. Self recorded data as normal sinus rhythm (Self\_NSR) of healthy subjects were also used in this work. Initially, the HRV time-series was decomposed to 4 levels using MSWP transform. Sixty two features were extracted from decomposed HRV signals by non-linear methods for HRV analysis, fuzzy entropy (FZE) and Kraskov nearest neighbour entropy (K-NNE). Out of sixty-two features, 31 entropy features were extracted by FZE and 31 entropy features were extracted by K-NNE method. These features were selected since every feature has a different physical premise and in this manner concentrates and uses HRV signals information in an assorted technique. Out of 62 features, top ten features were selected, ranked by a ranking method called as Fisher score. The top ten features were applied to the proposed model, GDA with Gaussian or RBF kernal + ELM having hidden node as sigmoid or multiquadric. The GDA method transforms top ten features to only one feature and ELM has been used for classification. Results. Numerical experimentations were performed on the combination of datasets as NSR-CAD and Self\_NSR- CAD subjects. The proposed approach has shown better performance using top ten ranked entropy features. The GDA with RBF kernel + ELM having hidden node as multiquadric method and GDA with Gaussian kernel + ELM having hidden node as sigmoid or multiquadric method achieved an approximate detection accuracy of 100\% compared to ELM and linear discriminant analysis (LDA)+ELM for both datasets. The subspaces level-4 and level-3 decomposition of HRV signals by MSWP transform can be used for detection and analysis of CAD patients. © 2018, Universitatea de Medicina si Farmacie Iuliu Hatieganu.},
  langid = {english},
  keywords = {/unread,accuracy,algorithm,Article,artificial neural network,cardiovascular disease,classifier,coronary artery disease,decomposition,discriminant analysis,entropy,Extreme learning machine (ELM),frequency analysis,Fuzzy entropy (FZE),fuzzy system,Generalized discriminant analysis (GDA),heart rate variability,k nearest neighbor,kernel method,Kraskov nearest neighbour entropy (K-NNE),machine learning,mathematical computing,mortality,multidimensional scaling,multiscale wavelet packet (MSWP) transform,orthogonal rotation,quantitative study,RNA translation,sensitivity analysis,sigmoid,signal transduction,sinus rhythm,support vector machine,wavelet analysis},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.991Z}
}

@book{singhNovelApproachProtein2012,
  title = {A Novel Approach to Protein Structure Prediction Using {{PCA}} or {{LDA}} Based Extreme Learning Machines},
  author = {Singh, L. and Chetty, G. and Sharma, D.},
  date = {2012},
  journaltitle = {Lect. Notes Comput. Sci.},
  volume = {7666 LNCS},
  pages = {499},
  doi = {10.1007/978-3-642-34478-7_60},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869025872&doi=10.1007%2f978-3-642-34478-7_60&partnerID=40&md5=b24b0e013e2c7986932872b55fab5404},
  abstract = {In the area of bio-informatics, large amount of data is harvested with functional and genetic features of proteins. The structure of protein plays an important role in its biological and genetic functions. In this study, we propose a protein structure prediction scheme based novel learning algorithms - the extreme learning machine and the Support Vector Machine using multiple kernel learning, The experimental validation of the proposed approach on a publicly available protein data set shows a significant improvement in performance of the proposed approach in terms of accuracy of classification of protein folds using multiple kernels where multiple heterogeneous feature space data are available. The proposed method provides the higher recognition ratio as compared to other methods reported in previous studies. © 2012 Springer-Verlag.},
  isbn = {03029743 (ISSN); 9783642344770 (ISBN)},
  issue = {PART 4},
  langid = {english},
  pagetotal = {492},
  keywords = {/unread,Classification (of information),Data processing,Experimental validations,Extreme learning machine (ELM),Extreme Learning Machines (ELM),Face recognition,Heterogeneous features,Image retrieval,Knowledge acquisition,Learning algorithms,Linear discriminant analysis,Linear Discriminant Analysis (LDA),Multiple Kernel Learning,Multiple kernels,Principal component analysis,Principal Component analysis (PCA),Protein data,Protein folding,Protein Folding,Protein folds,Protein structure prediction,Proteins,Recognition ratio,Support vector machines,Support Vector Machines (SVM)},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.946Z}
}

@book{singhNovelApproachProtein2012a,
  title = {A Novel Approach to Protein Structure Prediction Using {{PCA}} Based Extreme Learning Machines and Multiple Kernels},
  author = {Singh, L. and Chetty, G. and Sharma, D.},
  date = {2012},
  journaltitle = {Lect. Notes Comput. Sci.},
  volume = {7440 LNCS},
  pages = {299},
  doi = {10.1007/978-3-642-33065-0_31},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866648357&doi=10.1007%2f978-3-642-33065-0_31&partnerID=40&md5=97cf6dc3e5ba206c2f490453dcaf3bd6},
  abstract = {In the area of bio-informatics, large amount of data is harvested with functional and genetic features of proteins. The structure of protein plays an important role in its biological and genetic functions. In this study, we propose a protein structure prediction scheme based novel learning algorithms - the extreme learning machine and the Support Vector Machine using multiple kernel learning, The experimental validation of the proposed approach on a publicly available protein data set shows a significant improvement in performance of the proposed approach in terms of accuracy of classification of protein folds using multiple kernels where multiple heterogeneous feature space data are available. The proposed method provides the higher recognition ratio as compared to other methods reported in previous studies. © 2012 Springer-Verlag.},
  isbn = {03029743 (ISSN); 9783642330643 (ISBN)},
  issue = {PART 2},
  langid = {english},
  pagetotal = {292},
  keywords = {/unread,Classification (of information),Experimental validations,Extreme learning machine (ELM),Extreme Learning Machines (ELM),Face recognition,Heterogeneous features,Image retrieval,Learning algorithms,Linear discriminant analysis,Linear Discriminant Analysis (LDA),Multiple Kernel Learning,Multiple kernels,Principal component analysis,Principal Component analysis (PCA),Protein data,Protein folding,Protein Folding,Protein folds,Protein structure prediction,Proteins,Recognition ratio,Support vector machines,Support Vector Machines (SVM)},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.945Z}
}

@inproceedings{sunRobustVisualTracking2018,
  title = {Robust Visual Tracking Based on Extreme Learning Machine with Multiple Kernels Features Fusion},
  booktitle = {{{IEEE Int}}. {{Conf}}. {{Comput}}. {{Commun}}., {{ICCC}}},
  author = {Sun, R. and Wang, X. and Yan, X.},
  date = {2018},
  volume = {2018-January},
  pages = {2029--2033},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/CompComm.2017.8322893},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049778536&doi=10.1109%2fCompComm.2017.8322893&partnerID=40&md5=f484cfe2ade18b973ba4838f99913257},
  abstract = {Extreme learning machine (ELM) has attracted attentions in machine learning fields due to its remarkable advantages such as fast operation, simple solution, and strong generalization. In this paper, a new ELM based multiple kernels features fusion method is proposed for visual tracking. Visual tracking using multiple features has been proved as a robust approach because features could complement each other. In order to capture the non-linear relationship of features, we extend the ELM into multiple kernels framework. The shape and texture features are fused into Gaussian kernel space. Experimental results on publicly available videos show that the proposed method outperforms the state-of-the-art trackers. © 2017 IEEE.},
  isbn = {9781509063505 (ISBN)},
  langid = {english},
  keywords = {/unread,ELM,Extreme learning machine (ELM),Features Fusion,Features fusions,Kernel function,Kernel Function,Knowledge acquisition,Learning systems,Multiple features,Non-linear relationships,Robust approaches,Shape and textures,Visual Tracking},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.992Z}
}

@book{sunSentimentAnalysisUsing2016,
  title = {Sentiment Analysis Using Extreme Learning Machine with Linear Kernel},
  author = {Sun, S. and Gu, X.},
  editorb = {{Villa A.E.P.} and {Masulli P.} and {Rivero A.J.P.}},
  editorbtype = {redactor},
  date = {2016},
  journaltitle = {Lect. Notes Comput. Sci.},
  volume = {9887 LNCS},
  pages = {548},
  publisher = {{Springer Verlag}},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988433446&partnerID=40&md5=e6c33dc1b1b9e505acdedc6832b57ac3},
  abstract = {Sentiment classification is one of the hot research topics currently and Support Vector Machine (SVM) is usually used as the baseline method. In our research, Linear Kernel Extreme Learning Machine (Linear kernel ELM) has been applied firstly to the sentiment classification and it is compared with SVM on widely used sentiment (RT-2K) and subjectivity/ objective (Subj.) datasets. Furthermore, we build our datasets (Amazon Smartphone Review, ASR), an unbalanced dataset of product reviews with pre-defined 12 aspects. All the 2561 sentences belong to at least one aspect and every sentence is labeled as positive or negative sentiment. Therefore, ASR could be used in both sentiment classification at sentence level and aspect-based opinion summarization at aspect level. ELM is a standard single layer feedforward neural network(SLFN) and it is not required to tune the parameters of the hidden nodes [1]. Inspired by the successful use for sentiment analysis of SVM with linear kernel [2], we apply ELM with linear kernel to our three tasks. Meanwhile, the hidden node weights equal to sample values with zero bias. We use the bag-of-words model to test the robustness of SVM and linear kernel ELM through three different global term weighting schemes respectively [3]. (Table Presented) The experimental results show that the accuracy of linear kernel ELM is higher on the large dataset (Subj.), and is roughly the same as that of SVM on the small dataset (RT-2K). Linear kernel ELM is also a competitive sentiment classification approach on the unbalanced dataset(ASR). © Springer International Publishing Switzerland 2016.},
  isbn = {03029743 (ISSN); 9783319447803 (ISBN)},
  langid = {english},
  pagetotal = {547},
  keywords = {/unread,Bag-of-words models,Classification (of information),ELM,Extreme learning machine (ELM),Feedforward neural networks,Hot research topics,Information retrieval,Knowledge acquisition,Large dataset,Learning systems,Linear kernel,Multilayer neural networks,Negative sentiments,Network layers,Sentiment analysis,Sentiment classification,Single layer feed-forward neural networks,Support vector machines,Term weighting scheme},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.972Z}
}

@article{tavaresExtremeLearningMachine2015,
  title = {Extreme Learning Machine with Parallel Layer Perceptrons},
  author = {Tavares, L.D. and Saldanha, R.R. and Vieira, D.A.G.},
  date = {2015},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {166},
  pages = {164--171},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2015.04.018},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931561658&doi=10.1016%2fj.neucom.2015.04.018&partnerID=40&md5=03f0492948f5ab858c6947282abf51c7},
  abstract = {This paper proposes using the Parallel Layer Perceptron (PLP) network, instead of the Single Layer Feedforward neural network (SLFN) in the Extreme Learning Machine (ELM) framework. Differently from the SLFNs which consider cascade layers, the PLP is designed to accomplish also parallel layers, being the SLFN its particular case. This paper explores a particular PLP configuration which considers a nonlinear layer in parallel with a linear layer. For n inputs and m nonlinear neurons, it provides (. n+. 1). m linear parameters, while the SLFN would have only m linear parameters (one for each hidden neuron). Since the ELM is based on adjusting only the linear parameters using the least squares estimate (LSE), the PLP network provides more freedom for the proper adjustment. Results from 12 regression and 6 classification problems are presented considering the training and test errors, the linear vector norm and the system condition number. They point out that the PLP-ELM framework is more efficient than the SLFN-ELM approach. © 2015 Elsevier B.V..},
  langid = {english},
  keywords = {Article,artificial neural network,calculation,controlled study,Extreme learning machine (ELM),Feedforward neural networks,kernel method,Knowledge acquisition,least square estimate,Least square estimate,Least square estimates,Least squares estimate,Linear parameters,linear system,machine learning,Machine learning,mathematical computing,mathematical model,mathematical parameters,Multilayer neural networks,Network layers,Non-linear neurons,nonlinear system,Number theory,Parallel layer perceptron,parallel layer perceptron extreme learning machine,Parallel layer perceptrons,priority journal,Risk perception,Single layer feed-forward neural networks,single layer feedforward neural network,Structural risk minimization},
  annotation = {19 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T12:20:40.673Z},
  file = {/home/leix/Documents/Zotero/storage/F6FB7BE7/tavares2015.pdf.pdf}
}

@online{TreballFiMaster,
  title = {Treball de {{Fi}} de {{Màster}} | {{Facultat}} d'{{Informàtica}} de {{Barcelona}}},
  url = {https://www.fib.upc.edu/ca/estudis/masters/master-en-ciencia-de-dades/treball-de-fi-de-master},
  urldate = {2023-02-17},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-24T17:59:11.270Z},
  file = {/home/leix/Documents/Zotero/storage/5C67SAA5/Treball de Fi de Màster  Facultat d'Informàtica d.pdf;/home/leix/Documents/Zotero/storage/4BC36FI4/treball-de-fi-de-master.html}
}

@article{ulasNewApproachPrediction2020,
  title = {A New Approach for Prediction of the Wear Loss of {{PTA}} Surface Coatings Using Artificial Neural Network and Basic, Kernel-Based, and Weighted Extreme Learning Machine},
  author = {Ulas, M. and Altay, O. and Gurgenc, T. and Özel, C.},
  date = {2020},
  journaltitle = {Friction},
  shortjournal = {Friction},
  volume = {8},
  number = {6},
  pages = {1102--1116},
  publisher = {{Tsinghua University Press}},
  issn = {22237690 (ISSN)},
  doi = {10.1007/s40544-017-0340-0},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084549929&doi=10.1007%2fs40544-017-0340-0&partnerID=40&md5=2f36443dd5357b2ed1f66661531a6f9d},
  abstract = {Wear tests are essential in the design of parts intended to work in environments that subject a part to high wear. Wear tests involve high cost and lengthy experiments, and require special test equipment. The use of machine learning algorithms for wear loss quantity predictions is a potentially effective means to eliminate the disadvantages of experimental methods such as cost, labor, and time. In this study, wear loss data of AISI 1020 steel coated by using a plasma transfer arc welding (PTAW) method with FeCrC, FeW, and FeB powders mixed in different ratios were obtained experimentally by some of the researchers in our group. The mechanical properties of the coating layers were detected by microhardness measurements and dry sliding wear tests. The wear tests were performed at three different loads (19.62, 39.24, and 58.86 N) over a sliding distance of 900 m. In this study, models have been developed by using four different machine learning algorithms (an artificial neural network (ANN), extreme learning machine (ELM), kernel-based extreme learning machine (KELM), and weighted extreme learning machine (WELM)) on the data set obtained from the wear test experiments. The R2 value was calculated as 0.9729 in the model designed with WELM, which obtained the best performance [with 11among the models evaluated. © 2019, The Author(s).},
  langid = {english},
  keywords = {artificial neural network,Coatings,Dry sliding wear test,Electric welding,Equipment testing,Experimental methods,Extreme learning machine (ELM),Knowledge acquisition,Learning algorithms,Machine learning,Microhardness measurement,Neural networks,Plasma transfer arcs,plasma transferred arc welding,Sliding distances,Special test equipments,Statistical tests,surface coating,Surface coatings,wear loss prediction,Wear of materials},
  annotation = {14 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T16:10:03.537Z},
  file = {/home/leix/Documents/Zotero/storage/WHK3FCHC/ulas2020.pdf.pdf}
}

@dataset{v.sigillitoIonosphere1989,
  title = {Ionosphere},
  author = {V. Sigillito, S. Wing},
  date = {1989},
  publisher = {{UCI Machine Learning Repository}},
  doi = {10.24432/C5W01B},
  url = {https://archive.ics.uci.edu/dataset/52},
  urldate = {2023-08-17}
}

@inbook{vapnikDirectMethodsStatistical2000,
  title = {Direct {{Methods}} in {{Statistical Learning Theory}}},
  booktitle = {The {{Nature}} of {{Statistical Learning Theory}}},
  author = {Vapnik, Vladimir N.},
  date = {2000},
  pages = {225--265},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4757-3264-1_8},
  url = {http://link.springer.com/10.1007/978-1-4757-3264-1_8},
  urldate = {2023-02-24},
  bookauthor = {Vapnik, Vladimir N.},
  isbn = {978-1-4419-3160-3 978-1-4757-3264-1},
  annotation = {Read\_Status: In Progress Read\_Status\_Date: 2023-02-27T18:50:46.420Z},
  file = {/home/leix/Documents/Zotero/storage/SUFL3ZTY/Vapnik - 2000 - Direct Methods in Statistical Learning Theory.pdf}
}

@article{venkatesanNovelProgressiveLearning2016,
  title = {A Novel Progressive Learning Technique for Multi-Class Classification},
  author = {Venkatesan, R. and Er, M.J.},
  date = {2016},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {207},
  pages = {310--321},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2016.05.006},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971247457&doi=10.1016%2fj.neucom.2016.05.006&partnerID=40&md5=5a50f4d65beb23c154633b7ab86edcc9},
  abstract = {In this paper, a progressive learning technique for multi-class classification is proposed. This newly developed learning technique is independent of the number of class constraints and it can learn new classes while still retaining the knowledge of previous classes. Whenever a new class (non-native to the knowledge learnt thus far) is encountered, the neural network structure gets remodeled automatically by facilitating new neurons and interconnections, and the parameters are calculated in such a way that it retains the knowledge learnt thus far. This technique is suitable for real-world applications where the number of classes is often unknown and online learning from real-time data is required. The consistency and the complexity of the progressive learning technique are analyzed. Several standard datasets are used to evaluate the performance of the developed technique. A comparative study shows that the developed technique is superior. © 2016 Elsevier B.V.},
  langid = {english},
  keywords = {Article,Artificial intelligence,artificial neural network,Classification,Classification (of information),classifier,Classifiers,Comparative studies,comparative study,Complex networks,controlled study,extreme machine learning,intermethod comparison,Learning algorithms,Learning systems,Learning techniques,Machine learning,mathematical model,measurement accuracy,Multi-class,Multi-class classification,nerve cell,Neural network structures,Online learning,online sequential extreme machine learning,priority journal,Progressive learning,progressive learning algorithm,Sequential learning},
  annotation = {25 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T13:48:09.768Z},
  file = {/home/leix/Documents/Zotero/storage/NZ7NBENB/venkatesan2016.pdf.pdf}
}

@inproceedings{villmannFusionDeepLearning2017,
  title = {Fusion of Deep Learning Architectures, Multilayer Feedforward Networks and Learning Vector Quantizers for Deep Classification Learning},
  booktitle = {Int. {{Workshop Self-Organ}}. {{Maps Learn}}. {{Vector Quant}}., {{Clust}}. {{Data Vis}}., {{WSOM}} - {{Proc}}.},
  author = {Villmann, T. and Biehl, M. and Villmann, A. and Saralajew, S.},
  editor = {{Lamirel J.-C.} and {Olteanu M.} and {Cottrell M.}},
  date = {2017},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/WSOM.2017.8020009},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031312330&doi=10.1109%2fWSOM.2017.8020009&partnerID=40&md5=f219db09004d814fc3bf75cacd3f696a},
  abstract = {The advantage of prototype based learning vector quantizers are the intuitive and simple model adaptation as well as the easy interpretability of the prototypes as class representatives for the class distribution to be learned. Although they frequently yield competitive performance and show robust behavior nowadays powerful alternatives have increasing attraction. Particularly, deep architectures of multilayer networks achieve frequently very high accuracies and are, thanks to modern graphic processor units use for calculation, trainable in acceptable time. In this conceptual paper we show, how we can combine both network architectures to benefit from their advantages. For this purpose, we consider learning vector quantizers in terms of feedforward network architectures and explain how it can be combined effectively with multilayer or single-layer feedforward network architectures. This approach includes deep and flat architectures as well as the popular extreme learning machines. For the resulting networks, the multi-/ single-layer networks act as adaptive filters like in signal processing while the interpretability of the prototype-based learning vector quantizers is kept for the resulting filtered feature space. In this way a powerful combination of two successful architectures is obtained. © 2017 IEEE.},
  isbn = {9781509066384 (ISBN)},
  langid = {english},
  keywords = {Adaptive filtering,Adaptive filters,Class distributions,Competitive performance,Conformal mapping,Data visualization,Deep classifications,Deep learning,Extreme learning machine (ELM),Graphic processor units,Learning architectures,Learning systems,Multilayer feedforward networks,Multilayers,Network architecture,Network layers,Prototype-based learning,Self organizing maps,Signal processing,Vector quantization,Vector spaces,Vectors,Visualization},
  annotation = {7 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T13:59:04.906Z},
  file = {/home/leix/Documents/Zotero/storage/I27X3MXM/villmann2017.pdf.pdf}
}

@book{villmannGradientBasedLearning2013,
  title = {Gradient Based Learning in Vector Quantization Using Differentiable Kernels},
  author = {Villmann, T. and Haase, S. and Kästner, M.},
  date = {2013},
  journaltitle = {Adv. Intell. Sys. Comput.},
  volume = {198 AISC},
  pages = {204},
  publisher = {{Springer Verlag}},
  doi = {10.1007/978-3-642-35230-0_20},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872238020&doi=10.1007%2f978-3-642-35230-0_20&partnerID=40&md5=e9e8e3389ebaa72a830860a6e3cac63e},
  abstract = {Supervised and unsupervised prototype based vector quantization frequently are proceeded in the Euclidean space. In the last years, also non-standard metrics became popular. For classification by support vector machines, Hilbert space representations are very successful based on so-called kernel metrics. In this paper we give the mathematical justification that gradient based learning in prototype-based vector quantization is possible by means of kernel metrics instead of the standard Euclidean distance. We will show that an appropriate handling requires differentiable universal kernels defining the kernel metric. This allows a prototype adaptation in the original data space but equipped with a metric determined by the kernel. This approach avoids the Hilbert space representation as known for support vector machines. Moreover, we give prominent examples for differentiable universal kernels based on information theoretic concepts and show exemplary applications. © 2013 Springer-Verlag.},
  isbn = {21945357 (ISSN); 9783642352294 (ISBN)},
  langid = {english},
  pagetotal = {193},
  keywords = {/unread,Conformal mapping,Data space,Euclidean distance,Euclidean spaces,Gradient-based learning,Hilbert spaces,Information theory,Mathematical justification,Prototype adaptation,Support vector machines,Vector quantization},
  annotation = {Read\_Status: Not Reading Read\_Status\_Date: 2023-02-25T13:04:30.292Z}
}

@article{vongEmpiricalKernelMapbased2018,
  title = {Empirical Kernel Map-Based Multilayer Extreme Learning Machines for Representation Learning},
  author = {Vong, C.-M. and Chen, C. and Wong, P.-K.},
  date = {2018},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {310},
  pages = {265--276},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2018.05.032},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048832989&doi=10.1016%2fj.neucom.2018.05.032&partnerID=40&md5=5edb6b43ae27495f1767e513b16723d4},
  abstract = {Recently, multilayer extreme learning machine (ML-ELM) and hierarchical extreme learning machine (H-ELM) were developed for representation learning whose training time can be reduced from hours to seconds compared to traditional stacked autoencoder (SAE). However, there are three practical issues in ML-ELM and H-ELM: (1) the random projection in every layer leads to unstable and suboptimal performance; (2) the manual tuning of the number of hidden nodes in every layer is time-consuming; and (3) under large hidden layer, the training time becomes relatively slow and a large storage is necessary. More recently, issues (1) and (2) have been resolved by kernel method, namely, multilayer kernel ELM (ML-KELM), which encodes the hidden layer in form of a kernel matrix (computed by using kernel function on the input data), but the storage and computation issues for kernel matrix pose a big challenge in large-scale application. In this paper, we empirically show that these issues can be alleviated by encoding the hidden layer in the form of an approximate empirical kernel map (EKM) computed from low-rank approximation of the kernel matrix. This proposed method is called ML-EKM-ELM, whose contributions are: (1) stable and better performance is achieved under no random projection mechanism; (2) the exhaustive manual tuning on the number of hidden nodes in every layer is eliminated; (3) EKM is scalable and produces a much smaller hidden layer for fast training and low memory storage, thereby suitable for large-scale problems. Experimental results on benchmark datasets demonstrated the effectiveness of the proposed ML-EKM-ELM. As an illustrative example, on the NORB dataset, ML-EKM-ELM can be respectively up to 16 times and 37 times faster than ML-KELM for training and testing with a little loss of accuracy of 0.35\%, while the memory storage can be reduced up to 1/9. © 2018 Elsevier B.V.},
  langid = {english},
  keywords = {/unread,accuracy,algorithm,Approximation theory,Article,artificial neural network,controlled study,Digital storage,Empirical kernel map,Empirical kernel map (EKM),Encoding (symbols),Extreme learning machine (ELM),hierarchical extreme learning machine,human,information processing,Kernel learning,kernel method,Knowledge acquisition,learning,Learning systems,machine learning,Matrix algebra,memory consolidation,multilayer extreme learning machine,Multilayer extreme learning machine (ML-ELM),Multilayers,priority journal,Representation learning,Speech recognition,Stacked autoencoder,stacked autoencoder (SAE),Statistical tests,support vector machine,task performance},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.993Z}
}

@article{walterAutonomousLearningRepresentations2015,
  title = {Autonomous {{Learning}} of {{Representations}}},
  author = {Walter, O. and Haeb-Umbach, R. and Mokbel, B. and Paassen, B. and Hammer, B.},
  date = {2015},
  journaltitle = {KI - Kunstliche Intelligenz},
  shortjournal = {KI - Kunstl. Intell.},
  volume = {29},
  number = {4},
  pages = {339--351},
  publisher = {{Springer Science and Business Media Deutschland GmbH}},
  issn = {09331875 (ISSN)},
  doi = {10.1007/s13218-015-0372-1},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033342287&doi=10.1007%2fs13218-015-0372-1&partnerID=40&md5=7ec9d414c546d4edcc5f8660c41756ff},
  abstract = {Besides the core learning algorithm itself, one major question in machine learning is how to best encode given training data such that the learning technology can efficiently learn based thereon and generalize to novel data. While classical approaches often rely on a hand coded data representation, the topic of autonomous representation or feature learning plays a major role in modern learning architectures. The goal of this contribution is to give an overview about different principles of autonomous feature learning, and to exemplify two principles based on two recent examples: autonomous metric learning for sequences, and autonomous learning of a deep representation for spoken language, respectively. © 2015, Springer-Verlag Berlin Heidelberg.},
  langid = {english},
  keywords = {/notrelevant,Autonomous learning,Classical approach,Deep learning,Deep representation,Feature learning,Learn+,Learning algorithms,Learning technology,Metric learning,Representation learning,Spoken language,Spoken languages,Training data},
  annotation = {3 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T12:22:12.243Z},
  file = {/home/leix/Documents/Zotero/storage/GZK289MW/walter2015.pdf.pdf}
}

@inproceedings{wangActiveLearningBased2016,
  title = {Active {{Learning Based}} on {{Single-Hidden Layer Feed-Forward Neural Network}}},
  booktitle = {Proc. - {{IEEE Int}}. {{Conf}}. {{Syst}}., {{Man}}, {{Cybern}}., {{SMC}}},
  author = {Wang, R. and Kwong, S. and Jiang, Q. and Wong, K.-C.},
  date = {2016},
  pages = {2158--2163},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/SMC.2015.377},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964523738&doi=10.1109%2fSMC.2015.377&partnerID=40&md5=20f0ed2d92d7fd3ad756abdbad1e6907},
  abstract = {In this paper, we propose two stream-based active learning algorithms for single-hidden layer feed-forward neural networks (SLFNs) trained by extreme learning machine (ELM). Uncertainty and inconsistency are adopted as two sample selection criteria. Uncertainty reflects the nondeterminacy of a sample among different decision classes, which is calculated by information entropy or Gini-index. Inconsistency reflects the disagreement of the sample between its conditional features and decision labels, which is calculated by the lower approximations in fuzzy rough sets. Experimental results demonstrate that inconsistency-based strategy is more effective than uncertainty based strategy for SLFNs under stream-based environment. © 2015 IEEE.},
  isbn = {9781479986965 (ISBN)},
  langid = {english},
  keywords = {Active Learning,Algorithms,Approximation algorithms,Artificial intelligence,Cybernetics,Extreme Learning Machine,Extreme learning machine (ELM),Inconsistency,Knowledge acquisition,Learning algorithms,Learning systems,Rough set theory,SLFNs,Uncertainty},
  annotation = {1 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T13:53:23.849Z},
  file = {/home/leix/Documents/Zotero/storage/NXX5UCMV/wang2015.pdf.pdf}
}

@article{wangAsymptoticAnalysisLocally2020,
  title = {Asymptotic Analysis of Locally Weighted Jackknife Prediction},
  author = {Wang, D. and Wang, P. and Zhuang, S. and Wang, C. and Shi, J.},
  date = {2020},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {417},
  pages = {10--22},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2020.07.074},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089343234&doi=10.1016%2fj.neucom.2020.07.074&partnerID=40&md5=0f718d2de8efac2a5308b282c7e0ab21},
  abstract = {Locally weighted jackknife prediction(LW-JP) is a variant of conformal prediction, which can output interval prediction in regression problems built on traditional learning algorithms for point prediction named as underlying algorithms. Although empirical validity and efficiency of LW-JP have been reported in some works, there lacks theoretical understanding of it. This paper gives some theoretical analysis of LW-JP in the asymptotic setting, where the number of the training samples approaches infinity. Under some regularity assumptions and conditions, the asymptotic validity of LW-JP is proved in the nonlinear regression case with heteroscedastic errors. The proof is an extension of the asymptotic analysis of leave-one-out prediction intervals in linear regression with homoscedastic errors. Based on our analysis, two conformal regressors built on LW-JP are proposed and the experimental results showed that the algorithms are not only valid interval predictors, but also achieve the state-of-the-art performance of conformal regressors. © 2020 Elsevier B.V.},
  langid = {english},
  keywords = {/notrelevant,algorithm,article,Asymptotic analysis,Asymptotic validity,Conformal predictions,Forecasting,Interval prediction,Learning algorithms,linear regression analysis,Locally weighted jackknife prediction,Non-linear regression,prediction,Prediction interval,Regression problem,Regularity assumption,State-of-the-art performance,Theoretical analysis,theoretical study,Traditional learning,validity},
  annotation = {1 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T17:39:36.522Z},
  file = {/home/leix/Documents/Zotero/storage/SRGFPZQJ/wang2020.pdf.pdf}
}

@article{wangFastConformalPredictive2020,
  title = {A Fast Conformal Predictive System with Regularized Extreme Learning Machine},
  author = {Wang, D. and Wang, P. and Yuan, Y. and Wang, P. and Shi, J.},
  date = {2020},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Netw.},
  volume = {126},
  pages = {347--361},
  publisher = {{Elsevier Ltd}},
  issn = {08936080 (ISSN)},
  doi = {10.1016/j.neunet.2020.03.022},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083000530&doi=10.1016%2fj.neunet.2020.03.022&partnerID=40&md5=0f57c6619ee7aa3a3e2f4ad1ff29a8b7},
  abstract = {A conformal predictive system(CPS) is based on the learning framework of conformal prediction, which outputs cumulative distribution functions(CDFs) for labels in regression problems. The CDFs output by a CPS provide useful information for users, as they not only provide probability for the events related to the test labels, but also can be transformed to prediction intervals with the corresponding quantiles. Moreover, CPSs have the property of validity since the distributions and intervals they output have statistical compatibility with the realizations. This property is very useful for many risk-sensitive applications such as financial time series forecast and weather forecast. However, as based on conformal predictors, CPSs inherit the computational issue. To build a fast CPS, in this paper, we propose a CPS with regularized extreme learning machine as the underlying algorithm. To be specific, we combine the leave-one-out cross-conformal predictive system(Leave-One-Out CCPS), a variant of the original CPS, with regularized extreme learning machine(RELM), which is named as LOO-CCPS-RELM. We analyse the computational complexity of it and prove its asymptotic validity based on some regularity assumptions. We also prove that the error rate of the prediction interval output by LOO-CCPS-RELM is under control in the asymptotic setting. Experiments with 20 public data sets were conducted to test LOO-CCPS-RELM and the results showed that LOO-CCPS-RELM is empirically valid and compared favourably with the other CPSs. © 2020 Elsevier Ltd},
  langid = {english},
  keywords = {algorithm,Algorithms,article,Asymptotic validity,Conformal predictions,Conformal predictive system,Cross-conformal predictive system,Cumulative distribution function,Distribution functions,Extreme learning machine (ELM),Financial data processing,Financial time series,forecasting,Forecasting,human,Humans,Knowledge acquisition,machine learning,Machine learning,Machine Learning,prediction,Predictive systems,Regularity assumption,Regularized extreme learning machine,Sensitive application,time factor,Time Factors,time series analysis,validity,weather,Weather forecasting},
  annotation = {6 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T17:37:49.736Z},
  file = {/home/leix/Documents/Zotero/storage/EBYLV3QR/wang2020.pdf.pdf}
}

@article{wangFastEfficientConformal2018,
  title = {A Fast and Efficient Conformal Regressor with Regularized Extreme Learning Machine},
  author = {Wang, D. and Wang, P. and Shi, J.},
  date = {2018},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {304},
  pages = {1--11},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2018.04.012},
  abstract = {A conformal regressor combines conformal prediction and a traditional regressor for point predictions. It produces a valid prediction interval for a new testing input such that the probability of the target output being not included in the prediction interval is not more than a preset significance level. Although conformal prediction is both theoretically and empirically valid, one main drawback of the existing conformal regressors is their computational inefficiency. This paper proposes a novel fast and efficient conformal regressor named LW-JP-RELM, with combination of the local-weighted jackknife prediction (LW-JP), a new variant of conformal prediction, and the regularized extreme learning machine (RELM). The development of our learning algorithm is important both for the applications of extreme learning machine and conformal prediction. On the one hand, LW-JP-RELM complements ELM with interval predictions that satisfy a given level of confidence. On the other hand, the underlying learning process and the outstanding learning ability of RELM make LW-JP-RELM a very fast and informationally efficient conformal regressor. In the experiments, the empirical validity and informational efficiency of our method were compared to those of the state-of-art on 20 public data sets and the results confirmed that LW-JP-RELM is a competitive and promising conformal regressor. © 2018 Elsevier B.V.},
  langid = {english},
  keywords = {Article,artificial neural network,Computational efficiency,Conformal predictions,Conformal regressor,controlled study,Efficiency,Extreme learning machine (ELM),Forecasting,Interval prediction,Jackknife prediction,Knowledge acquisition,Learning abilities,learning algorithm,Learning algorithms,Learning process,Learning systems,local weighted jackknife prediction,machine learning,mathematical computing,prediction,Prediction interval,priority journal,probability,regularized extreme learning machine,Significance levels,validity},
  annotation = {11 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T14:32:30.088Z},
  file = {/home/leix/Documents/Zotero/storage/M26PEHJJ/wang2018.pdf.pdf}
}

@article{wangGaitRecognitionUsing2021,
  title = {Gait Recognition Using Optical Motion Capture: {{A}} Decision Fusion Based Method},
  author = {Wang, L. and Li, Y. and Xiong, F. and Zhang, W.},
  date = {2021},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {21},
  number = {10},
  publisher = {{MDPI AG}},
  issn = {14248220 (ISSN)},
  doi = {10.3390/s21103496},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105822309&doi=10.3390%2fs21103496&partnerID=40&md5=577da1f06db32cb7de08a66001f742fc},
  abstract = {Human identification based on motion capture data has received signification attentions for its wide applications in authentication and surveillance systems. The optical motion capture system (OMCS) can dynamically capture the high-precision three-dimensional locations of optical trackers that are implemented on a human body, but its potential in applications on gait recognition has not been studied in existing works. On the other hand, a typical OMCS can only support one player one time, which limits its capability and efficiency. In this paper, our goals are investigating the performance of OMCS-based gait recognition performance, and realizing gait recognition in OMCS such that it can support multiple players at the same time. We develop a gait recognition method based on decision fusion, and it includes the following four steps: feature extraction, unre-liable feature calibration, classification of single motion frame, and decision fusion of multiple motion frame. We use kernel extreme learning machine (KELM) for single motion classification, and in particular we propose a reliability weighted sum (RWS) decision fusion method to combine the fuzzy decisions of the motion frames. We demonstrate the performance of the proposed method by using walking gait data collected from 76 participants, and results show that KELM significantly outperforms support vector machine (SVM) and random forest in the single motion frame classification task, and demonstrate that the proposed RWS decision fusion rule can achieve better fusion accuracy compared with conventional fusion rules. Our results also show that, with 10 motion trackers that are implemented on lower body locations, the proposed method can achieve 100\% validation accuracy with less than 50 gait motion frames. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
  langid = {english},
  keywords = {/unread,algorithm,Algorithms,Classification tasks,Decision fusion,Decision fusion methods,Decision fusion rules,Decision trees,Extreme learning machine (ELM),gait,Gait,Gait analysis,Gait recognition,human,Human identification,Humans,Kernel ELM,Learning systems,motion,Motion,Motion capture,Motion classification,Optical motion capture,Pattern recognition,reproducibility,Reproducibility of Results,Sensor fusion,Support vector machines,Three dimensional locations,walking,Walking},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.022Z}
}

@article{wangGroundwaterContaminantSource2020,
  title = {Groundwater Contaminant Source Characterization with Simulation Model Parameter Estimation Utilizing a Heuristic Search Strategy Based on the Stochastic-Simulation Statistic Method},
  author = {Wang, H. and Lu, W. and Li, J.},
  date = {2020},
  journaltitle = {Journal of Contaminant Hydrology},
  shortjournal = {J. Contam. Hydrol.},
  volume = {234},
  publisher = {{Elsevier B.V.}},
  issn = {01697722 (ISSN)},
  doi = {10.1016/j.jconhyd.2020.103681},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088818924&doi=10.1016%2fj.jconhyd.2020.103681&partnerID=40&md5=93dcfcfee1d1f1a8c3261dab28cfe555},
  abstract = {In this study, a heuristic search strategy based on stochastic-simulation statistic (S–S) approach was developed for groundwater contaminant source characterization (GCSC) with simulation model parameter estimation. First, single kernel extreme learning machine (KELM) was built as surrogate system of the numerical simulation model to reduce huge computational load while evaluating the likelihood. However, compared with single KELM, multi-kernel extreme learning machine (MK-ELM) is more flexible for large amounts of data. To improve the approximation accuracy of the surrogate system to numerical simulation model, the MK-ELM surrogate system was first developed. Then, a heuristic search iterative process was first designed for GCSC with simulation model parameter estimation. The self-adaptive sampling method was proved to be more efficient than one-time sampling. Based on this idea, a self-adaptive feedback correction step was inserted into the heuristic search iterative process to ameliorate the training samples of the surrogate system in the posterior region, which further improved accuracy of simultaneous identification results. Finally, the identification results were obtained when the iteration terminated. The proposed approaches were tested in a hypothetical case study. It was shown that the heuristic search strategy can be used to assist in groundwater contaminant source characterization with simulation model parameter estimation. © 2020 Elsevier B.V.},
  langid = {english},
  keywords = {/unread,Adaptive sampling methods,algorithm,Algorithms,Approximation accuracy,Article,computer simulation,Computer Simulation,controlled study,data analysis,Extreme learning machine (ELM),ground water,Groundwater,Groundwater contaminant characterization,Groundwater contaminants,groundwater pollution,Groundwater pollution,Heuristic algorithms,Heuristic methods,Heuristic search strategy,heuristics,Heuristics,intermethod comparison,Iterative methods,Knowledge acquisition,Large amounts of data,machine learning,Machine learning,mathematical parameters,measurement accuracy,MK-ELM surrogate system,model test,multi kernel extreme learning machine,numerical model,Numerical models,parameter estimation,Parameter estimation,pollutant source,priority journal,sampling,self adaptive feedback correction,self adaptive sampling method,Self-adaptive feedback correction step,simulation,Simultaneous identification,single kernel extreme learning machine,statistical analysis,Stochastic models,stochastic simulation statistic method,Stochastic simulations,Stochastic systems,Stochastic-simulation statistic method,stochasticity,water contamination,water pollutant},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.013Z}
}

@article{wangImprovedIncrementalExtreme2016,
  title = {Improved incremental extreme learning machine based on multi-learning clonal selection algorithm},
  author = {Wang, C. and Wang, J.-H. and Gu, S.-S. and Wang, X. and Zhang, Y.-X.},
  date = {2016},
  journaltitle = {Kongzhi Lilun Yu Yingyong/Control Theory and Applications},
  shortjournal = {Kong Zhi Li Lun Yu Ying Yong},
  volume = {33},
  number = {3},
  pages = {368--379},
  publisher = {{South China University of Technology}},
  issn = {10008152 (ISSN)},
  doi = {10.7641/CTA.2016.50640},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964344111&doi=10.7641%2fCTA.2016.50640&partnerID=40&md5=e13e4d63f9697c2145957aed145dc047},
  abstract = {The great number of redundant nodes in an incremental extreme learning machine (I-ELM) may lower the learning efficiency of the algorithm, and complicate the network structure. To deal with this problem, we propose the improved I-ELM with kernel (I-ELMK) on the basis of multi-learning clonal selection algorithm (MLCSA). The MLCSA uses Baldwinian learning and Lamarckian learning, to exploit the search space by employing the information of antibodies, and reinforce the exploitation capacity of individual information. The proposed algorithm can limit the number of hidden layer neurons effectively to obtain more compact network architecture. The simulations show that MLCSI-ELMK has higher prediction accuracies online and off-line, while providing a better capacity of generalization compared with other algorithms. © 2016, Editorial Department of Control Theory \& Applications South China University of Technology. All right reserved.},
  langid = {chinese},
  keywords = {Algorithms,Baldwinian learning,Clonal selection algorithm,Clonal selection algorithms,Exploitation capacity,Hidden layer neurons,Incremental extreme learning machine,Knowledge acquisition,Lamarckian learning,Learning algorithms,Learning efficiency,Learning systems,Network architecture,Neural networks,Prediction accuracy,Soft computing},
  annotation = {Read\_Status: Not Reading Read\_Status\_Date: 2023-02-27T13:48:43.982Z}
}

@article{wangLearningCustomerBehaviors2019,
  title = {Learning Customer Behaviors for Effective Load Forecasting},
  author = {Wang, X. and Zhang, M. and Ren, F.},
  date = {2019},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  shortjournal = {IEEE Trans Knowl Data Eng},
  volume = {31},
  number = {5},
  pages = {938--951},
  publisher = {{IEEE Computer Society}},
  issn = {10414347 (ISSN)},
  doi = {10.1109/TKDE.2018.2850798},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049129118&doi=10.1109%2fTKDE.2018.2850798&partnerID=40&md5=e259f8cfbc2c114e8d399e74a291e30d},
  abstract = {Load forecasting has been deeply studied because of its critical role in Smart Grid. In current Smart Grid, there are various types of customers with different energy consumption patterns. Customer's energy consumption patterns are referred to as customer behaviors. It would significantly benefit load forecasting in a grid if customer behaviors could be taken into account. This paper proposes an innovative method that aggregates different types of customers by their identified behaviors, and then predicts the load of each customer cluster, so as to improve load forecasting accuracy of the whole grid. Sparse Continuous Conditional Random Fields (sCCRF) is proposed to effectively identify different customer behaviors through learning. A hierarchical clustering process is then introduced to aggregate customers according to the identified behaviors. Within each customer cluster, a representative sCCRF is fine-tuned to predict the load of its cluster. The final load of the whole grid is obtained by summing the loads of each cluster. The proposed method for load forecasting in Smart Grid has two major advantages. 1) Learning customer behaviors not only improves the prediction accuracy but also has a low computational cost. 2) sCCRF can effectively model the load forecasting problem of one customer, and simultaneously select key features to identify its energy consumption pattern. Experiments conducted from different perspectives demonstrate the advantages of the proposed load forecasting method. Further discussion is provided, indicating that the approach of learning customer behaviors can be extended as a general framework to facilitate decision making in other market domains. © 2018 IEEE.},
  langid = {english},
  keywords = {/notrelevant,Aggregates,Clustering algorithms,Conditional random field,continuous conditional random fields,Customer behavior,customer behaviors,Decision making,demand prediction,Demand prediction,Electric load management,Electric power plant loads,Electric power transmission networks,Energy utilization,Forecasting,Load forecasting,Load modeling,Neural networks,Predictive models,Random processes,Sales,Smart grid,Smart power grids,sparse CCRF,Sparse CCRF},
  annotation = {10 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T14:40:47.000Z},
  file = {/home/leix/Documents/Zotero/storage/88GDAGM2/wang2018.pdf.pdf}
}

@article{wangManifoldLearningLocal2016,
  title = {Manifold Learning in Local Tangent Space via Extreme Learning Machine},
  author = {Wang, Q. and Wang, W. and Nian, R. and He, B. and Shen, Y. and Björk, K.-M. and Lendasse, A.},
  date = {2016},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {174},
  pages = {18--30},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2015.03.116},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945325882&doi=10.1016%2fj.neucom.2015.03.116&partnerID=40&md5=2763788dfd1116e539ea422f6a6ddc97},
  abstract = {In this paper, we propose a fast manifold learning strategy to estimate the underlying geometrical distribution and develop the relevant mathematical criterion on the basis of the extreme learning machine (ELM) in the high-dimensional space. The local tangent space alignment (LTSA) method has been used to perform the manifold production and the single hidden layer feedforward network (SLFN) is established via ELM to simulate the low-dimensional representation process. The scheme of the ELM ensemble then combines the individual SLFN for the model selection, where the manifold regularization mechanism has been brought into ELM to preserve the local geometrical structure of LTSA. Some developments have been done to evaluate the inherent representation embedding in the ELM learning. The simulation results have shown the excellent performance in the accuracy and efficiency of the developed approach. © 2015 Elsevier B.V.},
  langid = {english},
  keywords = {accuracy,algorithm,Article,Extreme learning machine (ELM),Geometrical distribution,Geometrical structure,High dimensional spaces,High-dimensional space,image processing,information processing,intermethod comparison,Knowledge acquisition,Learning systems,local tangent space alignment,Local tangent space alignment,Low-dimensional representation,machine learning,manifold learning,Manifold learning,Manifold regularizations,mathematical model,Network layers,priority journal,process development,process optimization,single hidden layer feedforward network,validation process},
  annotation = {13 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T13:51:18.507Z},
  file = {/home/leix/Documents/Zotero/storage/38693RVU/wang2016.pdf.pdf}
}

@article{wangOnlineSequentialExtreme2014,
  title = {Online Sequential Extreme Learning Machine with Kernels for Nonstationary Time Series Prediction},
  author = {Wang, X. and Han, M.},
  date = {2014},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {145},
  pages = {90--97},
  publisher = {{Elsevier}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2014.05.068},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906938183&doi=10.1016%2fj.neucom.2014.05.068&partnerID=40&md5=3c1d0dd3c372ad6446684ef11a865bab},
  abstract = {In this paper, an online sequential extreme learning machine with kernels (OS-ELMK) has been proposed for nonstationary time series prediction. An online sequential learning algorithm, which can learn samples one-by-one or chunk-by-chunk, is developed for extreme learning machine with kernels. A limited memory prediction strategy based on the proposed OS-ELMK is designed to model the nonstationary time series. Performance comparisons of OS-ELMK with other existing algorithms are presented using artificial and real life nonstationary time series data. The results show that the proposed OS-ELMK produces similar or better accuracies with at least an order-of-magnitude reduction in the learning time. © 2014 Elsevier B.V.},
  langid = {english},
  keywords = {analytic method,article,controlled study,Extreme learning machine (ELM),Forecasting,kernel method,Knowledge acquisition,learning algorithm,Learning systems,Limited memory,machine learning,mathematical analysis,mean absolute percentage error,methodology,molecular dynamics,Non-stationary time series,Nonstationary,normalized root mean squared error,Online,Online sequential extreme learning machine,online sequential extreme learning machine with kernel,online support vector regression,online system,Performance comparison,prediction,priority journal,process optimization,root mean squared error,Sequential learning algorithm,simulation,support vector machine,Support vector machine,Support vector machines,symmetric mean absolute percentage error,Time series},
  annotation = {160 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-26T20:19:06.695Z},
  file = {/home/leix/Documents/Zotero/storage/E74NHDKP/wang2014.pdf.pdf}
}

@article{wangOscillationBoundGeneralization2015,
  title = {An Oscillation Bound of the Generalization Performance of Extreme Learning Machine and Corresponding Analysis},
  author = {Wang, D. and Wang, P. and Ji, Y.},
  date = {2015},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {151},
  number = {P2},
  pages = {883--890},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2014.10.006},
  abstract = {Extreme Learning Machine (ELM), proposed by Huang et al. in 2004 for the first time, performs better than traditional learning machines such as BP networks and SVM in some applications. This paper attempts to give an oscillation bound of the generalization performance of ELM and a reason why ELM is not sensitive to the number of hidden nodes, which are essential open problems proposed by Huang et al. in 2011. The derivation of the bound is in the framework of statistical learning theory and under the assumption that the expectation of the ELM kernel exists. It turns out that our bound is consistent with the experimental results about ELM obtained before and predicts that overfitting can be avoided even when the number of hidden nodes approaches infinity. The prediction is confirmed by our experiments on 15 data sets using one kind of activation function with every parameter independently drawn from the same Guasssian distribution, which satisfies the assumption above. The experiments also showed that when the number of hidden nodes approaches infinity, the ELM kernel with the activation is insensitive to the kernel parameter. © 2014 Elsevier B.V..},
  langid = {english},
  keywords = {analytical parameters,Article,Backpropagation,Chemical activation,controlled study,Extreme learning machine (ELM),Generalization performance,Hidden nodes,Infinite hidden nodes,Knowledge acquisition,Learning systems,machine learning,oscillation bound,Oscillation bound,performance,prediction,statistical analysis,Theoretical research},
  annotation = {13 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T12:42:04.280Z},
  file = {/home/leix/Documents/Zotero/storage/GG87YKR6/wang2015.pdf.pdf}
}

@article{weiyaoHumanActionRecognition2019,
  title = {Human {{Action Recognition Using Multilevel Depth Motion Maps}}},
  author = {Weiyao, X. and Muqing, W. and Min, Z. and Yifeng, L. and Bo, L. and Ting, X.},
  date = {2019},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {7},
  pages = {41811--41822},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {21693536 (ISSN)},
  doi = {10.1109/ACCESS.2019.2907720},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064834983&doi=10.1109%2fACCESS.2019.2907720&partnerID=40&md5=5e10bc6e72dfabedaf7223e1a1e088ca},
  abstract = {The advent of depth sensors opens up new opportunities for human action recognition by providing depth information. The main purpose of this paper is to present an effective method for human action recognition from depth images. A multilevel frame select sampling (MFSS) method are proposed to generate three levels of temporal samples from the input depth sequences first. Then, the proposed motion and static mapping (MSM) method is used to obtain the representation of MFSS sequences. After that, this paper exploits the block-based LBP feature extraction approach to extract features information from the MSM. Finally, the fisher kernel representation is applied to aggregate the block features, which is then combined with the kernel-based extreme learning machine classifier. The developed framework is evaluated on three public datasets captured by depth cameras. The experimental results demonstrate the great performance compared with the existing approaches. © 2013 IEEE.},
  langid = {english},
  keywords = {/unread,Depth camera,depth image,Depth image,Depth information,Depth sensors,ELM classifier,Extreme learning machine (ELM),fisher kernel,Fisher kernels,Human action recognition,Human-action recognition,Learning systems,Static mapping},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.003Z}
}

@online{WelcomePyMFEDocumentation,
  title = {Welcome to {{PyMFE}}’s Documentation! — Pymfe 0.4.2 Documentation},
  url = {https://pymfe.readthedocs.io/en/latest/},
  urldate = {2023-05-31},
  keywords = {/unread},
  file = {/home/leix/Documents/Zotero/storage/VMCBZMUX/latest.html}
}

@article{williamsComputationInfiniteNeural1998,
  title = {Computation with {{Infinite Neural Networks}}},
  author = {Williams, Christopher K. I.},
  date = {1998-07-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {10},
  number = {5},
  pages = {1203--1216},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976698300017412},
  url = {https://direct.mit.edu/neco/article/10/5/1203-1216/6175},
  urldate = {2023-02-25},
  abstract = {For neural networks with a wide class of weight priors, it can be shown that in the limit of an infinite number of hidden units, the prior over functions tends to a gaussian process. In this article, analytic forms are derived for the covariance function of the gaussian processes corresponding to networks with sigmoidal and gaussian hidden units. This allows predictions to be made efficiently using networks with an infinite number of hidden units and shows, somewhat paradoxically, that it may be easier to carry out Bayesian prediction with infinite networks rather than finite ones.},
  langid = {english},
  annotation = {90 citations (Crossref) [2023-02-25] Read\_Status: Read Read\_Status\_Date: 2023-02-25T12:41:27.568Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/kernels/Williams_1998_Computation with Infinite Neural Networks.pdf}
}

@inproceedings{williamsComputingInfiniteNetworks1996,
  title = {Computing with Infinite Networks},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Williams, Christopher K. I.},
  date = {1996-12-03},
  series = {{{NIPS}}'96},
  pages = {295--301},
  publisher = {{MIT Press}},
  location = {{Cambridge, MA, USA}},
  doi = {10.5555/2998981.2999023},
  abstract = {For neural networks with a wide class of weight-priors, it can be shown that in the limit of an infinite number of hidden units the prior over functions tends to a Gaussian process. In this paper analytic forms are derived for the covariance function of the Gaussian processes corresponding to networks with sigmoidal and Gaussian hidden units. This allows predictions to be made efficiently using networks with an infinite number of hidden units, and shows that, somewhat paradoxically, it may be easier to compute with infinite networks than finite ones.},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-25T12:41:36.214Z},
  file = {/home/leix/Documents/Zotero/storage/5MVKQCBI/Williams - 1996 - Computing with infinite networks.pdf}
}

@article{wongKernelBasedMultilayerExtreme2018,
  title = {Kernel-{{Based Multilayer Extreme Learning Machines}} for {{Representation Learning}}},
  author = {Wong, C.M. and Vong, C.M. and Wong, P.K. and Cao, J.},
  date = {2018},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  shortjournal = {IEEE Trans. Neural Networks Learn. Sys.},
  volume = {29},
  number = {3},
  pages = {757--762},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {2162237X (ISSN)},
  doi = {10.1109/TNNLS.2016.2636834},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008482332&doi=10.1109%2fTNNLS.2016.2636834&partnerID=40&md5=d70949aa03ae709251a54378df7433c2},
  abstract = {Recently, multilayer extreme learning machine (ML-ELM) was applied to stacked autoencoder (SAE) for representation learning. In contrast to traditional SAE, the training time of ML-ELM is significantly reduced from hours to seconds with high accuracy. However, ML-ELM suffers from several drawbacks: 1) manual tuning on the number of hidden nodes in every layer is an uncertain factor to training time and generalization; 2) random projection of input weights and bias in every layer of ML-ELM leads to suboptimal model generalization; 3) the pseudoinverse solution for output weights in every layer incurs relatively large reconstruction error; and 4) the storage and execution time for transformation matrices in representation learning are proportional to the number of hidden layers. Inspired by kernel learning, a kernel version of ML-ELM is developed, namely, multilayer kernel ELM (ML-KELM), whose contributions are: 1) elimination of manual tuning on the number of hidden nodes in every layer; 2) no random projection mechanism so as to obtain optimal model generalization; 3) exact inverse solution for output weights is guaranteed under invertible kernel matrix, resulting to smaller reconstruction error; and 4) all transformation matrices are unified into two matrices only, so that storage can be reduced and may shorten model execution time. Benchmark data sets of different sizes have been employed for the evaluation of ML-KELM. Experimental results have verified the contributions of the proposed ML-KELM. The improvement in accuracy over benchmark data sets is up to 7\%. © 2016 IEEE.},
  langid = {english},
  keywords = {/unread,Digital storage,Extreme learning machine (ELM),Inverse problems,Kernel learning,Knowledge acquisition,Learning systems,Linear transformations,Matrix algebra,Model generalization,Multi-layer kernels,multilayer extreme learning machine (ML-ELM),Multilayers,Pseudo-inverse solution,Random projections,Reconstruction error,representation learning,stacked autoencoder (SAE),Transformation matrices,Uncertain factors},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.994Z}
}

@article{wongPointsViewColor2011,
  title = {Points of View: {{Color}} Blindness},
  shorttitle = {Points of View},
  author = {Wong, Bang},
  date = {2011-06-01},
  journaltitle = {Nature Methods},
  volume = {8},
  number = {6},
  pages = {441--441},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/nmeth.1618},
  url = {https://www.nature.com/articles/nmeth.1618},
  urldate = {2023-08-31},
  issue = {6},
  langid = {english},
  keywords = {Media formats},
  file = {/home/leix/Documents/Zotero_Papers/TFM/Wong_2011_Points of view.pdf;/home/leix/Documents/Zotero/storage/P3S5V9HW/nmeth.html}
}

@inproceedings{wooDiscriminantPowerAnalyses2016,
  title = {Discriminant Power Analyses of Non-Linear Dimension Expansion Methods},
  booktitle = {Proc {{SPIE Int Soc Opt Eng}}},
  author = {Woo, S. and Lee, C.},
  editor = {{Lee C.} and {Huang B.} and {Chang C.-I.}},
  date = {2016},
  volume = {9874},
  publisher = {{SPIE}},
  doi = {10.1117/12.2224454},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991508716&doi=10.1117%2f12.2224454&partnerID=40&md5=b49fa93165b93ad6e465ae6dd32d31dc},
  abstract = {Most non-linear classification methods can be viewed as non-linear dimension expansion methods followed by a linear classifier. For example, the support vector machine (SVM) expands the dimensions of the original data using various kernels and classifies the data in the expanded data space using a linear SVM. In case of extreme learning machines or neural networks, the dimensions are expanded by hidden neurons and the final layer represents the linear classification. In this paper, we analyze the discriminant powers of various non-linear classifiers. Some analyses of the discriminating powers of non-linear dimension expansion methods are presented along with a suggestion of how to improve separability in non-linear classifiers. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.},
  isbn = {0277786X (ISSN); 9781510601154 (ISBN)},
  langid = {english},
  keywords = {/unread,classification,Classification (of information),Data compression,dimension expansion,Discriminant power,ELM,Extreme learning machine (ELM),kernel,Learning systems,Linear classification,Linear classifiers,Multivariate polynomial,Nonlinear classification,Nonlinear classifiers,reduced multivariate polynomial,Remote sensing,Support vector machines,Vector spaces},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.973Z}
}

@article{wuHybridIntelligentDeep2019,
  title = {Hybrid Intelligent Deep Kernel Incremental Extreme Learning Machine Based on Differential Evolution and Multiple Population Grey Wolf Optimization Methods},
  author = {Wu, D. and Qu, Z.S. and Guo, F.J. and Zhu, X.L. and Wan, Q.},
  date = {2019},
  journaltitle = {Automatika},
  shortjournal = {Autom.},
  volume = {60},
  number = {1},
  pages = {48--57},
  publisher = {{Taylor and Francis Ltd.}},
  issn = {00051144 (ISSN)},
  doi = {10.1080/00051144.2019.1570642},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065860274&doi=10.1080%2f00051144.2019.1570642&partnerID=40&md5=6b68b27fd235dfe968620ae73efc3625},
  abstract = {Focussing on the problem that redundant nodes in the kernel incremental extreme learning machine (KI-ELM) which leads to ineffective iteration increase and reduce the learning efficiency, a novel improved hybrid intelligent deep kernel incremental extreme learning machine (HI-DKIELM) based on a hybrid intelligent algorithms and kernel incremental extreme learning machine is proposed. At first, hybrid intelligent algorithms are proposed based on differential evolution (DE) and multiple population grey wolf optimization (MPGWO) methods which used to optimize the hidden layer neuron parameters and then to determine the effective hidden layer neurons number. The learning efficiency of the algorithm is improved by reducing the network complexity. Then, we bring in the deep network structure to the kernel incremental extreme learning machine to extract the original input data layer by layer gradually. The experiment results show that the HI-DKIELM methods proposed in this paper with more compact network structure have higher prediction accuracy and better ability of generation compared with other ELM methods. © 2019, © 2019 The Author(s). Published by Informa UK Limited, trading as Taylor \& Francis Group.},
  langid = {english},
  keywords = {/unread,differential evolution (DE),Extreme learning machine (ELM),hybrid intelligence (HI),kernel incremental extreme learning machine (KIELM),multiple population grey wolf optimization methods (MPGWO)},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.004Z}
}

@article{xiaKernelClusteringBasedPossibilistic2015,
  title = {A {{Kernel Clustering-Based Possibilistic Fuzzy Extreme Learning Machine}} for {{Class Imbalance Learning}}},
  author = {Xia, S.-X. and Meng, F.-R. and Liu, B. and Zhou, Y.},
  date = {2015},
  journaltitle = {Cognitive Computation},
  shortjournal = {Cognitive Comput.},
  volume = {7},
  number = {1},
  pages = {74--85},
  publisher = {{Springer Science and Business Media, LLC}},
  issn = {18669956 (ISSN)},
  doi = {10.1007/s12559-014-9256-1},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922835665&doi=10.1007%2fs12559-014-9256-1&partnerID=40&md5=34366078a4c47160c189d7118063851d},
  abstract = {Compared with traditional computational intelligence techniques such as the support vector machine, extreme learning machine (ELM) provides better generalization performance at a much faster learning speed without tuning model parameters. Unfortunately, the training process of ELM is still sensitive to the outliers or noises in the training set. On the other hand, when it comes to imbalanced datasets, ELM produces suboptimal classification models. In this paper, a kernel possibilistic fuzzy c-means clustering-based ELM algorithm for class imbalance learning (CIL) is developed to handle the class imbalance problem in the presence of outliers and noises. A set of experiments are conducted on several artificial and real-world imbalanced datasets for testing the generalization performance of the proposed algorithm. Additionally, we compare its performance with some typical CIL methods. The results indicate that the proposed method is a very effective method for CIL, especially in the presence of outliers and noises in datasets. © 2014, Springer Science+Business Media New York.},
  langid = {english},
  keywords = {C (programming language),Class imbalance learning,Class imbalance learning (CIL),Classification (of information),Clustering algorithms,Extreme learning machine (ELM),High-dimensional feature space,Intelligent computing,Kernel clustering,Knowledge acquisition,Learning systems,Outliers or noises,Statistics,Support vector machines},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-03-09T21:39:44.085Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/elm kernel scopus/Xia et al_2015_A Kernel Clustering-Based Possibilistic Fuzzy Extreme Learning Machine for.pdf}
}

@book{xieExtremeLearningMachine2018,
  title = {Extreme Learning Machine Based Diagnosis Models for Erythemato-Squamous Diseases},
  author = {Xie, J. and Ji, X. and Wang, M.},
  editorb = {{Zhou R.} and {Siuly S.} and {Wang H.} and {Huang Z.} and {Lee I.} and {Xiang W.}},
  editorbtype = {redactor},
  date = {2018},
  journaltitle = {Lect. Notes Comput. Sci.},
  volume = {11148 LNCS},
  pages = {74},
  publisher = {{Springer Verlag}},
  doi = {10.1007/978-3-030-01078-2_6},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057326396&doi=10.1007%2f978-3-030-01078-2_6&partnerID=40&md5=9ab595b4da6947aacd44fbf4bca68bd0},
  abstract = {Extreme learning machine based features selection algorithms are proposed in this paper for diagnosing erythemato-squamous diseases. The algorithms adopt the traditional ELM (extreme learning machine), EM-ELM (the error minimum extreme learning machine) and K-ELM (kernel extreme learning machine), respectively, to evaluate the power of the detected feature subset. The improved F-score and SFS (sequential forward search) strategy are combined to detect feature subsets. To detect a much more accurate diagnosis model for erythemato-squamous diseases, an ensemble diagnosis model is constructed by combining three models (classifiers) built on three feature subsets detected by proposed feature selection algorithms respectively. 5-fold cross validation experiments are conducted to test the performance of each feature selection algorithm, and the ensemble model. Experimental results demonstrate that the ensemble model has got the best accuracy. Its highest and average classification accuracy in 5-fold cross validation experiments are 100\% and 98.31\%, respectively. © Springer Nature Switzerland AG 2018.},
  isbn = {03029743 (ISSN); 9783030010775 (ISBN)},
  langid = {english},
  pagetotal = {61},
  keywords = {/unread,Classification accuracy,Computer aided diagnosis,Error minimum extreme learning machine,Erythemato-squamous disease,Extreme learning machine (ELM),F-score,Feature extraction,Feature selection,Feature selection algorithm,Features selection,Improved F-score,Kernel extreme learning machine,Knowledge acquisition,Learning algorithms,Learning systems,Neural networks,Sequential forward search},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.994Z}
}

@article{xieNewPredictionModel2016,
  title = {A New Prediction Model Based on the Leaching Rate Kinetics in the Alumina Digestion Process},
  author = {Xie, Y. and Wei, S. and Wang, X. and Xie, S. and Yang, C.},
  date = {2016},
  journaltitle = {Hydrometallurgy},
  shortjournal = {Hydrometallurgy},
  volume = {164},
  pages = {7--14},
  publisher = {{Elsevier B.V.}},
  issn = {0304386X (ISSN)},
  doi = {10.1016/j.hydromet.2016.05.005},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969543219&doi=10.1016%2fj.hydromet.2016.05.005&partnerID=40&md5=a49396606cda24140ab7df0e51edd3a4},
  abstract = {The leaching rate of alumina in the alumina digestion process is usually obtained via off-line analysis with a long time delay, leading to delayed control of the process and creating ongoing problems, such as a low leaching rate and wasted energy. Therefore, prediction of the online leaching rate is highly important. Based on mechanistic analysis of the double stream digestion process and the digestion kinetics of diaspore, a kinetics model established at the laboratory scale was scaled up to an industrial process. The unknown model parameters were estimated from the industrial data using a state transition algorithm (STA), which is a new and effective optimization algorithm. An error compensation model based on the kernel extreme learning machine (KELM) was subsequently built, and a prediction model for the leaching rate of alumina was established by parallel connection of the kinetics model with the compensation model. The validation results show that the model can predict the leaching rate of alumina for 90\% of the samples with relative errors within ± 2\% compared with the actual industrial data. The developed model will be further evaluated for control in the corresponding industrial process. © 2016 Elsevier B.V. All rights reserved.},
  langid = {english},
  keywords = {Algorithms,Alumina,Compensation model,Compensation modeling,Delay control systems,Electric connectors,Error compensation,Extreme learning machine (ELM),Forecasting,Industrial processs,Kernel extreme learning machine,Kinetics,Knowledge acquisition,Leaching,Leaching rate prediction,Leaching rates,Learning systems,Mechanistic analysis,Neural networks,Optimization,Optimization algorithms,Parallel connections,Process control,Reaction kinetics,State transition algorithm,State transitions,Time delay},
  annotation = {10 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T13:55:49.318Z},
  file = {/home/leix/Documents/Zotero/storage/9DDTILRJ/xie2016.pdf.pdf}
}

@article{xueIncrementalMultipleKernel2018,
  title = {Incremental Multiple Kernel Extreme Learning Machine and Its Application in {{Robo-advisors}}},
  author = {Xue, J. and Liu, Q. and Li, M. and Liu, X. and Ye, Y. and Wang, S. and Yin, J.},
  date = {2018},
  journaltitle = {Soft Computing},
  shortjournal = {Soft Comput.},
  volume = {22},
  number = {11},
  pages = {3507--3517},
  publisher = {{Springer Verlag}},
  issn = {14327643 (ISSN)},
  doi = {10.1007/s00500-018-3031-2},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045067743&doi=10.1007%2fs00500-018-3031-2&partnerID=40&md5=27d678b240168beefb96be023b222e1e},
  abstract = {Robo-advisors are a class of robots based on the financial needs of investors, through the algorithm and products to complete the previous financial advisory services provided by human intervention. They provide financial advice based on machine learning algorithms. However, many of the previous general algorithms are less suitable for information fusion in heterogeneous data. We propose an incremental multiple kernel extreme learning machine (IMK-ELM) model, which initializes a generic training database and then tunes itself to the classification task. Our IMK-ELM simultaneously updates the training dataset as well as the weights used to combine multiple information sources. We demonstrate our system on a financial recommendation problem in BCSs. We analyze the behavior of the algorithm, comparing its performance and scaling properties to other state-of-the-art approaches. Experimental results demonstrate that the proposed method appropriately solves a wide range of classification problems and is able to efficiently deal with large-scale tasks like Robo-advisors. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.},
  langid = {english},
  keywords = {Classification (of information),Classification tasks,Extreme learning machine (ELM),Finance,Incremental extreme learning machine,Incremental extreme learning machine (IELM),Information sources,Iterative methods,Knowledge acquisition,Learning algorithms,Learning systems,Multiple Kernel Learning,Multiple kernel learning (MKL),Neural networks,Non-iterative,Non-iterative learning,Robo-advisors,State-of-the-art approach},
  annotation = {9 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T14:34:03.650Z},
  file = {/home/leix/Documents/Zotero/storage/9IPM5WYV/ea38ebb9045b40976a8fb7f3f043f30b.pdf.pdf}
}

@article{xuIntervalPredictionMethod2022,
  title = {Interval {{Prediction Method}} for {{Wind Power Based}} on {{VMD-ELM}}/{{ARIMA-ADKDE}}},
  author = {Xu, T. and Du, Y. and Li, Y. and Zhu, M. and He, Z.},
  date = {2022},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {10},
  pages = {72590--72602},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {21693536 (ISSN)},
  doi = {10.1109/ACCESS.2022.3189477},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134229815&doi=10.1109%2fACCESS.2022.3189477&partnerID=40&md5=fad6efb0574cf6555d061fc05ac2bed9},
  abstract = {Wind power is a new energy source, for which the forecasting accuracy is related to the feasibility of various decision-making schemes for the power grid. The random, fluctuating and uncertain characteristics of wind power make accurate point forecasting very difficult and result in low practical application value. Therefore, a novel wind-power interval prediction method is proposed in this paper. First, variational model decomposition (VMD) is applied to the historical time sequence for wind power, followed by using an extreme learning machine (ELM) to establish a prediction model for the intrinsic mode function (IMF). An autoregressive integrated moving average (ARIMA) is used to establish a prediction model for the residual components, and the predicted IMF and residual components are reconstructed to predict the wind power. Finally, iterative self-organizing data analysis technique algorithm (ISODATA) are used with the error sequence to perform clustering segmentation, and adaptive diffusion kernel density estimation (ADKDE) is used to fit the probability density function for the prediction error of each segment and thereby obtain the cumulative distribution function and the final interval prediction under a given confidence level (CL). The results of a simulation based on real wind farm data from Hunan Province, China, from January to April 2020 show that the proposed method can significantly improve interval coverage. © 2013 IEEE.},
  langid = {english},
  keywords = {/unread,Adaptive diffusion kernel density estimation,adaptive diffusion kernel density estimation (ADKDE),Adaptive diffusions,Auto-regressive,Autoregressive integrated moving average,autoregressive integrated moving average (ARIMA),Clustering algorithms,Decision making,Diffusion kernel,Distribution functions,Electric power generation,Electric power transmission networks,Estimation,extreme learning machine (ELM),Extreme learning machine (ELM),Fitting,Interval prediction,Intrinsic mode functions,Iterative methods,Iterative self-organizing data technique algorithm,iterative self-organizing data technique algorithm (ISODATA),Kernel,Kernel Density Estimation,Knowledge acquisition,Learning algorithms,Learning machines,Learning systems,Moving averages,Predictive models,Probability density function,Self-organising,Statistics,Variational mode decomposition,variational mode decomposition (VMD),Weather forecasting,Wind farm,Wind forecasting,wind power,Wind power generation},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.025Z}
}

@article{xuNewDROSExtremeLearning2015,
  title = {A {{New DROS-Extreme Learning Machine}} with {{Differential Vector-KPCA Approach}} for {{Real-Time Fault Recognition}} of {{Nonlinear Processes}}},
  author = {Xu, Y. and Ye, L.-L. and Zhu, Q.-X.},
  date = {2015},
  journaltitle = {Journal of Dynamic Systems, Measurement and Control, Transactions of the ASME},
  shortjournal = {J Dyn Syst Meas Control Trans ASME},
  volume = {137},
  number = {5},
  publisher = {{American Society of Mechanical Engineers (ASME)}},
  issn = {00220434 (ISSN)},
  doi = {10.1115/1.4028716},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930667642&doi=10.1115%2f1.4028716&partnerID=40&md5=ae4b892486f691051bfdc7cf5b11cfa8},
  abstract = {In this paper, a new dynamic recurrent online sequential-extreme learning machine (DROS-ELM) OS-ELM with differential vector-kernel based principal component analysis (DV-KPCA) fault recognition approach is proposed to reconstruct the process feature and detect the process faults for real-time nonlinear system. Toward this end, the differential vector plus KPCA is first proposed to reduce the dimension of process data and enlarge the feature difference. In DV-KPCA, the differential vector is the difference between the input sample and the common sample, which is obtained from the historical data and represents the common invariant properties of the class. The optimal feature vectors of input sample and the common sample are obtained by KPCA procedure for the difference vectors. Through the differential operation between the input vectors and the common vectors, the reconstructed feature is derived by calculating the two-norm distance for the result of differential operation. The reconstructed features are then utilized to detect the process faults that may occur. In order to enhance the accuracy of fault recognition, a new DROS-ELM is developed by adding a self-feedback unit from the output of hidden layer to the input of hidden layer to record the sequential information. In the DROS-ELM, the output weight of feedback layer is updated dynamically by the change rate of output of the hidden layer. The DV-KPCA for feature reconstruction is exemplified using UCI handwriting (UCI handwriting recognition data: Database, using "Pen-Based Recognition of Handwritten Digits' produced in the Department of Computer Engineering Bogazici University, Istanbul 80815, Turkey, 1998), which the classification accuracy is obviously enhanced. Meanwhile, the DROS-ELM for process prediction is tested by the sunspot data from 1700 to 1987, which also shows better prediction accuracy than common methods. Finally, the new joint DROS-ELM with DV-KPCA method is exemplified in the complicated Tennessee Eastman (TE) benchmark process to illustrate the efficiencies. The results show that the DROS-ELM with DV-KPCA shows superiority not only in detection sensitivity and stability but also in timely fault recognition. © 2015 by ASME.},
  langid = {english},
  keywords = {/unread,Character recognition,Classification (of information),Classification accuracy,Differential operation,differential vector-KPCA,Dynamic recurrent OS-ELM,Extreme learning machine (ELM),Fault detection,fault recognition,Fault recognition,Feature extraction,Handwriting recognition,Kernel based principal component analysis,Knowledge acquisition,Learning systems,Online sequential extreme learning machine,Principal component analysis,Vectors},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.963Z}
}

@inproceedings{xuSoftsensingDevelopmentUsing2018,
  title = {Soft-Sensing Development Using Adaptive {{PSO}} Optimization Based Multi-Kernel {{ELM}} with Error Feedback},
  booktitle = {Proc. {{IEEE Data Driven Control Learn}}. {{Syst}}. {{Conf}}., {{DDCLS}}},
  author = {Xu, Y. and Du, Q. and Zhang, M. and Zhu, Q. and He, Y.},
  date = {2018},
  pages = {431--435},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/DDCLS.2018.8516051},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057012486&doi=10.1109%2fDDCLS.2018.8516051&partnerID=40&md5=3551e8e95e9aec477ef7ecddf43fe01c},
  abstract = {It is very hard to measure some process variables directly in actual industrial processes, so a soft senor model using adaptive particle swarm optimization (PSO) optimization based multi-kernel ELM with error feedback is proposed in this paper. Firstly, multi-kernel ELM is constructed by adding Gaussian and polynomial kernel function to ameliorate the overfitting problem in traditional ELM. Secondly, we propose an adaptive PSO (APSO) for ameliorating the low efficiency problem in the later period of PSO method by adding mutation operator. When given parameter reaches a threshold, the mutation operator adaptively adjusts the position of the particle. Also, the proportion of two kernel functions and the kernel parameters in training process are obtained by APSO. In each iteration, the training error is back propagated to the hidden layer as the co-outputs of hidden layer for further improving the accuracy and stability of the model. Finally, a simulation experiment on the purified terephthalic acid (PTA) solvent system is made to verify the modeling accuracy and optimized performances. The evaluation result demonstrates that the proposed method can provide higher accuracy and a more reliable soft senor model compared with other method. © 2018 IEEE.},
  isbn = {9781538626184 (ISBN)},
  langid = {english},
  keywords = {/unread,Adaptive particle swarm optimizations,error feedback,Error feedback,Errors,Extreme learning machine (ELM),Feedback,Iterative methods,Learning systems,multi-kernel,Multi-kernel,Optimized performance,Over fitting problem,Particle swarm optimization (PSO),PSO,Purification,Purified terephthalic acid,purified terephthalic acid (PTA) solvent system,Solvent system},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.995Z}
}

@book{yahiaDeepWaveletExtreme2020,
  title = {Deep {{Wavelet Extreme Learning Machine}} for {{Data Classification}}},
  author = {Yahia, S. and Said, S. and Zaied, M.},
  editorb = {{Quintian H.} and {Saez Munoz J.A.} and {Corchado E.} and {Martinez Alvarez F.} and {Troncoso Lora A.}},
  editorbtype = {redactor},
  date = {2020},
  journaltitle = {Adv. Intell. Sys. Comput.},
  volume = {951},
  pages = {113},
  publisher = {{Springer Verlag}},
  doi = {10.1007/978-3-030-20005-3_11},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065742021&doi=10.1007%2f978-3-030-20005-3_11&partnerID=40&md5=e041e907b43dfaedd1a6d3a712e20f58},
  abstract = {Recently, Extreme Learning Machine (ELM) has drawn an increasing attention, Due to its fast and good generalization ability. This paper proposes a new learning method for Extreme Learning Machine based wavelet and deep architecture. We have applied a composite wavelet activation function at the hidden nodes of ELM and the learning is done by a Deep Extreme Learning Machine. To evaluate the performance of our approach we have used a standard benchmark dataset for multi-class image classification (MNIST). Results show that our approach offers a significantly better performance relative to others approaches. \&\#x00A9; 2020, Springer Nature Switzerland AG.},
  isbn = {21945357 (ISSN); 9783030200046 (ISBN)},
  langid = {english},
  pagetotal = {105},
  keywords = {Auto encoders,Benchmark datasets,Benchmarking,Classification (of information),Data classification,Deep architectures,Deep learning,ELM auto-encoder,Extreme Learning Machine,Extreme learning machine (ELM),Generalization ability,Information systems,Information use,Knowledge acquisition,Wavelet activation function,Wavelet neural networks,Wavelet Neural Networks},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-27T17:46:53.849Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/citen scopus/Yahia et al_2020_Deep Wavelet Extreme Learning Machine for Data Classification.pdf}
}

@article{yahiaWaveletExtremeLearning2022,
  title = {Wavelet Extreme Learning Machine and Deep Learning for Data Classification},
  author = {Yahia, S. and Said, S. and Zaied, M.},
  date = {2022},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {470},
  pages = {280--289},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2020.04.158},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100406785&doi=10.1016%2fj.neucom.2020.04.158&partnerID=40&md5=dfdd53fc237a7e7ed387273b675bc2a9},
  abstract = {Recently, the Extreme Learning Machine (ELM) algorithm has been applied to various fields due to its rapidity and significant generalization performance. Traditionally, deep learning (DL) and wavelet neural networks (WNN) methods reach a high classification accuracy in machine learning applications. As a result, a new structure based on WNN, deep architecture and ELM is proposed in this paper. The proposed method is based on Extreme Learning Machine Auto-Encoder with DL structure and a composite wavelet activation function used in the hidden nodes. To evaluate the performance of our approach, we used standard benchmark data-sets, namely COIL-20, Pima Indian Diabetes (PID), MNIST and EMNIST. Experimental results show that our method offers satisfactory results and performance compared to other approaches. © 2021 Elsevier B.V.},
  langid = {english},
  keywords = {Article,artificial neural network,autoencoder,Benchmarking,Classification accuracy,Composite structures,convolutional neural network,data classification,Data classification,decision tree,deep learning,Deep learning,ELM auto-encoder,Extreme learning machine (ELM),Generalization performance,human,k nearest neighbor,Knowledge acquisition,learning algorithm,Learning systems,machine learning,Machine learning applications,Pima Indian Diabetes,radial basis function neural network,support vector machine,Wavelet activation function,Wavelet neural networks},
  annotation = {10 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T18:12:47.988Z},
  file = {/home/leix/Documents/Zotero/storage/W5WNKJG6/siwar2021.pdf.pdf}
}

@article{yangResearchPipelineBlocking2018,
  title = {Research on Pipeline Blocking State Recognition Algorithm Based on Mixed Domain Feature and {{KPCA-ELM}}},
  author = {Yang, J. and Feng, Z. and Wang, X. and Huang, G.},
  date = {2018},
  journaltitle = {International Journal of Computing Science and Mathematics},
  shortjournal = {Int. J. Comput. Sci. Math.},
  volume = {9},
  number = {5},
  pages = {442--454},
  publisher = {{Inderscience Publishers}},
  issn = {17525055 (ISSN)},
  doi = {10.1504/IJCSM.2018.095497},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054979785&doi=10.1504%2fIJCSM.2018.095497&partnerID=40&md5=62641d4520f99f2378d433f6f5288fab},
  abstract = {Aiming at the problem of recognition on pipeline blockage, a method based on mixed domain feature and KPCA-ELM is proposed. Firstly, the original acoustic impulse response signals are analysed by statistical analysis and local mean decomposition (LMD), in order to construct the mixed domain features, which are made up of time, frequency and time-frequency domain features. Then the kernel principal component analysis (KPCA) is adopted to reduce the high-dimensional features of mixed domain and extract the main features which reflect the operation state of main components. Finally, the main features are input to extreme learning machine (ELM) for state recognition. After the feature extraction by KPCA, the redundancy of input features is eliminated. The simulation results show that KPCA is more sensitive to the nonlinear characteristics of the pipeline blockage signal when compared with PCA. Meanwhile, ELM is superior to BP in terms of classification accuracy and time consuming. Copyright © 2018 Inderscience Enterprises Ltd.},
  langid = {english},
  keywords = {/unread,Acoustic impulse response,ELM,Extreme learning machine (ELM),Frequency domain analysis,High dimensional feature,Impulse response,Kernel principal component analyses (KPCA),Kernel principal component analysis,Knowledge acquisition,KPCA,Local mean decomposition (LMD),Machine learning,Nonlinear characteristics,Pipeline,Pipelines,State estimation,State recognition},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.996Z}
}

@article{yinRGBDObjectRecognition2020,
  title = {{{RGB-D}} Object Recognition Based on the Joint Deep Random Kernel Convolution and {{ELM}}},
  author = {Yin, Y. and Li, H.},
  date = {2020},
  journaltitle = {Journal of Ambient Intelligence and Humanized Computing},
  shortjournal = {J. Ambient Intell. Humanized Comput.},
  volume = {11},
  number = {11},
  pages = {4337--4346},
  publisher = {{Springer Science and Business Media Deutschland GmbH}},
  issn = {18685137 (ISSN)},
  doi = {10.1007/s12652-018-1067-x},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054299597&doi=10.1007%2fs12652-018-1067-x&partnerID=40&md5=cff99e76cfc603632b3393d33f08a973},
  abstract = {Nowadays RGB-D object recognition has been a challenging and important task in computer vision field. Convolutional Neural Network is a current popular algorithm for feature extraction from RGB and Depth modality separately, which cannot fully exploit some potential and complementary information between different modalities. The conventional training methods designed for CNN involve many gradient-descent searching, and usually face some troubles such as time-consuming convergence, local minima. In order to solve these problems, we propose a Joint Deep Radom Kernel Convolution and ELM (JDRKC-ELM) method for object recognition, which integrating the power of CNN feature extraction and fast training of ELM-AE. Our JDRKC-ELM can learn feature representations from raw RGB-D data directly. In this structure, Radom Kernel Convolutional neural network (RKCNN) is used for lower-level feature extraction from RGB and Depth modality separately. And then, combining these features from different modality by a feature fusion layer and feeding these fusion features to a Double-layer ELM-AE (DLELM-AE) for higher-level features. At last, the final feature representations are sent to a standard ELM for the object classification. We evaluate the quality of the JDRKC-ELM method on the RGB-D Object Dataset. The results show that the proposed method achieves high recognition accuracy and good generalization performance in comparison with deep learning methods and other ELM methods. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.},
  langid = {english},
  keywords = {/unread,CNN,Convolution,Convolutional neural networks,Deep learning,ELM,Extraction,Extreme learning machine (ELM),Feature extraction,Feature representation,Generalization performance,Gradient descent,Gradient methods,Kernel convolution,Learning systems,Object classification,Object recognition,Recognition accuracy,RGB-D,Training methods},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.013Z}
}

@article{youHighlyEfficientFramework2017,
  title = {Highly {{Efficient Framework}} for {{Predicting Interactions Between Proteins}}},
  author = {You, Z.-H. and Zhou, M. and Luo, X. and Li, S.},
  date = {2017},
  journaltitle = {IEEE Transactions on Cybernetics},
  shortjournal = {IEEE Trans. Cybern.},
  volume = {47},
  number = {3},
  pages = {731--743},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {21682267 (ISSN)},
  doi = {10.1109/TCYB.2016.2524994},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047726651&doi=10.1109%2fTCYB.2016.2524994&partnerID=40&md5=df399f58e313ea9683145d44b2a65f20},
  abstract = {Protein-protein interactions (PPIs) play a central role in many biological processes. Although a large amount of human PPI data has been generated by high-throughput experimental techniques, they are very limited compared to the estimated 130 000 protein interactions in humans. Hence, automatic methods for human PPI-detection are highly desired. This work proposes a novel framework, i.e., Low-rank approximation-kernel Extreme Learning Machine (LELM), for detecting human PPI from a protein's primary sequences automatically. It has three main steps: 1) mapping each protein sequence into a matrix built on all kinds of adjacent amino acids; 2) applying the low-rank approximation model to the obtained matrix to solve its lowest rank representation, which reflects its true subspace structures; and 3) utilizing a powerful kernel extreme learning machine to predict the probability for PPI based on this lowest rank representation. Experimental results on a large-scale human PPI dataset demonstrate that the proposed LELM has significant advantages in accuracy and efficiency over the state-of-art approaches. Hence, this work establishes a new and effective way for the automatic detection of PPI. © 2016 IEEE.},
  langid = {english},
  keywords = {/unread,Approximation theory,Arts computing,Automatic Detection,Big data,Biological process,biology,Computational Biology,{Databases, Protein},Experimental techniques,Extreme learning machine (ELM),feature extraction,Feature extraction,human,Humans,kernel extreme learning machine (K-ELM),Knowledge acquisition,Large dataset,Low rank approximations,low-rank approximation (LRA),machine learning,Machine learning,Machine Learning,Matrix algebra,Primary sequences,procedures,protein analysis,protein database,Protein interaction,Protein Interaction Mapping,Protein-protein interactions,protein-protein interactions (PPIs),Proteins,support vector machine,Support Vector Machine,support vector machine (SVM),Support vector machines},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.979Z}
}

@article{youKELMMethodSensor2017,
  title = {K-ELM method for sensor signal reconfiguration of aero-engine},
  author = {You, C.-X. and Lu, F. and Huang, J.-Q.},
  date = {2017},
  journaltitle = {Hangkong Dongli Xuebao/Journal of Aerospace Power},
  shortjournal = {Hangkong Dongli Xuebao},
  volume = {32},
  number = {1},
  pages = {221--226},
  publisher = {{BUAA Press}},
  issn = {10008055 (ISSN)},
  doi = {10.13224/j.cnki.jasp.2017.01.029},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015615798&doi=10.13224%2fj.cnki.jasp.2017.01.029&partnerID=40&md5=bf7366ea94b693f13d6a7eb6d744be91},
  abstract = {A fast leave-one-out cross validation method was presented to provide a criterion for kernel-based extreme learning machine (K-ELM) used for sensor signal reconfiguration of aero-engine. The procedure can avoid N times explicit training (N is the number of training samples) so that the computation cost was reduced 1/N when compared with normal leave-one-out procedure. Simulation results show that this method can provide a fast and accurate criterion, allowing to select best kernel parameters for K-ELM. © 2017, Editorial Department of Journal of Aerospace Power. All right reserved.},
  langid = {chinese},
  keywords = {Aero-engine,Aircraft engines,Engines,Extreme learning machine (ELM),Kernel parameter,Kernel parameters,Kernel-based extreme learning machine (K-ELM),Knowledge acquisition,Learning systems,Leave-one-out cross validation method,Leave-one-out cross validations,Sensor signal reconfiguration,Sensor signals,Statistical methods},
  annotation = {Read\_Status: Not Reading Read\_Status\_Date: 2023-02-27T13:59:29.603Z}
}

@article{yuanNewRemoteIntelligent2015,
  title = {A New Remote Intelligent Diagnosis System for Marine Diesel Engines Based on an Improved Multi-Kernel Algorithm},
  author = {Yuan, Y. and Yan, X. and Wang, K. and Yuan, C.},
  date = {2015},
  journaltitle = {Proceedings of the Institution of Mechanical Engineers, Part O: Journal of Risk and Reliability},
  shortjournal = {Proc. Inst. Mech. Eng. Part O J. Risk Reliab.},
  volume = {229},
  number = {6},
  pages = {604--611},
  publisher = {{SAGE Publications Ltd}},
  issn = {1748006X (ISSN)},
  doi = {10.1177/1748006X15595541},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947730383&doi=10.1177%2f1748006X15595541&partnerID=40&md5=e61b30798ef91185425d3aa38c02bd34},
  abstract = {Due to heavy work load of marine diesel engines, the failure in their mechanical components may result in serious accidents. Existing condition monitoring methods for marine diesel engines usually adopt warning after the failure occurred. In order to predict potential faults, this work has put forward a remote intelligent monitoring system for marine diesel engines. The global system for mobile communication mode was employed to construct the basis of data remote transmission, and a new multi-kernel extreme learning machine algorithm was proposed to diagnose the early faults in an intelligent method. Experimental tests were carried out in the marine diesel engine fault diagnosis set-up. The analysis results show that the proposed remote intelligent monitoring system can accurately, timely and reliably detect the potential failures. Meanwhile, the proposed multi-kernel extreme learning machine was compared with the existing methods. The comparison indicates that the multi-kernel extreme learning machine outperforms its rivals in term of fault detection rate by an increase of 3.4\%. Therefore, the proposed remote intelligent monitoring system has good prospects for engineering applications. © 2015 Institution of Mechanical Engineers.},
  langid = {english},
  keywords = {Condition monitoring,Diesel engines,Engineering applications,Extreme learning machine (ELM),Failure analysis,Fault detection,fault diagnosis,Global system for mobile communications,Global system for mobiles,Intelligent diagnosis system,Knowledge acquisition,Learning algorithms,Machine learning,Marine communication,Marine diesel engines,Marine Diesel Engines,Marine engines,Multi-kernel,multi-kernel algorithm,remote communication,Remote communication,Remote intelligent monitoring},
  annotation = {3 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T12:49:10.335Z},
  file = {/home/leix/Documents/Zotero/storage/9LEXHZQS/yuan2015.pdf.pdf}
}

@article{yuLearningDeepRepresentations2015,
  title = {Learning Deep Representations via Extreme Learning Machines},
  author = {Yu, W. and Zhuang, F. and He, Q. and Shi, Z.},
  date = {2015},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {149},
  pages = {308--315},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2014.03.077},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961291828&doi=10.1016%2fj.neucom.2014.03.077&partnerID=40&md5=594602d3111d3a53b0fb3276275c0831},
  abstract = {Extreme learning machine (ELM) as an emerging technology has achieved exceptional performance in large-scale settings, and is well suited to binary and multi-class classification, as well as regression tasks. However, existing ELM and its variants predominantly employ single hidden layer feedforward networks, leaving the popular and potentially powerful stacked generalization principle unexploited for seeking predictive deep representations of input data. Deep architectures can find higher-level representations, thus can potentially capture relevant higher-level abstractions. But most of current deep learning methods require solving a difficult and non-convex optimization problem. In this paper, we propose a stacked model, DrELM, to learn deep representations via extreme learning machine according to stacked generalization philosophy. The proposed model utilizes ELM as a base building block and incorporates random shift and kernelization as stacking elements. Specifically, in each layer, DrELM integrates a random projection of the predictions obtained by ELM into the original feature, and then applies kernel functions to generate the resultant feature. To verify the classification and regression performance of DrELM, we conduct the experiments on both synthetic and real-world data sets. The experimental results show that DrELM outperforms ELM and kernel ELMs, which appear to demonstrate that DrELM could yield predictive features that are suitable for prediction tasks. The performances of the deep models (i.e. Stacked Auto-encoder) are comparable. However, due to the utilization of ELM, DrELM is easier to learn and faster in testing. © 2014 Elsevier B.V.},
  issue = {Part A},
  langid = {english},
  keywords = {algorithm,Article,Classification (of information),classifier,controlled study,Convex optimization,Deep Belief Networks,Deep learning,deep representations learning via extreme learning machine,DrELM,Extreme learning machine (ELM),information processing,kernel based extreme learning machine,Knowledge acquisition,layer by layer learning algorithm,linear extreme learning machine,linear kernel,machine learning,Network layers,Optimally Pruned Extreme Learning Machine,prediction,Representation learning,Stacked Auto encoder,Stacked ELMs,Stacked generalization,statistical analysis},
  annotation = {84 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T12:48:11.703Z},
  file = {/home/leix/Documents/Zotero/storage/S5BKWEIX/yu2015.pdf.pdf}
}

@article{yuongwongConstrainedOptimizationBased2016,
  title = {A {{Constrained Optimization}} Based {{Extreme Learning Machine}} for Noisy Data Regression},
  author = {Yuong Wong, S. and Siah Yap, K. and Jen Yap, H.},
  date = {2016},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {171},
  pages = {1431--1443},
  publisher = {{Elsevier}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2015.07.065},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944463466&doi=10.1016%2fj.neucom.2015.07.065&partnerID=40&md5=a0d84d6f5a1df2b69bfc1cc373100ee3},
  abstract = {Most of the existing Artificial Intelligence (AI) models for data regression commonly assume that the data samples are completely clean without noise or worst yet, only the symmetrical noise is in considerations. However in the real world applications, this is often not the case. This paper addresses a significant note of inefficiency in methods for regression when dealing with outliers, especially for cases with polarity of noise involved (i.e., one sided noise with either only positive noise or negative noise). Using soft margin loss function concept, we propose Constrained Optimization method based Extreme Learning Machine for Regression, hereafter denoted as CO-ELM-R. The proposed method incorporates the two Lagrange multipliers that mimic Support Vector Regression (SVR) into the basis of ELM to cope with infeasible constraints of the regression optimization problem. Thus, CO-ELM-R will complement the recursive iterations of SVR in the training phase due to the fact that ELM is much simpler in structure and faster in implementation. The proposed CO-ELM-R is evaluated empirically on a few benchmark data sets and a real world application of NO. x gas emission data set collected from one of the power plant in Malaysia. The obtained results have demonstrated its validity and efficacy in handling noisy data regression problems. © 2015 Elsevier B.V.},
  langid = {english},
  keywords = {/unread,algorithm,Article,artificial intelligence,Artificial intelligence,artificial neural network,Benchmark data,Benchmarking,classifier,combustion,Constrained optimization,Constrained optimization methods,Data handling,Data regression,entropy,exhaust gas,Extreme learning machine (ELM),fuzzy system,generalized regression neural network,generalized regression neural network and fuzzy art,housing,Kernel function,kernel method,Knowledge acquisition,Lagrange multipliers,Learning systems,logistic regression analysis,machine learning,Malaysia,nitric oxide,Noisy data,Noisy data regression,Optimization,Optimization problems,priority journal,probabilitistic entropy based neural network,process optimization,radial based function,regression analysis,Regression analysis,support vector machine,Support vector regression (SVR)},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.974Z}
}

@inproceedings{zainuddinExtremeLearningMachine2019,
  title = {Extreme Learning Machine for Distinction of {{EEG}} Signal Pattern of Dyslexic Children in Writing},
  booktitle = {{{IEEE EMBS Conf}}. {{Biomed}}. {{Eng}}. {{Sci}}., {{IECBES}} - {{Proc}}.},
  author = {Zainuddin, A.Z.A. and Lee, K.Y. and Mansor, W. and Mahmoodin, Z.},
  date = {2019},
  pages = {652--656},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/IECBES.2018.8626700},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062785312&doi=10.1109%2fIECBES.2018.8626700&partnerID=40&md5=4de37157d592a49d0c7c88c20eee69d2},
  abstract = {Dyslexia is neurological disorder that affects the brain ability to process symbols such as letters and numbers. The process of writing involves learning pathway that can be monitored non-invasively using electroencephalogram (EEG). Majority EEG based studies on dyslexia have been on reading. Here, in this paper, an extreme learning machine (ELM) classifier with radial basis function (RBF) kernel is employed to distinguish between normal, poor and capable dyslexic subjects, from EEG signals of their writing. The RBF kernel allows its center and width randomly to be generated, such that the output weights of RBF networks can be calculated analytically instead of being iteratively tuned, resulting in faster learning speed. Power band coefficients of beta and beta/theta ratio are extracted using discrete wavelet transform (DWT) with Daubechies family order 2, 4, 6 and 8 to serve as inputs to the classifier. From the experimental results, it is found that Db2 yields the highest accuracy at 89\% and the best ROC performance for the three cohorts. © 2018 IEEE},
  isbn = {9781538624715 (ISBN)},
  langid = {english},
  keywords = {/unread,Biomedical engineering,Discrete wavelet transforms,Dyslexia,EEG,Electro-encephalogram (EEG),Electroencephalography,ELM,Extreme learning machine (ELM),Knowledge acquisition,Learning pathway,Learning speed,Learning systems,Neurological disorders,Radial basis function networks,Radial Basis Function(RBF),RBF,RBF kernels,Wavelet transform,Wavelet transforms},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.005Z}
}

@article{zhangAluminaConcentrationDetection2017,
  title = {Alumina Concentration Detection Based on the Kernel Extreme Learning Machine},
  author = {Zhang, S. and Zhang, T. and Yin, Y. and Xiao, W.},
  date = {2017},
  journaltitle = {Sensors (Switzerland)},
  shortjournal = {Sensors},
  volume = {17},
  number = {9},
  publisher = {{MDPI AG}},
  issn = {14248220 (ISSN)},
  doi = {10.3390/s17092002},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028689811&doi=10.3390%2fs17092002&partnerID=40&md5=baf386d55bde8b2bf9cda8b754b265e3},
  abstract = {The concentration of alumina in the electrolyte is of great significance during the production of aluminum. The amount of the alumina concentration may lead to unbalanced material distribution and low production efficiency and affect the stability of the aluminum reduction cell and current efficiency. The existing methods cannot meet the needs for online measurement because industrial aluminum electrolysis has the characteristics of high temperature, strong magnetic field, coupled parameters, and high nonlinearity. Currently, there are no sensors or equipment that can detect the alumina concentration on line. Most companies acquire the alumina concentration from the electrolyte samples which are analyzed through an X-ray fluorescence spectrometer. To solve the problem, the paper proposes a soft sensing model based on a kernel extreme learning machine algorithm that takes the kernel function into the extreme learning machine. K-fold cross validation is used to estimate the generalization error. The proposed soft sensing algorithm can detect alumina concentration by the electrical signals such as voltages and currents of the anode rods. The predicted results show that the proposed approach can give more accurate estimations of alumina concentration with faster learning speed compared with the other methods such as the basic ELM, BP, and SVM. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.},
  langid = {english},
  keywords = {Alumina,Alumina concentration,Aluminum,Aluminum electrolysis,Efficiency,Electrolysis,Electrolytes,Extreme learning machine (ELM),K fold cross validations,K-fold cross validation,Kernel extreme learning machine,Knowledge acquisition,Learning systems,Neural networks,Ore reduction,Predict,X ray spectrometers},
  annotation = {11 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T14:05:23.681Z},
  file = {/home/leix/Documents/Zotero/storage/3SDDK9NV/zhang2017.pdf.pdf}
}

@book{zhangGraphEmbeddedMultiple2020,
  title = {Graph Embedded Multiple Kernel Extreme Learning Machine for Music Emotion Classification},
  author = {Zhang, X. and Yang, Z. and Ren, J. and Wang, M. and Ling, W.-K.},
  editorb = {{Ren J.} and {Hussain A.} and {Zhao H.} and {Cai J.} and {Chen R.} and {Xiao Y.} and {Huang K.} and {Zheng J.}},
  editorbtype = {redactor},
  date = {2020},
  journaltitle = {Lect. Notes Comput. Sci.},
  volume = {11691 LNAI},
  pages = {191},
  publisher = {{Springer}},
  doi = {10.1007/978-3-030-39431-8_17},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080857184&doi=10.1007%2f978-3-030-39431-8_17&partnerID=40&md5=66aab8f721da2d6e4affb8288eb2abe8},
  abstract = {Music emotion classification is one of the most importance parts of music information retrieval (MIR) because of its potential commercial value and cultural value. However, music emotion classification is still a tough challenge, due to the low representation of music features. In this paper, a novel Extreme Learning Machine (ELM), combining graph regularization term and multiple kernel, is proposed to enhance the accuracy of music emotion classification. We use nonnegative matrix factorization (NMF) to find the optimal weights of combining multiple kernels. Furthermore, the graph regularization term is added to increase the relevance between predictions from the same class. The proposed Graph embedded Multiple Kernel Extreme Learning Machine (GMK-ELM) is tested on three music emotion datasets. Experiment results show that the proposed GMK-ELM outperforms several well-known ELM methods. © Springer Nature Switzerland AG 2020.},
  isbn = {03029743 (ISSN); 9783030394301 (ISBN)},
  langid = {english},
  pagetotal = {180},
  keywords = {/unread,Brain,Classification (of information),Cognitive systems,Extreme learning machine (ELM),Extreme Learning Machine (ELM),Factorization,Graph embedded,Knowledge acquisition,Machine learning,Matrix algebra,Multiple kernel learning,Multiple Kernel Learning,Multiple kernels,Music emotion classification,Music emotion classifications,Music information retrieval,Nonnegative matrix factorization,Regularization terms},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.014Z}
}

@article{zhangIntegratedMultipleKernel2021,
  title = {Integrated {{Multiple Kernel Learning}} for {{Device-Free Localization}} in {{Cluttered Environments Using Spatiotemporal Information}}},
  author = {Zhang, J. and Li, Y. and Xiao, W.},
  date = {2021},
  journaltitle = {IEEE Internet of Things Journal},
  shortjournal = {IEEE Internet Things J.},
  volume = {8},
  number = {6},
  pages = {4749--4761},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {23274662 (ISSN)},
  doi = {10.1109/JIOT.2020.3028574},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102336391&doi=10.1109%2fJIOT.2020.3028574&partnerID=40&md5=3d85524af729d8d8fa4132abf91f5e5c},
  abstract = {Ubiquitous WiFi signals not only provide fundamental communications for a large number of Internet of Things devices, but also enable to estimate target's location in a contactless manner. However, most of the existing device-free localization (DFL) methods only utilize the time dynamics of the received WiFi signals, leading to inaccurate DFL in the cluttered indoor environments. Because different layouts of environments and deployments of WiFi devices cause the different mathematical distributions of the data collected from the cluttered indoor environments. In this article, a multiple kernel representation-based extreme learning machine (ELM) is proposed, named integrated multiple kernel ELM (IMK-ELM), for strengthening the localization performance in the cluttered indoor environments utilizing spatiotemporal information. In the proposed IMK-ELM-based DFL, the whole data set is first divided into several subsets depending on their mathematical distributions through the K-means clustering algorithm, and then a corresponding number of local DFL models are built for all the subsets to capture both the time dynamics and spatial properties of the data. Finally, a global DFL model is achieved by seamlessly integrating all the local DFL models due to the consistency mechanism. In addition, the Fresnel zone sensing theory is utilized for helping understand and explain the essence of indoor DFL. Comprehensive experiments indicate that the proposed IMK-ELM-based DFL outperforms state-of-The-Art methods in the cluttered indoor environments. © 2014 IEEE.},
  langid = {english},
  keywords = {/unread,Cluttered environments,Device-free localization (DFL),Device-free localizations,extreme learning machine (ELM),Extreme learning machine (ELM),Indoor positioning systems,K-means clustering,Learning systems,Localization performance,Multiple Kernel Learning,multiple kernel representation,Spatial properties,Spatiotemporal information,State-of-the-art methods,Wireless local area networks (WLAN)},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.022Z}
}

@article{zhangMultipleHiddenLayer2014,
  title = {Multiple hidden layer output matrices extreme learning machine},
  author = {Zhang, W.-B. and Ji, H.-B. and Wang, L. and Zhu, M.-Z.},
  date = {2014},
  journaltitle = {Xi Tong Gong Cheng Yu Dian Zi Ji Shu/Systems Engineering and Electronics},
  shortjournal = {Xi Tong Cheng Yu Dian Zi Ji Shu/Syst Eng Electron},
  volume = {36},
  number = {8},
  pages = {1656--1659},
  publisher = {{Chinese Institute of Electronics}},
  issn = {1001506X (ISSN)},
  doi = {10.3969/j.issn.1001-506X.2014.08.33},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907102521&doi=10.3969%2fj.issn.1001-506X.2014.08.33&partnerID=40&md5=f403751f23a9218e183087cdad15a93e},
  abstract = {The extreme learning machine (ELM) achieves good performance for classification and runs at a fast learning speed because of choosing the learning parameters of hidden nodes randomly. However, when the parameters of the hidden nodes are absolutely randomly chosen, the performance of ELM is not always optimal. The multiple hidden layer output matrices extreme learning machine (M-ELM) is proposed which optimizes the architecture of hidden nodes by weighted calculation of different output matrices, and the matrices weights and the output weights are analytically determined simultaneously. In addition, the feature level fusion of ELM can be achieved by this method. For the real word classification problems, simulation experiments verify that M-ELM can provide a better performance than ELM.},
  langid = {chinese},
  keywords = {/unread,Extreme learning machine (ELM),Fast learning,Feature level fusion,Hidden layers,Knowledge acquisition,Learning parameters,Machine learning,Multiple kernel learning,Multiple Kernel Learning,Shuffled frog leaping algorithm (SFLA),Word classification},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.955Z}
}

@article{zhangNonlinearSystemOnline2017,
  title = {Nonlinear system online identification based on kernel sparse learning algorithm with adaptive regulation factor},
  author = {Zhang, W. and Xu, A. and Ping, D.},
  date = {2017},
  journaltitle = {Xi Tong Gong Cheng Yu Dian Zi Ji Shu/Systems Engineering and Electronics},
  shortjournal = {Xi Tong Cheng Yu Dian Zi Ji Shu/Syst Eng Electron},
  volume = {39},
  number = {1},
  pages = {223--230},
  publisher = {{Chinese Institute of Electronics}},
  issn = {1001506X (ISSN)},
  doi = {10.3969/j.issn.1001-506X.2017.01.33},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007583955&doi=10.3969%2fj.issn.1001-506X.2017.01.33&partnerID=40&md5=0f4160b081ed26dc39c832d1f8d093f9},
  abstract = {In order to curb the continuous growing of model network size and effectively adapt to real-time system variation, an online sparse kernel extreme learning machine (KELM) with adaptive regulation factor is proposed to model time-varying nonlinear systems. Construction of a new objective function makes the model have different structural risks in different nonlinear regions and ensures the regulation factor vary over time with the time-varying nonlinear dynamics. A three-step solving method is used to determine the sparse dictionary and current optimal regulation factor. The proposed method has the capability of online updating both the kernel weight coefficient and the regulation factor vector. The effectiveness of the proposed method is demonstrated through applying it to the modeling of a practical case. Comparisons between the proposed method and existing KELM-based modeling methods indicate that the proposed method can effectively improve modeling accuracy and has better stability. © 2017, Editorial Office of Systems Engineering and Electronics. All right reserved.},
  langid = {chinese},
  keywords = {/unread,Adaptive regulation,Continuous time systems,E-learning,Extreme learning machine (ELM),Interactive computer systems,Kernel method,Kernel methods,Knowledge acquisition,Learning algorithms,Learning systems,Neural networks,Nonlinear system identification,Nonlinear systems,Objective functions,Online sparsification,Online systems,Real time systems,Regularization,Sparse dictionaries,Sparsification,Time varying nonlinear dynamics},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.982Z}
}

@article{zhangOnlineConditionPrediction2017,
  title = {An Online Condition Prediction Algorithm Based on Cumulative Coherence Measurement},
  author = {Zhang, W. and Xu, A. and Gao, M.},
  date = {2017},
  journaltitle = {Shanghai Jiaotong Daxue Xuebao/Journal of Shanghai Jiaotong University},
  shortjournal = {Shanghai Jiaotong Daxue Xuebao},
  volume = {51},
  number = {11},
  pages = {1391--1398},
  publisher = {{Shanghai Jiao Tong University}},
  issn = {10062467 (ISSN)},
  doi = {10.16183/j.cnki.jsjtu.2017.11.016},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044422298&doi=10.16183%2fj.cnki.jsjtu.2017.11.016&partnerID=40&md5=30feaaff1200544b721bbcaa2a1a2df0},
  abstract = {It is difficult for extreme learning machine with kernel (KELM) to curb kernel matrix expansion and track the system dynamic changes effectively when it is applied to solve online learning tasks. So the sliding time window method is regarded as the basic modeling strategy, and a new online sparsification learning algorithm for KELM is proposed in this paper. In the process of forward sparsification and backward sparsification, a sparse dictionary with predefined size can be selected by online minimization of its cumulative coherence based on our proposed constructive and pruning strategy. In the process of incremental learning and decremental learning, the model parameters can be directly updated by elementary transformation of matrices and block matrix inversion formula based on the selected dictionary. The performance of the proposed algorithm is compared with several well-known online sequential ELM algorithms. The simulation results show that the proposed algorithm can achieve higher prediction accuracy and better stability, meanwhile, it costs the similar testing time. © 2017, Shanghai Jiao Tong University Press. All right reserved.},
  langid = {chinese},
  keywords = {/unread,Coherence measurements,Condition prediction,Cumulative coherence,E-learning,Elementary transformation,Extreme learning machine (ELM),Forecasting,Kernel method,Kernel methods,Knowledge acquisition,Learning algorithms,Learning systems,Linear transformations,Online sequential,Online sequential elms,Online sequential learning,Sliding time windows},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.981Z}
}

@article{zhangOnlineConditionPrediction2017a,
  title = {Online condition prediction of avionic devices based on sparse kernel incremental extreme learning machine},
  author = {Zhang, W. and Xu, A. and Gao, M.},
  date = {2017},
  journaltitle = {Beijing Hangkong Hangtian Daxue Xuebao/Journal of Beijing University of Aeronautics and Astronautics},
  shortjournal = {Beijing Hangkong Hangtian Daxue Xuebao},
  volume = {43},
  number = {10},
  pages = {2089--2098},
  publisher = {{Beijing University of Aeronautics and Astronautics (BUAA)}},
  issn = {10015965 (ISSN)},
  doi = {10.13700/j.bh.1001-5965.2016.0802},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033571893&doi=10.13700%2fj.bh.1001-5965.2016.0802&partnerID=40&md5=a5796c9963e2d27af136ffd01ec44133},
  abstract = {In order to achieve the online condition prediction for avionic devices, a sparse kernel incremental extreme learning machine (ELM) algorithm is presented. For the problem of Gram matrix expansion in kernel online learning algorithms, a novel sparsification rule is presented by measuring the instantaneous learnable information contained on a data sample for dictionary selection. The proposed sparsification method combines the constructive strategy and the pruning strategy in two stages. By minimizing the redundancy of dictionary in the constructive phase and maximizing the instantaneous conditional self-information of dictionary atoms in the pruning phase, a compact dictionary with predefined size can be selected adaptively. For the kernel weight updating of kernel based incremental ELM, an improved decremental learning algorithm is proposed by using matrix elementary transformation and block matrix inversion formula, which effectively moderate the computational complexity at each iteration. In proposed algorithm, the inverse matrix of Gram matrix of the other samples can be directly updated after one sample is deleted from previous dictionary. The experimental results of the aero-engine condition prediction show that the proposed method can make the whole average error rate reduce to 2.18\% when the prediction step is equal to 20. Compared with three well-known kernel ELM online learning algorithms, the prediction accuracy is improved by 0.72\%, 0.14\% and 0.13\% respectively. © 2017, Editorial Board of JBUAA. All right reserved.},
  langid = {chinese},
  keywords = {/unread,Active set,Active sets,Aircraft engines,Avionics,Condition prediction,E-learning,Extreme learning machine (ELM),Extreme learning machine(ELM),Forecasting,Inverse problems,Iterative methods,Kernel online learning,Knowledge acquisition,Learning algorithms,Learning systems,Linear transformations,Matrix algebra,Online learning,Sparsity measure,Sparsity measures},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.981Z}
}

@article{zhangOnlineModelingKernel2014,
  title = {Online modeling of kernel extreme learning machine based on fast leave-one-out cross-validation},
  author = {Zhang, Y.-T. and Ma, C. and Li, Z.-N. and Fan, H.-B.},
  date = {2014},
  journaltitle = {Shanghai Jiaotong Daxue Xuebao/Journal of Shanghai Jiaotong University},
  shortjournal = {Shanghai Jiaotong Daxue Xuebao},
  volume = {48},
  number = {5},
  pages = {641--646},
  publisher = {{Shanghai Jiao Tong University}},
  issn = {10062467 (ISSN)},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902472583&partnerID=40&md5=d4dd03cbe3a5559fab8ee810cb0b0843},
  abstract = {A novel algorithm based on fast leave-one-out cross-validation was proposed, named as online kernel extreme learning machine (OKELM). Online modeling was accomplished by importing the latest training sample and discarding the oldest training sample. An adaptive FLOO-CV prediction error-based threshold without any manual work was used to enhance the sparsity and generalization ability of the model by only introducing the samples with larger predictive error. The output weights of the OKELM were determined recursively based on Hermitian formula. Thus, the online storage space and calculation time was reduced. Numerical experiments on chaotic time series prediction and identification of a continuous stirred tank reactor show that the OKELM has faster calculation speed and higher learning accuracy in comparison with off-line kernel extreme learning machine, unsparsity online kernel extreme learning machine and on-line sequential extreme learning machine.},
  langid = {chinese},
  keywords = {Algorithms,Calculation speed,Chaotic time series prediction,Continuous stirred tank reactor,Extreme learning machine (ELM),Fast leave-one-out,Fast leave-one-out cross-validation (FLOO-CV),Generalization ability,Kernel method,Kernel methods,Knowledge acquisition,Learning systems,Neural networks,Numerical experiments,Radial basis function networks,Sampling},
  annotation = {Read\_Status: Not Reading Read\_Status\_Date: 2023-02-26T20:25:27.916Z}
}

@article{zhangPtSNEMMEMPMBased2019,
  title = {A {{P-t-SNE}} and {{MMEMPM}} Based Quality-Related Process Monitoring Method for a Variety of Hot Rolling Processes},
  author = {Zhang, C. and Peng, K. and Dong, J.},
  date = {2019},
  journaltitle = {Control Engineering Practice},
  shortjournal = {Control Eng. Pract.},
  volume = {89},
  pages = {1--11},
  publisher = {{Elsevier Ltd}},
  issn = {09670661 (ISSN)},
  doi = {10.1016/j.conengprac.2019.05.006},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065834170&doi=10.1016%2fj.conengprac.2019.05.006&partnerID=40&md5=f5756ed78e8c6487ee6425bb4a17e733},
  abstract = {A quality-related process monitoring method based on parametric t-distributed stochastic neighbour embedding (P-t-SNE) and modified minimum error minimax probability machine (MMEMPM) is proposed for a variety of hot rolling processes. This work pays close attention to the connections between different types of strip steel products instead of conventional multimode methods, which could be useful in process modelling and monitoring. First, a parametric version of t-SNE is developed for streaming data in the hot rolling process. Then, a new space separation method based on parametric t-SNE and quality variables is implemented to extract internal shared information and external unique information among different types for quality-related fault detection. After that, a variety identification method is proposed to identify the online data in the hot rolling process. Finally, the performance of proposed quality-related process monitoring method is examined through a real hot rolling process. The efficiency and feasibility are demonstrated. © 2019 Elsevier Ltd},
  langid = {english},
  keywords = {A variety of hot rolling processes,Fault detection,Hot rolling,Hot rolling process,Minimax probability machine,MMEMPM,Monitoring,P-t-SNE,Process control,Process monitoring,Quality-related fault detection,Separation methods,Shared information,Stochastic systems,Streaming data,Strip steel products,Variety identification},
  annotation = {15 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T14:42:46.593Z},
  file = {/home/leix/Documents/Zotero/storage/UY23RFTA/10.1016@j.conengprac.2019.05.006.pdf.pdf}
}

@article{zhangRealtimeAutomaticMethod2020,
  title = {A Real-Time Automatic Method for Target Locating under Unknown Wall Characteristics in through-Wall Imaging},
  author = {Zhang, H.-M. and Zhou, S. and Xu, C. and Zhang, Y.-R.},
  date = {2020},
  journaltitle = {Progress In Electromagnetics Research M},
  shortjournal = {Prog. Electromagn. Res. M},
  volume = {89},
  pages = {189--197},
  publisher = {{Electromagnetics Academy}},
  issn = {19378726 (ISSN)},
  doi = {10.2528/PIERM19111101},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092167401&doi=10.2528%2fPIERM19111101&partnerID=40&md5=7fbc508a6d48da866ba8af2ef41d7f7b},
  abstract = {—To solve the real-time through-wall detection problem in the presence of wall ambiguities, an approach based on kernel extreme learning machine (KELM) is proposed in this paper. The wall ambiguity and propagation effect are included in single-hidden-layer feedforward networks, and then the technique converts the through-wall problem into a regression problem. The relationship between the scattered data and the target properties is determined after the KELM training process. Numerical results demonstrate the good performance in terms of the effectiveness, generalization, and robustness. Compared with the support vector machine (SVM) and least-squares support vector machine (LS-SVM), the KELM provides almost the same estimated accuracy but at a much faster learning speed, which greatly contributes to solving the real-time detection problem. In addition, the situations of two targets, different target radii, and noisy circumstances are discussed. © 2020, Electromagnetics Academy. All rights reserved.},
  langid = {english},
  keywords = {Detection problems,Extreme learning machine (ELM),Feed-forward network,Feedforward neural networks,Learning systems,Least squares support vector machines,Network layers,Propagation effect,Real-time detection,Support vector machines,Through-wall imaging,Through-wall problems},
  annotation = {1 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T17:54:35.804Z},
  file = {/home/leix/Documents/Zotero/storage/YZRQSZ33/zhang2020.pdf.pdf}
}

@article{zhangTargetTrackingAlgorithm2017,
  title = {Target tracking algorithm based on extreme learning machine and multiple kernel boosting learning},
  author = {Zhang, D. and Sun, R. and Gao, J.},
  date = {2017},
  journaltitle = {Xi Tong Gong Cheng Yu Dian Zi Ji Shu/Systems Engineering and Electronics},
  shortjournal = {Xi Tong Cheng Yu Dian Zi Ji Shu/Syst Eng Electron},
  volume = {39},
  number = {9},
  pages = {2149--2156},
  publisher = {{Chinese Institute of Electronics}},
  issn = {1001506X (ISSN)},
  doi = {10.3969/j.issn.1001-506X.2017.09.33},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034429165&doi=10.3969%2fj.issn.1001-506X.2017.09.33&partnerID=40&md5=2abc24286b501cba8926506be7a659f7},
  abstract = {How to construct a robust classifier is always a hot research spot in target tracking based on discriminant. In recent years, multiple kernel learning combining multiple classifier to achieve better classification performance has attract wide attention. Traditional multiple kernel learning cannot be directly used in target tracking because it has a very complicated optimal question. A multiple kernel learning is proposed which is based on boosting framework. It assures target tracking can keep efficient and accurate in complicated scenes. In order to decrease the computation and increase the classify performance, extreme learning machine (ELM) is used as the base classifier. ELM has a very simple structure and rapid training speed. Compared to support vector machine, ELM has a better generalization ability. Finally, the proposed algorithm is compared with other state-of-art tracking algorithms in some challenge videos to verify the effectiveness of the proposed algorithm. © 2017, Editorial Office of Systems Engineering and Electronics. All right reserved.},
  langid = {chinese},
  keywords = {/unread,Boosting learning,Classification performance,Clutter (information theory),Extreme learning machine (ELM),Generalization ability,Knowledge acquisition,Learning systems,Multiple classifiers,Multiple kernel learning,Multiple Kernel Learning,Support vector machines,Target tracking,Target tracking algorithm,Tracking algorithm},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.980Z}
}

@article{zhangWatermelonRipenessDetection2019,
  title = {Watermelon {{Ripeness Detection}} via {{Extreme Learning Machine}} with {{Kernel Principal Component Analysis Based}} on {{Acoustic Signals}}},
  author = {Zhang, Y. and Deng, X. and Xu, Z. and Yuan, P.},
  date = {2019},
  journaltitle = {International Journal of Pattern Recognition and Artificial Intelligence},
  shortjournal = {Int J Pattern Recognit Artif Intell},
  volume = {33},
  number = {8},
  publisher = {{World Scientific Publishing Co. Pte Ltd}},
  issn = {02180014 (ISSN)},
  doi = {10.1142/S0218001419510029},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059549767&doi=10.1142%2fS0218001419510029&partnerID=40&md5=02185cd44a526e96f6fbb4084a1abf44},
  abstract = {Many investigations have proved that the acoustics method is intuitive and effective for determining watermelon ripeness. The objective of this work is to drive a new robust acoustics classification scheme KPCA-ELM, which is based on the kernel principal component analysis (KPCA) and extreme learning machine (ELM). Acoustic signals are sampled by a microphone from unripe, ripe and over-ripe watermelon samples, which are randomly divided into two sample sets for training and testing. A set of basic signals is first obtained via KPCA of the training sample. Thus, any given signal can be represented as a linear combination of basis signals, and the coefficients of linear combination are extracted as the features of a signal. Corresponding to the unripe, ripe and over-ripe watermelons, a three-class ELM identification model is constructed based on the training data. The scheme presented in this paper is tested with the testing sample and an accuracy of 92\% is achieved. To further evaluate the scheme performance, a comparison of ELM and SVM is conducted in terms of the classification results. The results reveal that the proposed scheme can classify faster than SVM, while ELM is better than SVM in accuracy. © 2019 World Scientific Publishing Company.},
  langid = {english},
  keywords = {/unread,Acoustic waves,Classification results,Classification scheme,Digital storage,extreme learning machine (ELM),Extreme learning machine (ELM),feature extraction,Feature extraction,Identification model,Kernel principal component analyses (KPCA),kernel principal component analysis (KPCA),Knowledge acquisition,Learning systems,Linear combinations,Principal component analysis,Ripeness,Training and testing},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.006Z}
}

@article{zhangWaveletTransformKernelbased2018,
  title = {Wavelet Transform and {{Kernel-based}} Extreme Learning Machine for Electricity Price Forecasting},
  author = {Zhang, Y. and Li, C. and Li, L.},
  date = {2018},
  journaltitle = {Energy Systems},
  shortjournal = {Energy Syst.},
  volume = {9},
  number = {1},
  pages = {113--134},
  publisher = {{Springer Verlag}},
  issn = {18683967 (ISSN)},
  doi = {10.1007/s12667-016-0227-3},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043259647&doi=10.1007%2fs12667-016-0227-3&partnerID=40&md5=0ec294c3dc14c07dc46a614cb9ad1890},
  abstract = {In deregulated electricity markets, sophisticated factors, such as the weather, the season, high frequencies, the presence of jumps and the relationship between electricity loads and prices, make electricity prices difficult to predict. To increase the accuracy of electricity price forecasting, this paper investigates a hybrid approach that is based on a combination of the wavelet transform, a kernel-based extreme learning machine and a particle swarm optimization algorithm. The performance and robustness of the proposed method are evaluated by using electricity price data from two Australian districts (New South Wales and Victoria) and Pennsylvania-New Jersey-Maryland (PJM) electricity markets. These case studies show that the proposed method can effectively capture the nonlinearity features from the price data series with a smaller computation time cost and high prediction accuracy compared with other price forecasting methods. The results also demonstrate that the proposed method represents an accurate price forecasting technique for power market price analysis. © 2016, Springer-Verlag Berlin Heidelberg.},
  langid = {english},
  keywords = {/unread,Commerce,Costs,Deregulated electricity market,Deregulation,Electricity price forecasting,Electricity prices,Extreme learning machine (ELM),Forecasting,High frequency HF,Kernel-based ELM,Knowledge acquisition,Learning systems,Optimization,Particle swarm optimization (PSO),Power markets,Prediction accuracy,Price forecasting,PSO,Wavelet transform,Wavelet transforms},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.997Z}
}

@article{zhaoKernelizedExtremeLearning2015,
  title = {Kernelized extreme learning machine in distributed environment},
  author = {Zhao, X.-G. and Bi, X. and Zhang, Z. and Yang, H.-B.},
  date = {2015},
  journaltitle = {Dongbei Daxue Xuebao/Journal of Northeastern University},
  shortjournal = {Dongbei Daxue Xuebao},
  volume = {36},
  number = {6},
  pages = {769--772},
  publisher = {{Northeast University}},
  issn = {10053026 (ISSN)},
  doi = {10.3969/j.issn.1005-3026.2015.06.003},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937555254&doi=10.3969%2fj.issn.1005-3026.2015.06.003&partnerID=40&md5=c121f215786b30726dcc940da02754f5},
  abstract = {With the exponentially increasing volume of training data, the performance of centralized ELM with kernels suffers due to large matrix operations. A distributed algorithm named MapReduce based kernelized ELM (MR-KELM) was proposed, which realized an implementation of ELM with kernels on MapReduce in the cloud. The kernel matrix generated by distributed radial basis function was decomposed and then the output weights by distributed multiplication of matrix and vector were calculated by the proposed algorithm. Communications and data exchanges in distributed matrix operations were reduced and good scalability was achieved by MR-KELM. Extensive experiments on synthetic datasets were conducted to verify the training performance and scalability of MR-KELM. Experimental results showed that MR-KELM was effective and efficient for massive learning applications. ©, 2015, Northeastern University. All right reserved.},
  langid = {chinese},
  keywords = {/unread,Classification,Classification (of information),Distributed,Distributed environments,Electronic data interchange,ELM with kernels,Extreme learning machine (ELM),Knowledge acquisition,Machine learning,Map-reduce,MapReduce,Matrix algebra,Performance and scalabilities,Radial basis functions,Scalability},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.965Z}
}

@article{zhaoSelectingOptimalHidden2018,
  title = {Selecting the Optimal Hidden Layer of Extreme Learning Machine Using Multiple Kernel Learning},
  author = {Zhao, W. and Liu, Q. and Liu, D. and Liu, X. and Li, P.},
  date = {2018},
  journaltitle = {KSII Transactions on Internet and Information Systems},
  shortjournal = {KSII Trans. Internet Inf. Syst.},
  volume = {12},
  number = {12},
  pages = {5765--5781},
  publisher = {{Korean Society for Internet Information}},
  issn = {19767277 (ISSN)},
  doi = {10.3837/tiis.2018.12.009},
  abstract = {Extreme learning machine (ELM) is emerging as a powerful machine learning method in a variety of application scenarios due to its promising advantages of high accuracy, fast learning speed and easy of implementation. However, how to select the optimal hidden layer of ELM is still an open question in the ELM community. Basically, the number of hidden layer nodes is a sensitive hyperparameter that significantly affects the performance of ELM. To address this challenging problem, we propose to adopt multiple kernel learning (MKL) to design a multi-hidden-layer-kernel ELM (MHLK-ELM). Specifically, we first integrate kernel functions with random feature mapping of ELM to design a hidden-layer-kernel ELM (HLK-ELM), which serves as the base of MHLK-ELM. Then, we utilize the MKL method to propose two versions of MHLK-ELMs, called sparse and non-sparse MHLK-ELMs. Both two types of MHLK-ELMs can effectively find out the optimal linear combination of multiple HLK-ELMs for different classification and regression problems. Experimental results on seven data sets, among which three data sets are relevant to classification and four ones are relevant to regression, demonstrate that the proposed MHLK-ELM achieves superior performance compared with conventional ELM and basic HLK-ELM. © 2018 KSII.},
  langid = {english},
  keywords = {Application scenario,Classification (of information),Extreme learning machine (ELM),Hidden layer kernel,Hidden layer nodes,Hidden layers,Knowledge acquisition,Learning systems,Linear combinations,Machine learning methods,Multiple kernel learning,Multiple Kernel Learning,Optimization,Regression problem},
  annotation = {0 citations (Crossref) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T14:37:09.747Z},
  file = {/home/leix/Documents/Zotero/storage/4ELSSCMI/Zhao et al. - 2018 - Selecting the optimal hidden layer of extreme lear.pdf}
}

@article{zhengMixtureCorrentropyBasedKernel2022,
  title = {Mixture {{Correntropy-Based Kernel Extreme Learning Machines}}},
  author = {Zheng, Y. and Chen, B. and Wang, S. and Wang, W. and Qin, W.},
  date = {2022},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  shortjournal = {IEEE Trans. Neural Networks Learn. Sys.},
  volume = {33},
  number = {2},
  pages = {811--825},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {2162237X (ISSN)},
  doi = {10.1109/TNNLS.2020.3029198},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124053625&doi=10.1109%2fTNNLS.2020.3029198&partnerID=40&md5=3eab996931fd84f92745c3b31746c131},
  abstract = {Kernel-based extreme learning machine (KELM), as a natural extension of ELM to kernel learning, has achieved outstanding performance in addressing various regression and classification problems. Compared with the basic ELM, KELM has a better generalization ability owing to no needs of the number of hidden nodes given beforehand and random projection mechanism. Since KELM is derived under the minimum mean square error (MMSE) criterion for the Gaussian assumption of noise, its performance may deteriorate under the non-Gaussian cases, seriously. To improve the robustness of KELM, this article proposes a mixture correntropy-based KELM (MC-KELM), which adopts the recently proposed maximum mixture correntropy criterion as the optimization criterion, instead of using the MMSE criterion. In addition, an online sequential version of MC-KELM (MCOS-KELM) is developed to deal with the case that the data arrive sequentially (one-by-one or chunk-by-chunk). Experimental results on regression and classification data sets are reported to validate the performance superiorities of the new methods. © 2012 IEEE.},
  langid = {english},
  keywords = {/unread,article,Classification (of information),Correntropy,E-learning,Extreme learning machine (ELM),Generalization ability,Kernel learning,kernel method,Kernel-methods,Knowledge acquisition,Learning algorithms,machine learning,Machine learning,Mean square error,Minimum mean square error criterion,mixture correntropy,Mixture correntropy,Mixtures,Natural extension,online learning,Online learning,Performance},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.026Z}
}

@inproceedings{zhouCreditRiskEvaluation2012,
  title = {Credit Risk Evaluation with Extreme Learning Machine},
  booktitle = {Conf. {{Proc}}. {{IEEE Int}}. {{Conf}}. {{Syst}}. {{Man Cybern}}.},
  author = {Zhou, H. and Lan, Y. and Soh, Y.C. and Huang, G.-B. and Zhang, R.},
  date = {2012},
  pages = {1064--1069},
  doi = {10.1109/ICSMC.2012.6377871},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872397124&doi=10.1109%2fICSMC.2012.6377871&partnerID=40&md5=5e60496d3f44672df9c92d5d69c5d4a5},
  abstract = {Credit risk evaluation has become an increasingly important field in financial risk management for financial institutions, especially for banks and credit card companies. Many data mining and statistical methods have been applied to this field. Extreme learning machine (ELM) classifier as a type of generalized single hidden layer feed-forward networks has been used in many applications and achieve good classification accuracy. Thus, we use ELM (kernel based) as a classification tool to perform the credit risk evaluation in this paper. The simulations are done on two credit risk evaluation datasets with three different kernel functions. Simulation results show that the kernel based ELM is more suitable for credit risk evaluation than the popular used Support Vector Machines (SVMs) with consideration of overall, good and bad accuracies. © 2012 IEEE.},
  isbn = {1062922X (ISSN); 9781467317146 (ISBN)},
  langid = {english},
  keywords = {/important,Classification accuracy,Classification tool,Confusion matrices,Confusion Matrix,Credit cards,Credit risk evaluation,Credit Risk Evaluation,Cybernetics,Data sets,Extreme learning machine (ELM),Feed-forward network,Financial institution,Financial risk management,Hidden layers,Kernel function,Knowledge acquisition,Risk assessment,Risk management,Support Vector Machine (SVM),Support vector machines},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-02-27T18:50:42.329Z},
  file = {/home/leix/Documents/Zotero_Papers/TFM/elm kernel scopus/Zhou et al_2012_Credit risk evaluation with extreme learning machine.pdf}
}

@article{zhouExtremeLearningMachine2015,
  title = {Extreme {{Learning Machine}} with {{Composite Kernels}} for {{Hyperspectral Image Classification}}},
  author = {Zhou, Y. and Peng, J. and Chen, C.L.P.},
  date = {2015},
  journaltitle = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  shortjournal = {IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.},
  volume = {8},
  number = {6},
  pages = {2351--2360},
  publisher = {{Institute of Electrical and Electronics Engineers}},
  issn = {19391404 (ISSN)},
  doi = {10.1109/JSTARS.2014.2359965},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027956117&doi=10.1109%2fJSTARS.2014.2359965&partnerID=40&md5=f287ba590be2684fd8392c6a5aaf56a4},
  abstract = {Due to its simple, fast, and good generalization ability, extreme learning machine (ELM) has recently drawn increasing attention in the pattern recognition and machine learning fields. To investigate the performance of ELM on the hyperspectral images (HSIs), this paper proposes two spatial-spectral composite kernel (CK) ELM classification methods. In the proposed CK framework, the single spatial or spectral kernel consists of activation-function-based kernel and general Gaussian kernel, respectively. The proposed methods inherit the advantages of ELM and have an analytic solution to directly implement the multiclass classification. Experimental results on three benchmark hyperspectral datasets demonstrate that the proposed ELM with CK methods outperform the general ELM, SVM, and SVM with CK methods. © 2014 IEEE.},
  langid = {english},
  keywords = {/unread,Activation functions,algorithm,Analytic solution,Classification methods,Composite kernel (CK),Composite kernels,data set,extreme learning machine (ELM),Extreme learning machine (ELM),Gaussian kernels,Generalization ability,hyperspectral image (HSI) classification,image classification,Image classification,Independent component analysis,Knowledge acquisition,Learning systems,Multi-class classification,multispectral image,pattern recognition,Spectroscopy},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.966Z}
}

@article{zhouNovelCompoundFaulttolerant2023,
  title = {A Novel Compound Fault-Tolerant Method Based on Online Sequential Extreme Learning Machine with Cycle Reservoir for Turbofan Engine Direct Thrust Control},
  author = {Zhou, X. and Huang, J. and Lu, F. and Zhou, W. and Liu, P.},
  date = {2023},
  journaltitle = {Aerospace Science and Technology},
  shortjournal = {Aerosp Sci Technol},
  volume = {132},
  publisher = {{Elsevier Masson s.r.l.}},
  issn = {12709638 (ISSN)},
  doi = {10.1016/j.ast.2022.108059},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144344103&doi=10.1016%2fj.ast.2022.108059&partnerID=40&md5=a015732a925cb2ad270fbd98c59a4933},
  abstract = {Sensors are the primary information source of the aeroengine control system, their measurement accuracy is closely related to whether the engine can operate safely and efficiently. Aiming at the direct thrust control system of aeroengines, this paper proposes an intelligent prediction algorithm combining feedforward and recurrent networks and a fault-tolerant control strategy combining analytical redundancy and controller switching. First, the cycle reservoir with regular jumps is introduced into the online sequential extreme learning machine. The CR-OSELM is developed, which maps the inputs from the low-dimensional space to the high-dimensional space. Then the CR-OSELM is adopted to build the sensor measurement prediction module, and identify and reconstruct the sensor bias and drift fault. Furthermore, in allusion to the rotor speed sensor and temperature/pressure sensor faults, analytic redundancy and controller switching strategies are designed respectively, realizing the fault-tolerant control of thrust feedback. The main contribution of this paper is to come up with the CR-OSELM algorithm with the ability to save historical input information, which overcomes the limitations of traditional feedforward neural networks in identifying time series. And the compound fault-tolerant control scheme is put forward based on the fault characteristics and their impact on the direct thrust control system, which can deal with different types of sensor faults. Simulations are performed on some benchmark datasets and a turbofan engine model to investigate the time series prediction accuracy, sensor fault diagnosis, and thrust fault-tolerant control effect. The results show that the system can detect various faults rapidly, realize the analytic redundancy with high accuracy, reduce the influence of sensor faults and restore the engine performance. © 2022 Elsevier Masson SAS},
  langid = {english},
  keywords = {Aero-engine,Aeroengine,Aircraft engines,Analytical redundancy,Compound faults,Control systems,Controllers,Direct thrust control,Fault detection,Faults tolerant controls,Feedforward neural networks,Forecasting,Intelligent analytical redundancy,Knowledge acquisition,Learning systems,Machine learning,Online sequential extreme learning machine,Redundancy,Reservoir computing,Reservoir Computing,Sensor fault-tolerant control,Sensors faults,Time series,Turbofan engines},
  annotation = {0 citations (Crossref) [2023-02-21] 0 citations (Semantic Scholar/DOI) [2023-02-21] Read\_Status: Read Read\_Status\_Date: 2023-02-27T18:14:18.620Z},
  file = {/home/leix/Documents/Zotero/storage/N5H95QZM/Zhou et al. - 2023 - A novel compound fault-tolerant method based on on.pdf}
}

@article{zhouSemisupervisedExtremeLearning2015,
  title = {Semi-Supervised Extreme Learning Machine with Manifold and Pairwise Constraints Regularization},
  author = {Zhou, Y. and Liu, B. and Xia, S. and Liu, B.},
  date = {2015},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {149},
  pages = {180--186},
  publisher = {{Elsevier B.V.}},
  issn = {09252312 (ISSN)},
  doi = {10.1016/j.neucom.2014.01.073},
  abstract = {Traditional kernel-based semi-supervised learning (SSL) algorithms usually have high computational complexity. Moreover, few SSL methods have been proposed to utilize both the manifold of unlabeled data and pairwise constraints effectively. In this paper, we first construct a unified SSL framework to combine the manifold regularization and the terms based on the pairwise constraints for semi-supervised classification tasks. Motivated by the effectiveness of extreme learning machine (ELM), we further utilize ELM to approximate the established kernel-based SSL framework. Finally, we present a fast semi-supervised extreme learning machine with manifold regularization and pairwise constraints. Experimental results on a variety of real-world data sets demonstrate the effectiveness of the proposed fast SSL algorithm. © 2014 Elsevier B.V.},
  issue = {Part A},
  langid = {english},
  keywords = {/unread,Article,classification algorithm,controlled study,data processing,Extreme learning machine (ELM),kernel method,Knowledge acquisition,machine learning,Manifold regularizations,mathematical parameters,pairwise constraint regularization,Pairwise constraints,Regularization,semi supervised learning,Semi- supervised learning,Semi-supervised,Semi-supervised classification,Semi-supervised learning,Semi-supervised learning (SSL),statistical analysis,Supervised learning},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:35.965Z}
}

@article{zhuFaultDetectionMethod2020,
  title = {Fault detection method for avionics based on LMKL and OC-ELM},
  shorttitle = {基于LMKL和OC-ELM的航空电子部件故障检测方法},
  author = {Zhu, M. and Liu, Q. and Liu, X. and Xu, Q.},
  date = {2020},
  journaltitle = {Xi Tong Gong Cheng Yu Dian Zi Ji Shu/Systems Engineering and Electronics},
  shortjournal = {Xi Tong Cheng Yu Dian Zi Ji Shu/Syst Eng Electron},
  volume = {42},
  number = {6},
  pages = {1424--1432},
  publisher = {{Chinese Institute of Electronics}},
  issn = {1001506X (ISSN)},
  doi = {10.3969/j.issn.1001-506X.2020.06.29},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087488706&doi=10.3969%2fj.issn.1001-506X.2020.06.29&partnerID=40&md5=31512524390a2cc35294c11822b36c04},
  abstract = {In consideration of the difficulty in acquiring real fault samples and the low detection accuracy of the avionics, a fault detection method based on localized multiple kernel learning (LMKL) and one-class extreme learning machine (OC-ELM) is proposed. The mathematical expression of LMK-OC-ELM is given onlyby using small sample data in normal state, and the localized kernel weights in LMK-OC-ELM is deduced under different gating models. On the basis of obtaining the localized kernel weights, the test statistic and threshold required for offline fault detection are defined to facilitate the engineering implementation. The proposed method is applied to the receiver. On the premise of controllable training time, the proposed method can improve recall, precision and specificity equitably compared with the other four common one-class classification algorithms. Taking LMK-OC-ELM-sig as the representative, the indicators of F1, area under curve (AUC), G-mean and accuracy are increased by 1.60\%, 1.57\%, 1.53\% and 2.23\% respectively compared with the localized multiple kernel anomaly detection (LMKAD) which is recently proposed. © 2020, Editorial Office of Systems Engineering and Electronics. All right reserved.},
  langid = {chinese},
  keywords = {/unread,Anomaly detection,Avionics,Detection accuracy,Engineering implementation,Extreme learning machine (ELM),Fault detection,Learning systems,Localized kernels,Localized multiple kernel learning (LMKL),Mathematical expressions,Multiple Kernel Learning,One-class classification (OCC),One-class classification algorithm,Small sample datum,Statistical tests},
  annotation = {Read\_Status: New Read\_Status\_Date: 2023-02-24T11:24:36.015Z}
}
