\section{Experiments}

\subsection{Reproducing the results of \textcite{frenayParameterinsensitiveKernelExtreme2011}}
\label{sec:reproducing-frenay}

% \begin{figure}[H]
% 	\includegraphics{frenay-cross-test}
% 	\caption{Illustration of the cross-test method from \cite{frenayParameterinsensitiveKernelExtreme2011}}
% 	\label{fig:frenay-cross-test}
% \end{figure}

In the paper, \textcite{frenayParameterinsensitiveKernelExtreme2011} use a double cross-test resampling
method which is illustrated in \cref{fig:frenay-cross-test}. First, they perform a 10-fold cross validation,
where the 9 folds of the training set are then used on a second 10-fold cross validation to determine the
best hyperparameters. This means that there are 100 training processes in total.

\begin{figure}[H]
    \input{figures/frenay-cross-test.tikz}
	\caption{Illustration of the cross-test method from \cite{frenayParameterinsensitiveKernelExtreme2011}}
	\label{fig:frenay-cross-test}
\end{figure}

\subsection{Hypothesis}

% TODO: formalize all the hypothesis
Our main hypothesis are:

\begin{description}
    \item[Shared behaviour] All kernels originating from limits towards infinity,
        have exhibit the same behaviour described by \textcite{frenayParameterinsensitiveKernelExtreme2011}.
        Where increasing the $\sigma$ hyperparameter past a certain point
        does not affect the kernel prediction capability.
\end{description}


\subsection{Comparing the performance of the kernels with different datasets}
\subsectionmark{Comparing the performance of the kernels}

\begin{figure}
    \includegraphics{plots/MSE_frenay}
    \caption{MSE results on datasets from \cite{frenayParameterinsensitiveKernelExtreme2011}}
\end{figure}

\subsubsection{Resampling}

Instead of using the double cross-test resampling used in \cref{sec:reproducing-frenay},
we opted to use a 5\texttimes2 cross-validation resampling method as described by \textcite{dietterichApproximateStatisticalTests1998}.

The 5\texttimes2 cross-validation
consists of doing a 50\textendash50 split of the dataset into two sets. First, we train on the first set and test on the second set
and then we train on the second set and test on the first set. The process is repeated 5 times, each time
with a different random split. We choose this method since it is faster than the double cross-test method
requiring only 10 training processes instead of 100.

% TODO: Add figure of the 5-2 cross-validation ?

\subsubsection{Normalization}

In order to be able to compare the results between different datasets, an effort
was made to normalize the data and hyperparameters. To that end, the
following measures were implemented:

\begin{enumerate}
    \item Variables (including target) are standardized
    \item We use normalized root-mean-square error as our performance
        measure.
    \item The kernel itself is normalized.
    \item The hyperparameter $\sigma_w$ is divided by the number of samples in
        the training data of each dataset.
\end{enumerate}

In the next paragraphs we clarify how each of these steps are implemented and
what is their effect.

\paragraph{Variable Standardization} this is implemented by taking the mean and
standard derivation of each column of the training set and using it to
normalize the columns ($z_i = (x_i - \mu_i)/\sigma_i$). These values are saved as
part of the model and used again when running on the test set. This process
is also applied to the target variable.

This acts as a method of scaling the data to values that are well behaved
for the SVM.

\paragraph{Normalized Root-Mean-Square Error} We define the
normalized root mean square error (\emph{nRMSE}) as follows:

\begin{equation}
    nRMSE = \frac{RMSE}{\sigma_{obs}}
\end{equation}

\subsubsection{Parameter grid}

For the parameter search, we tried all the combinations of values
in the bounds show in \cref{tab:paramgrid} in a $\log 10$ scale with
spacing of factor of $10$ between values. For example, for $\sigma_w$ the values
were $10^{-3},\,10^{-2},\,\dots,\,10^{3}$
% TODO: write this part so that it is more clear?

\begin{table}[H]
    \caption{Bounds of parameter grid}%
    \label{tab:paramgrid}
    \begin{tabular}{ccc}
        \toprule
        Variable & lower & upper \\
        \midrule
        $\sigma_w$ & $10^{-3}$ & $10^3$ \\
        $\varepsilon$ & $10^{-5}$ & $10^1$ \\
        $C$ & $10^{-2}$ & $10^4$ \\
        \bottomrule
    \end{tabular}
\end{table}

% NOTE: this is without scaling sigma
\begin{figure}
    \includegraphics{plots/nRMSE_frenay}
    \caption{nRMSE results on datasets from \cite{frenayParameterinsensitiveKernelExtreme2011}}
\end{figure}

% WARN: these don't have all values of epsilon in our grid
\begin{figure}
    \includegraphics{plots/nRMSE_bank}
    \caption{nRMSE results on Delve Bank dataset}
\end{figure}

% WARN: these don't have all values of epsilon in our grid
\begin{figure}
    \includegraphics{plots/nRMSE_pumadyn}
    \caption{nRMSE results on Delve PumaDyn dataset}
\end{figure}

\begin{figure}
    \includegraphics{plots/nRMSE_delve_bank_32_scaled}
    \caption{nRMSE results on Delve Bank32 dataset with $\sigma_w$ scaled}
\end{figure}

\begin{figure}
    \includegraphics{plots/nRMSE_all_scaled}
    \caption{nRMSE results on Regression datasets with $\sigma_w$ scaled}
\end{figure}

\begin{figure}
    \includegraphics{plots/nRMSE_frenay_scaled}
    \caption{nRMSE results on Frenay datasets with $\sigma_w$ scaled}
\end{figure}
